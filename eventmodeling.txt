Contents
Foreword by Adam Dymitruk vi Foreword by Gabriel N. Schenker xii Why I care xv
I Foundations
1 Why you should care 3
2 Event Sourcing - what is it? 9
3 Planning Systems using Event Modeling 41
4 CQRS, Concurrency, (eventual) Consistency 71
5 Internal versus external data 88
6 The Anatomy of an event-sourced Application 99
7 Event Streaming , Event Sourcing and
Stream Design 113
8 Domain Driven Design 128
9 Handling transactions in distributed sys-
tems using Sagas 144
10 Vertical Slicing 157
II Modeling the System
11 Brainstorming 175 12 Modeling Use Cases with Wireframes 184 13 “Given/When/Then”Scenarios 204
14 Use Case: Clear Cart 216
15 Use Case: Submit Cart 226
16 Use Case: Inventory Changed 242
17 Use Case: Price Changed 256
18 Structuring an Event Model 269
III From zero to running software
19 Technology Stack 279
20 Brief introduction to Axon 288
21 Implementing the first slice - “Add Item” 302
22 Implementing state view slices using
Live-Projections 327
23 Implementing Remove-Item and Clear-Cart 341
24 Example Integration with Apache Kafka
and Translations 354
25 Implementing a database projection for inventories 367
26 Implementing Automations 384
27 Submitting the Cart 421
28 Handling breaking changes 444
IV Implementation Patterns
29 What this part is about 463
30 Pattern: Database Projected Read Model 465
31 Pattern: Live Model 472
32 Pattern: The (partially) synchronous Projection 476
33 Pattern: The Logic Read Model 481
34 Pattern: Snapshots 485
35 Pattern: “Processor-TODO-List” - Pattern 490
36 Pattern: The Reservation Pattern 505

V
37 38 39 40 41
The missing chapters
Why the missing chapter? 515 Handling Metadata 517 Handling Security 528 GDPR - Handling sensitive Data 545 Handling the UI 559
Where to go from here? 584

Foreword by Adam Dymitruk
When I first sat down in front of a computer in the early 1980s, I was captivated by the endless possibilities technology could offer. Like many of my peers, I was driven by a passion to harness computing power to create more reliable systems and enhance usability for both end-users and technologists. Over the years, I witnessed the emergence of methodologies like Rapid Application Development, UML, Extreme Programming and Agile. Each promised a breakthrough — a better way of working. Yet, each eventually reached its own plateau, a local maximum in efficiency and effectiveness.
Then came Event Sourcing, and it was a revelation. Event Sourcing wasn’t just another methodology; it was a paradigm shift, a proper way to use systems thinking. It introduced a level of clarity to system design through full traceability, accountability and predictability. It treated information with the same rigor accountants apply to financial data. For me, it was a game changer. Implementing systems with Event Sourcing felt not only revolutionary but also responsible. With just two core patterns, it was remarkably simple — and remains so to this day.
After deploying my first production system using Event Sourc- ing, I realized I could never look at information systems the
vi

same way again. It was like taking the red pill in “The Matrix” — suddenly, the limitations and inefficiencies of traditional software development were starkly apparent. The industry seemed stuck in practices that had reached the asymptote of
diminishing returns.
This epiphany led me to develop Event Modeling. At Adaptech Group, the company I founded to champion Event Sourcing, we needed a way to communicate and design systems built this way. Traditional methods of capturing requirements, scoping and designing were too slow, error-prone and costly. Event Modeling emerged as a simple yet powerful notation and guidance to bridge the gap between requirements, design and implementation. Remarkably, it proved effective even for traditional systems not yet ready to adopt Event Sourcing.
I shared these ideas in a short article, and to my surprise, it resonated widely — it went viral. This overwhelming response affirmed that we were onto something significant.
Over nearly a decade of applying Event Modeling and Event Sourcing at scale, we’ve witnessed incredible results:
Rapid Issue Resolution: We can consistently fix problems without introducing regressions, within just four hours.
Elimination of Merge Conflicts: We’ve moved beyond the night- mares of concurrent development causing version control strug- gles.
Empowered Teams: Our team members feel autonomous and vii

valued, leading to the lowest turnover rates I’ve ever seen.
Risk Mitigation: By aligning units of work with units of pay, we offer clients true control over budgets and timelines through fixed costs and guaranteed bug fixes.
Objective Decision-Making: Contributions are assessed on merit, reducing subjectivity and fostering a fair working en- vironment with collaboration and transparency.
Simplified Schema Evolution: Changes are integrated proac- tively, eliminating the headaches of data migration.
Efficient Testing: With clearly defined events, testing becomes straightforward and extremely performant.
Flexible Team Dynamics: Our processes scale smoothly, allow- ing team sizes to vary without friction.
I could fill pages with more benefits, but the essence is clear: simplicity and clarity lead to profound efficiency and effective- ness.
It’s with great pleasure that I introduce this comprehensive guide by Martin Dilger, a professional who not only understands the nuances of Event Modeling and Event Sourcing but also brings a fresh, insightful perspective. Martin is the first person to capture these two concepts in a book, and his dedication mirrors my own.
So, who is Martin Dilger, and why should his perspective matter viii

to you?
I first met Martin in October 2023 in Munich during a presenta- tion on Event Modeling. Despite challenging weather, he drove hundreds of kilometers to attend. We connected immediately over our shared passion. Later, we met again at a conference in Austria focused on collaborative techniques in the tech industry. Martin’s expertise was evident as he guided others through the concepts with clarity and enthusiasm. It felt like finding a kindred spirit — someone equally committed to advancing our industry.
Like many of us, Martin has a relentless drive to improve the software industry. His deep understanding of Event Sourcing and Event Modeling, combined with his ability to implement and refine tools for collaboration, sets him apart. Martin doesn’t just theorize; he delivers practical solutions swiftly and effectively. At the time of this writing, he has extended the most popular digital collaborative whiteboard to support Event Modeling! And before that, he has single-handedly lead organizations to adopt modern version control systems and many more highlights. His leadership in adopting new approaches and technologies has been transformative for those he guides.
In this book, Martin provides you with the specific tools to:
Use Event Modeling as the Single Source of Truth: Learn how a simple diagram can unify your team’s understanding and communication of the entire system.
Harness the Flow of Information: Trace data from its origin ix

through its transformations, ensuring your system fulfills its purpose with clarity.
Apply Specification by Example: Utilize example data pre- dictably placed on a storyboard to illustrate concepts effectively.
Represent Behavior as Data: Understand and author system behaviors clearly by treating them as data.
Drive Multiple Models: Learn how to build purpose-built ab- stractions instead of trying to make one-fits-all compromises.
Simplify Integration: Use straightforward “to-do lists” to illustrate integrations without delving into unnecessary imple- mentation details.
Deconstruct Systems into Subsystems: Break down complexity into manageable components.
Automate Testing Efficiently: Implement testing seamlessly with events that execute instantaneously, without extra frame- works.
Proactively Evolve Your System: Integrate event schemas changes as they occur, avoiding big-bang schema migrations and minimizing disruptions.
By reading this book, you won’t just learn new techniques — you’ll transform the way you approach software develop- ment. You’ll gain clear, actionable steps to solve problems, communicate with unprecedented clarity, and estimate projects
x

more accurately. In essence, you’ll be equipped to dismantle complexity and foster collaboration like never before.
Looking Ahead
Event Modeling and Event Sourcing are continually evolving, but their core principles remain rooted in simplicity. As improve- ments emerge, they serve to refine and enhance without adding unnecessary complexity. This book lays the foundation for you to adapt to future advancements while reaping the benefits today.
Closing Remarks
I invite you to embark on this journey with Martin as your guide. His insights and practical approach make this book not just a technical manual but a gateway to more effective, transparent and responsible system design and implementation. Whether you’re new to these concepts or seeking to deepen your understanding, this book is an invaluable resource.
May this exploration inspire you as it has inspired me. Together, let’s redefine what’s possible in software development.
Sincerely, Adam Dymitruk
Event Modeling Creator and Event Sourcing Expert
xi

Foreword by Gabriel N. Schenker
In the landscape of modern software development, we often treat the creation of business applications as an art form rather than an engineering discipline. This approach, while romanti- cized, has led to countless projects exceeding budgets, missing deadlines, and delivering systems that resist change. Martin Dilger’s “Understanding Event Sourcing” arrives as a breath of fresh air, presenting a systematic and practical approach to software development that challenges this status quo.
At its core, business software development is about automating well-understood processes that humans have been performing manually. It’s not rocket science—it’s about asking the right questions and modeling the answers effectively. This is where Dilger’s book shines. He introduces Event Modeling (EM) as a powerful methodology that brings technical and non- technical stakeholders together, speaking a common language and working toward shared understanding.
What particularly impresses me about this book is its elegant simplicity. Dilger demonstrates that even the most complex business processes can be modeled using just four fundamental patterns: state change, state view, translation, and automation. Like building with LEGO blocks, these basic patterns combine to create sophisticated systems. This simplification doesn’t just
xii

aid understanding—it transforms the traditionally challenging tasks of planning and estimation into straightforward exercises.
The book bridges theory and practice masterfully. While many resources on Event Modeling exist in blogs and videos, Dilger provides what has been missing: a comprehensive, step-by- step guide from initial concept through complete implemen- tation. His practical examples using Java, SpringBoot, and the Axon framework serve as clear illustrations of the principles, readily translatable to other technology stacks.
Perhaps most importantly, Dilger addresses the crucial aspect of testing, showing how business rules can be specified using familiar Gherkin syntax and implemented naturally within an event-sourced system. This approach ensures that testing re- mains accessible to all stakeholders while maintaining technical rigor.
For too long, we’ve accepted declining velocity and increasing complexity as inevitable facts of software development. This book challenges that assumption, showing how Event Modeling and Event Sourcing can maintain consistent development speed while managing complexity effectively. Whether you’re a seasoned architect or a developer looking to improve your approach to building business applications, “Understanding Event Sourcing” provides the practical guidance needed to transform how you think about and build software systems.
This is not just another technical book—it’s a roadmap to more predictable, maintainable, and successful software projects. Martin Dilger has created an invaluable resource that I believe
xiii

will influence how we approach software development for years to come.
Gabriel N. Schenker
Head of Platform Engineering, iptiQ
xiv

Why I care
Initially this chapter wasn ́t planned. But many people asked, why I actually cared. Why do I make this effort. What drives me.
I began developing software around 2005, so I’m nearing my 20th anniversary in this business. It’s kind of incredible, as I don’t feel like it’s been 20 years at all. But one thing has not changed: Software Development seemed hard back then, and it seems hard today.
However, I have also seen different projects. Projects that did well. That worked, were carefully planned and successful. I think overall we can do better, if we learn to ignore all the distractions that divert us from what we should truly be focusing on—delivering value.
I once had a client say, “Software developers are like pizza deliveries: they’re always late, often deliver only half of what they promised, and it’s never as good as it looks on the shiny prospect.”
In essence, he’s right. In my opinion, software development hasn’t changed much in the last 20 years—maybe not even in the last 40 years. Agile, microservices, cloud, and AI haven’t fundamentally altered that fact. Software development is still
xv

like the pizza nobody wants in the end. But you have to eat it because you ordered it.
However, in 2021, I learned about Event Modeling as something that was about to change everything for me. I finally could talk to my customers in a language they understood. I could discuss the problems that truly bother them without confusing them with technical terms and details they neither understood nor cared about.
We could suddenly focus entirely on one thing: the real problem to solve.
We found that the root of many issues wasn’t premature opti- mization, as many developers think. It was the lack of a common language—the lack of a structured way to communicate success- fully.
This is not a new thing. The Book “Domain Driven Design” by Eric Evans talks about this common language between IT and Business at length. He famously named it the “Ubiquitous Language”. We will discuss this in more detail in chapter 8.
Alberto Brandolini, the inventor of Eventstorming1 famously said, “It’s the developers’ understanding, not your business knowledge that becomes software.”2
But what if the developer simply gets it wrong? What if the lack
1 https://www.eventstorming.com/
2 https://x.com/ziobrando/status/1347126001340358656
 xvi

of a common language, the absence of clarity in terminology used, and the lack of time to properly discuss the problems to solve result in constant misunderstandings between business and development? What if this is the problem we’ve been trying to tackle for 20 years with one technical solution after another?
Event Modeling seemed magical to me back then. Every time I used it, it worked, even with people I had never met before. Suddenly, everyone understood. Suddenly, we could talk to each other. I began using it whenever I could. At first, I was unsure if I was doing it correctly, but more and more, I felt confident because it was actually simple—almost too easy.
Finally, there was clear communication. We could confidently discuss the business and the problems we aimed to solve next. More importantly, we could talk about the cost of delivering valuable solutions that would address these problems once and for all. We gained a lot of trust in the systems we planned and got rid of that lingering uncertainty—the nagging feeling that something important was being overlooked.
I’ve seen really bad things: huge amounts of money burned for nothing, waste of time, labor, and knowledge. Meetings with endless discussions but no outcomes. Requirements that nobody truly understood but were used to estimate and plan for months and years ahead - for a product that in the end took much longer than initially planned.
For me I decided from then on, that I wanted to pursue a different way. I decided that our industry can do better. And I realized, that we don’t have to change everything to get there. It ́s just
xvii

some minor tweaks. In January 2024, I said in a webinar 3, Software Development will fundamentally change within the next 2 years. And I think we are on the way as an industry.
This book is about everything I learned in the last 15 years. It is about what I discovered in 2021. Tiny changes in the process, focus on the real problems, communication in a language that is understood by everyone makes all the difference you need.
Internally, we have a name for it. We call it “accelerate”. Teams working with these ideas should feel “accelerated” pretty quickly. The longer you work with this approach, the faster you iterate. Software development can be a well-oiled machine, constantly going from planning to development and back. Em- bracing change as part of the game, without slowing down. You will find a description of this development process and how I typically work in the Appendix about “Accelerate”.
I ́m standing on the shoulder of giants. None of the ideas presented in this book are my very own. I just combine what some people way smarter than me came up with like Adam Dymitruk, Greg Young, Jimmy Bogard, Eric Evans and Vaughn Vernon.
This is what drives me and I can ́t wait. I want to see what will change when we start to do things this way.
Martin, July 2024
3 https://nebulit.de/bessere-software-2024 xviii
 
PS: One more thing maybe.. I put my heart into this book and with the help of many, many people managed to write it in 3 months. I hope it ́s a great book, but it ́s certainly not perfect. Not yet.
If you find issues, help me to make it better. Open issues here on github:
https://github.com/dilgerma/eventsourcing-book/issues Let ́s make this the best book it can be.
xix

I Foundations

1
Why you should care
What is Event Sourcing and why should you care as a developer, team manager, or company? Event Sourcing is far from a new technology; it emerged around 2006 in the software industry. So it’s about 15 years at the time of writing this book since it hit the software development mainstream (and much older if you trace it back to its ancient origins).
So why would you care?
Event Sourcing as an implementation pattern is rather simple. Instead of storing your data in tables and relations, you store the facts that actually happened in the system in the order they happened.
This means the data in your system is like a simple story of what happened recently.
First, the customer was created, then the customer moved to a 3

UNDERSTANDING EVENTSOURCING
new address. Then the customer made a big purchase. Because of that, the customer got promoted to premium status, and so on.
If you add up all these facts (in Event Sourcing we call the facts “events”), you get a customer with a new address in premium
status.
Although you get the same customer with the traditional CRUD- based approach, where a system stores data in a normalized relational schema, what you lose is the history of how the customer became a premium customer over time.
Fig 1.1 - Historical Data (top) / flattened data (bottom)
So in the end, you have the same result. Why would it matter in the first place?
It starts to matter when your company wants to work with the data. What if it was important to know if the customer became
 4

WHY YOU SHOULD CARE
premium before or after a certain point in time?
What if you wanted to know how much money people spend on new furniture within two weeks after the customer has moved to a new address?
What if you needed to know when the customer’s address was changed and how often?
By using Event Sourcing, we are adding the dimension of time to our system. The system records the facts that happen along a timeline. This dimension gets lost when data is flattened to a relational database schema.
Of course, you can consult your data warehouse (if you have one) to get back some of the data. In simple terms, a data warehouse is a big database to capture all relevant data changes over time. But what would happen if all this data would be freely available and could be used for whatever ideas your business unit comes up with in the future?
What if you could feed an AI model with the data you collected in the last 10 years? And what if this model would help you explore new markets based on hard facts rather than blank theories?
The only way for your company to truly learn and benefit from the past is keep detailed records. This all requires some new thinking and a different approach. And it requires the information to be accessible in a flexible format.
Using Event Sourcing makes it easier to reason about your 5

UNDERSTANDING EVENTSOURCING
system without quickly falling into the trap of thinking in purely technical terms. It’s much easier to understand a system if it’s modeled the way it really works.
Instead of saying, “We need to set this blocked flag on the customer” (which no business person would ever understand), it’s much more natural to create a new event named “Customer Blocked.” This event indicates that a customer was blocked at a certain point in time and for a specific reason.
But unfortunately, this way of thinking is rarely taught in schools or universities, and that’s why many developers strug- gle with this simple concept even after years in the software industry.
“Not losing information” is the foundation of Event Sourcing. We keep information readily available at all times because you never know what it might be useful for in a few years.
Learning Event Sourcing is similar to learning a new language. You often need to search for how things are done using this approach. However, if you try googling or asking ChatGPT, you’ll quickly realize that finding information about these concepts isn’t as straightforward as you would expect. Although the information is out there, it’s scattered across many different sources. Plus, there are significantly fewer books on Event Sourcing compared to those covering traditional information systems.
So now, again the question: why would you care? 6

WHY YOU SHOULD CARE
Because information is the new gold. Data helps you make informed business decisions. And keeping data available comes basically as a by-product when working with Event Sourcing.
In this book, we will dive deeper into Event Sourcing and everything around it, mainly focusing on the implementation side of it. That is what we lack most - clear guidance of how to actually do it using real world examples.
Of course, we will discuss the theories around it and what it means to work like that. But compared to other books and information available, we will try to dive much deeper into the real implementation of it to really understand what it means to build an event-sourced system.
But not only that.
We’ll also delve deeper into the planning phase of event-sourced systems, using collaborative modeling techniques you might or might not be familiar with. Our main focus will be on Event Modeling, a technique developed and refined by Adam Dymitruk over the past decade4. Event Modeling helps us plan and discuss information systems in a clear language. While it’s not limited to event-sourced systems, you’ll find that it’s a particularly good fit for them.
So what will you learn in this book?
You will learn pretty quickly what took me 15 years to learn.
4 https://eventmodeling.org/
7
 
UNDERSTANDING EVENTSOURCING
It’s entirely possible to build plannable, scalable, and well- structured information systems consistently, not just once, with the right approach and a solid plan. In this book, I’ll share how I create these plans. Some of the content reflects my own perspectives, and you may disagree with certain points— that’s understandable. This book is not abstract; it dives deeply into the details of Event Modeling, Event Sourcing, and the implementation aspects behind them. I recognize that some readers might find this challenging, and I apologize if that’s the case.
We’ll dive into Event Sourcing patterns, where you’ll learn some techniques that aren’t secret but aren’t widely shared either, which can help you implement an event-sourced system in a scalable way.
I’ll share how, after 15 years of developing information systems, I’ve structured my architectures and systems in a modular way. This may not be the ‘best’ or ‘perfect’ approach, but it’s the one that has worked well for me.
I’ll keep things as brief and focused as possible, leaving out anything that isn’t crucial to your success.
Believe me, implementing event-sourced systems is simpler than you might expect—as long as you have a clear roadmap and someone to guide you. I wish I’d had a resource like this when I started, so my goal is to help you along your journey and get you started today.
8

2
Event Sourcing - what is it?
A real world Event Sourcing Example
One of the most interesting things about Event Sourcing is its natural alignment with human thinking. I was reminded of this recently when my kids started a street sale of their old toys. They constructed a sales stand and lined up all their old toys. Pricing was simple: small toys cost 1 €, big toys cost 2 €.
9

UNDERSTANDING EVENTSOURCING
 Fig 2.1 - different types of products
They cut out price tags from paper and put them on each toy. My son used red- and my daughter yellow paper.
Sales went pretty well.
Whenever a sale was made, they erased the current number of sales on their “sales report” and wrote down the new number.
Soon they realized that when all the toys were sold, they only needed to count the red and yellow price tags to know how much money each child made that day. What they didn’t realize is that they just naturally discovered Event Sourcing—something many senior developers struggle to understand nowadays.
How were two children with paper price tags able to naturally use Event Sourcing?
10

EVENT SOURCING - WHAT IS IT?
Every sale is an event. Something that has happened. Let ́s name it “Small Toy Sold” and “Big Toy Sold”. We store Events using the price tags, which are put into a box. The box is used to store our events. It is our Event Store. The more sales, the more price tags and events in the box.
The color code of the event represents different event types.
At the end of the day, we can use all the events in our Event Store to build different reports. Even two kids with a sales stand need some reporting at the end of the day.
First, let’s sort the events by color and then simply sum the price for each event to get the profit per color. That’s how much money each child made. Amazing, isn’t it? We project the information in the events to one simple number per color code.
Fig 2.2 - Profit Calculation Projection
The events in the Event Store are the source of truth. We can build any order-related projection out of these events.
 11

UNDERSTANDING EVENTSOURCING
I asked how many toys were sold. The kids just counted all the price tags in the box, regardless of color. We again project all events to one single number - the number of sales.
Fig 2.3 - Order Report Projection
Want to know when there were the most customers? Imagine the kids noted down the hour of the day on the price tag when the sale was made (they didn’t do this because it was not relevant to their business, they thought). But if this information was recorded, you could go over all the events and make a tally per hour. They would have known exactly which hours were the most profitable. This is where they should focus on selling next time and ignore the hours where nothing was sold.
Want to know which types of toys were requested the most? Imagine again that they noted down the category of toys sold, like “Lego,” “Schleich,” or “Book.” They could then go over all the events and make a tally per category. There we have our order report calculated directly from the sales made.
Why is this so amazing?
 12

EVENT SOURCING - WHAT IS IT?
Event Sourcing is all about this. It’s all about recording infor- mation and building different kinds of projections out of the raw data in the Event Store. It doesn’t really matter how your projections look like. From a technical point of view, anything is possible. The really important aspect is that we keep the information and give ourselves the possibility to work with it.
Event Sourcing Building Blocks
Believe it or not, most software systems are simple in nature. They receive data. They process data. They store data. They pass data along to the next system. Of course, that ́s an over- simplication, but in general, it is true for most systems.
Whenever I talk about Software-Systems, I use the term “information-system” interchangeably. Because the job of most Software Systems is to transform raw data to structured,
business-relevant information.
I was taught in university, that handling data means writing it to a database, and if you need the data again, query it with SQL. This approach looks simple in the beginning and is known as CRUD: Create, Read, Update, and Delete.
However, this isn’t how most businesses fundamentally work. By designing information systems this way, we limit ourselves to a mental model that constantly requires translation to the real world. Wouldn’t it make more sense to model software more closely to how the business actually works?
13

UNDERSTANDING EVENTSOURCING
The real world happens in business meetings, about the business by the people running the business. Business people do not talk about storing an order in a SQL database. They talk about customers making a purchase. In business discussions, you will rarely hear the term “database” or “SQL” at all, unless the business happens to sell database-systems.
When planning information systems, developers (including me) tend to jump much too fast into technology discussions instead of focusing on the real problems to solve. The reason is simple, Technology is what we fundamentally understand. For us this is a safe place with less question marks where we know what to do.
When we start to think and reason about a system with Events and Facts, it forces everybody to look at the system from a different perspective. We no longer think about data, columns and rows but behavior.
What is it, that the system actually does? In Figure 2.4 you see a summary of all the “Facts” that could happen in an online-shop.
14

EVENT SOURCING - WHAT IS IT?
 Figure 2.4 - Facts of a System
Event Sourcing is about not only using Events in discussions, but also in the implementation of our system. We more or less directly translate the Events to Code. You will see in the next chapter how this works in practice, when we talk about Event Modeling. But before that, let’s dive into some of the typical components you will find in an event-sourced system and why there is so much (unnecessary?) fear around it.
What are events?
As mentioned earlier, events are simple facts and not technical by any means. Whenever something happens in the system, we record this as an Event. So simply said, Events are records of the past, therefore we write them in the past tense. Instead of “Customer is placing an order”, we say “Order was placed” or in short “Order placed”. If an Order was placed, there is no way to change this fact. Even deleting the order from your database will not change the fact, that order has been placed earlier.
Since we can ́t change what happened in the system, we consider 15

UNDERSTANDING EVENTSOURCING
Events immutable. You can ́t change an Event that happened. Don’t even try to find a way to change them. This requires a minor mindset shift at first, but you ́ll see during this book that keeping Events immutable actually makes it much simpler to maintain the system.
Looking back at our previously gathered events for the e- commerce-system, we could find the following Events:
• “Order Created”
• “Order Submitted” • “Order Confirmed”
But now that we have all these Events in our system, how do we actually use them? When using a traditional system with a relational database, we can simply query any kind of data by using some simple SQL Queries.
In Event Sourcing, by design there is no state, no tables and no documents we can simply query, it’s all Events. But how are these Events organized?
What are Event Streams?
Storing events alone is not enough to structure information. In the context of event sourcing, an event stream represents the chronological sequence of all state-changing events that have occurred for a particular business entity or process. Each event reflects a discrete action or change, captured as an immutable record that contributes to the entity’s history.
16

EVENT SOURCING - WHAT IS IT?
Much like how a database table structures data in a relational database, event streams organize these recorded events in a meaningful way. Business entities in traditional databases are identified by their primary key, business entities in Event Sourcing are typically identified by their Stream-Id.
In a database, we store only the latest state of an entity, with each update overwriting any previous data. Event streams, on the other hand, capture every event, preserving the entire history of changes. This makes event streams a powerful mechanism for reconstructing an entity’s state by replaying its event history, while also providing an auditable and flexible approach to managing data over time.
For example, consider the previously mentioned shopping process in an e-commerce application. Each user action—such as adding an item to the cart, removing an item, or submitting an order—generates an event that is appended to the shopping event stream. Let’s imagine a customer starts a new session:
• Event1:ProductAwasaddedtothecart
• Event2:ProductBwasaddedtothecart
• Event3:ProductAwasremovedfromthecart
By replaying these events, the system can accurately reconstruct the current state of the cart, which in this case holds only product B. This not only allows the system to present the correct state of the cart to the user, but also provides a complete audit trail of the customer’s activity. Every state transition is preserved, which can be crucial for improving the customer experience, debugging issues, or even performing forensic
17

UNDERSTANDING EVENTSOURCING
analysis in case of errors.
In essence, an event stream in event sourcing acts as the backbone for rebuilding the state of an entity based entirely on past events, enabling a clear understanding of how any particular state was reached.
What are Projections?
Projections allow us to use the events stored in event-streams to build specific views of our information for different use cases. A projection takes all events from one or more event streams and builds a specific view.
There can be many different views for our order history:
• A tabular view for the order history of the customer, dis- played in a UI
• Anaggregatedviewforreporting“cancelledordersinJuly” • Areport“ordersperhour”,constantlyupdatedinreal-time
for marketing to see when most customers are active
18

EVENT SOURCING - WHAT IS IT?
 Fig 2.5 - Order Projections
These are all based on the same data but represent it in a different format.
One of the strengths of Event Sourcing is that since we do not delete any data but keep it as long as we need it, we can build the projections today we didn’t know we needed yesterday. And the same for tomorrow.
Event Sourcing gives us the freedom and flexibility to work with our data. Technically, a projection can be a database table filled from events; it could also be a CSV file sent via batch to an external partner to detect fraud. A projection can be an Excel sheet for accounting showing the latest orders or like shown in Fig. 2.5 a generated PDF.
Don ́t worry, we ́ll get into the details of how projections work in the next few chapters where I ́ll provide some practical use cases, that hopefully make it much clearer.
19

UNDERSTANDING EVENTSOURCING
What is the Event Store?
Very often, you’ll hear the term Event Store when talking about Event Sourcing. This is simply the component that stores events, like a special-purpose database. There are powerful Event Stores like the Axon Server5 or EventstoreDB6 that are tailor-made for Event Sourcing, but an Event Store can also be just a simple relational database. Sometimes even plain files will do the job.
Event Stores only support a limited set of operations, but these operations are highly optimized.
Appending Events
Event-Streams are append only. This simplifies datahandling, as the data basically only grows in one direction and everything that was written is read-only by default. In Event Sourcing we can rely on the fact that events don ́t change.
Reading Events
Event Stores provide efficient access to the event-sequences stored in streams. This could be reading all events or partial events for a given Event-Stream. An example could be all events for a given Customer-Id, but it could also be only the events for this customer in the last week.
Typically Event Stores do not offer an API to alter events.
5 https://www.axoniq.io/download/axon-server-releases/ 6 https://www.eventstore.com/eventstoredb
 20

EVENT SOURCING - WHAT IS IT?
The relational schema used to build an event-sourced applica- tion using a database is quite simple and provided here as an example.
Fig 2.6 - Events Schema
The id-column is typically a database sequence that is increased with every event. This is necessary as the order of events is important and needs to be maintained.
The stream_id column identifies a sequence- or a stream of events. An example could be “stream_customer_47” for all events related to the customer with id “47”.
The payload column can be used to serialize an event to a suitable representation like JSON, XML or even plain-text.
The metadata column holds meta-information about the event, like who created it, when was it created, which system created it and so on. This could also be in JSON Format.
Oftentimes you ́ll also see a type column which indicates the type of event for deserialization.
When our application evolves, it ́s sometimes necessary to add new versions of an Event. To be able to correctly deserialize these events, we maintain their version in the version-column.
 21

UNDERSTANDING EVENTSOURCING
We ́ll learn much more about versioning in chapter 28.
No matter how you structure it, in the end it is just a handful of tables to store events and their metadata. You don’t need to think about the perfect model of your application and design the table schema for any eventualities right at the beginning. We simply store events in an “events” table and build the projections as we go and need them.
You don’t even have to use a database as previously mentioned. Adam Dymitruk recently built an event-sourced Open Space Management System using a file-based Event Store. Every event is just a file. Crazy, isn’t it? You can follow the stream on Twitch.7
Of course, you wouldn’t want to do this in a real-world applica- tion (Adam might disagree), at least not a commercial one, but the essence is simple. Using a commercial Event Store like Axon Server has some benefits. It allows you to scale and gives you features like multi-tenancy, cryptography support and location transparency. But to get started, the best approach is to keep it simple and just start. You can always adjust and migrate the data later. I often see eventsourced systems getting stuck in analysis paralysis, analyzing tools and frameworks to get started. It’s much better to just start and improve along the way. In Germany we say “probieren geht über studieren” which translates to
“don ́t just read about it, do it.”
7 You can follow the stream on Twitch here: https://www.twitch.tv/eventmo deling
 22

EVENT SOURCING - WHAT IS IT?
What Event Sourcing is Not
Event Sourcing is not a panacea. As with anything in the IT industry, there is no silver bullet. Everything involves trade- offs, and Event Sourcing is no exception.
But first, let me clarify one thing. I often read that if you use Event Sourcing everywhere, you are doing it wrong. I disagree, fundamentally so.
Why?
I can only speak from my personal experience; it’s an approach- able way to develop flexible software. I often get asked what kind of software I would develop using Event Sourcing. For me, this is the wrong question. It implies that Event Sourcing is only suitable for special kinds of systems, that it’s a special weapon you only use in certain circumstances. In all other cases, you fall back to the default, which is most often a simple CRUD System.
If you ask me, it should be the other way around. Event Sourcing should be the default way in which information is processed, and only if you have very good reasons you should deviate from this path.
Also, I often hear the misconception that Event Sourcing is only valid for systems that require an audit trail, like banking or insurance. Sorry, but again, I disagree. Every system design profits from properly handling and storing information, no
23

UNDERSTANDING EVENTSOURCING
matter which industry or sector.
Event Sourcing is just a certain way to handle data. Nothing more, nothing less. It’s not a special kind of architecture pattern. It’s a simple way to implement the persistence in your system.
Common Misconceptions
Miconception #1: Event Sourcing is Complex
Why is it considered complex? Or let’s ask it the other way around, why is using CRUD considered the simple way?
Because we are primed to think in state. We learned to think in state in university and in most books we ́ve read. And because we have been learning to think in state for decades now, the tooling for this is pretty good. It’s super simple to bootstrap a CRUD application, get some state, normalize it, and flush it to the database. Just use Spring, Spring Data, and you are just a few annotations away from persisting your customer into the customer table.
It’s not so easy to get started with Event Sourcing (unless you use something like Axon, which gives you a similar experience to get started, but more on that later).
Let’s be honest, you do not need a CS degree to build a simple working CRUD application. Anyone can learn this within one or two hours. So is this misconception actually not a misconception at all? Is CRUD the simpler way?
24

EVENT SOURCING - WHAT IS IT?
Most beginner tutorials online show you exactly this. Build a simple entity, persist it into a simple database. If the tutorial is any good, you will at least implement some kind of relationship between a handful of entities and define a table schema using foreign keys to enable referential integrity.
You basically start with an assumption of how the data should look like based on your current requirements. A best guess.
And I agree. Using CRUD for this kind of application is absolutely fine. But what if the application grows? As most applications and software systems inevitably do? What if we define additional entities?
And what if the relationships between these entities grow as well? We’ll add foreign key constraints, we’ll add relationships, and we’ll add field after field to these entities to implement new features.
Let’s look at this in a concrete scenario.
Assume you have to implement a part of a bigger CRM system that allows creating different kinds of persons who are managed in the system.
Persons have very different attributes. You have a natural person with names, surnames, and one address. But also legal entities, which do not have surnames and can have any number of addresses.
25

UNDERSTANDING EVENTSOURCING
 Fig 2.7 - Different Entity Types
And what if the person type “legal entity” wasn ́t known when you started to design your schema in the first place?
Very likely you have something like a Person class with some attributes that make up a person. And now, if this additional requirement comes into the system, you’ll need to figure out how to match a legal person to the person-structure. In the end, they are both persons somehow, right?
Is it the same schema? Having seen enough projects, most of the time it will be the same table. You will add additional attributes, and you’ll make other attributes nullable to squeeze “legal person” into the person table. Not by any bad intention but just because it looks like the easy path.
This tiny change, that looks like a good idea in the beginning has profound side effects later. The data in the database no longer matches the data in the Person class. For a person in
26

EVENT SOURCING - WHAT IS IT?
code, the surname is required; in the database, it’s nullable as legal persons don ́t have surnames
And what if additional person types come up?
Fig 2.8 - Table Schema
• Authorities
• Governmentagencies
• Nonprofitorganizations
We have to make decisions on how to structure entities, how to define the correct boundaries, and how to define the table schema so queries can be done efficiently. There are many questions to answer, and unfortunately, we have to make a lot of critical decisions when we know the least about the system we are building.
And we will get some of them wrong. That’s inevitable. If you live under the illusion that you just need to spend enough time crafting your perfect data model to prevent this, you are wrong. Sorry.
Always hope for the best, but assume the worst. And the worst is, we’ll get it wrong.
 27

UNDERSTANDING EVENTSOURCING
So how do you address this? This is the fundamental dilemma of using CRUD-based systems. We’re so accustomed to thinking in terms of things—nouns and static state—rather than behavior. When you model an e-commerce system, it seems natural to define a “Shopping Cart” that is stored in a “cart” table, or a “Customer” that’s stored in a “customer” table. And in the beginning, this might work fine. But as the system evolves, the assumptions about what a “Customer” is can change dramatically. At that point, you’re stuck with the rigid “Customer” view you started with, which no longer fits the needs of the system.
The bigger the application grows, the more you need to invest to keep it flexible and maintainable. I’m not saying it’s not possible. Of course, it is. But in CRUD systems, complexity tends to grow unless you invest more and more time and money to fix it.
Our systems need to be flexible and adaptable. How does Event Sourcing help? Event Sourcing has this flexibility built in. We don ́t design table schemas upfront.
Instead of thinking about how to store the different person types in the system, we think about how these different person types are used in the system in the first place. Maybe legal persons are only necessary in a very specific area of the system? Using the same datastructure for persistence might create accidental coupling between all person types which does not exist in the real world.
You might have different types of events like “Person Added,” “Organization Added,” and “Legal Person Added.” All of these
28

EVENT SOURCING - WHAT IS IT?
events can have different schemas, contain different informa- tion and describe what actually really happens in the system.
Fig 2.9 - Event Types
We gain much more flexibility in how we structure information because we’re no longer confined to a static view of the system. Instead of being bound by fixed structures, our events describe the system’s behavior. When that behavior evolves, we can simply introduce new events to capture and accommodate these changes, without needing to overhaul the existing data model.
Could we make the same mistake by just having one event type like “Person Added” again with our nullable fields? For sure. But in Event Sourcing, it’s easier to do it right. Adding an additional event type for every person type added is simple. And the best part - we do not have to adjust any previously defined person type events when doing this.
In the CRUD approach, every time we add fields to the globally used table, we need to make sure all use cases still work and proper null checks are in place.
 29

UNDERSTANDING EVENTSOURCING
Don’t get me wrong, there are of course better designs than the global person table even in the CRUD approach; that is not the point. The point is, it’s easier to get it wrong if you do not have all the information.
Misconception #2: Event Sourcing is not performant
There is a lot of fear about the performance of event-sourced applications, especially when there are millions of events in the Event Store. First of all, I can assure you, performance is not really something that differs between event-sourced systems and CRUD-based systems. If it’s slow in CRUD, it’s most probably also slow in the event-sourced variant.
The performance problems typically have nothing to do with the number of events stored in the database or the Event Store. Why is that?
I get this question, or at least a similar one, all the time: “Does it really make sense to work with Event Sourcing here? Does it make sense to play back thousands of events to get the customer projection?”
I bet you have either heard this question already or you may even have it in mind yourself.
There are some misconceptions and misalignments in this phrase that we should discuss.
30

EVENT SOURCING - WHAT IS IT?
If you need to play back thousands of events to get to the state of a customer, there is something weird, maybe even wrong. You should not have to go through thousands or millions of events to build the current state of your system.
Against the common assumption, when working with Event Sourcing, you do not have one long continuous stream of events that you play back time and time again. In reality, it’s more like you have many smaller streams, and you play back only the portions you need to build up your state.
Exactly as you would design your tables in a relational model, you design your streams in an event-based model. By designing streams, you typically also design your business capabilities.
Therefore, for the customer, it could make sense to have a “customers” stream containing all the events that belong to one customer. It does not have to be a stream containing all
customers. You also could define a stream per customer.
Getting all the events from the stream (in pseudo-code) then would be like:
Fig 2.10 - Pseudo Code to get all events for a customer
To build this customer from the stream of events, you would have to read 30, maybe 100 events, not thousands. Do you think
 var streamId = "stream-" + customer.getId()
eventstore.getEventsByStreamId(streamId)
31

UNDERSTANDING EVENTSOURCING
a modern IT system could handle loading 30 events and applying them in order? Oftentimes, this is even much faster than CRUD because the events will be stored close to each other and loading them could then require less IO than the necessary JOINs in a relational model. In the end, for most applications it simply won ́t matter.
So yes, if you store all events in one global stream, Event Sourcing will be slow. But if you invest a bit in designing streams in a meaningful way, the whole performance argument disappears. And we go even further, because oftentimes, we do not even read directly from the stream but from a projected database table.
So technically, if the client just reads from a projected table, from a client perspective, there is no real difference performance-wise between an event-sourced application and a CRUD application. We’ll talk much more about this when we discuss database projections in chapter 25.
32

EVENT SOURCING - WHAT IS IT?
Misconception #3: Event-Sourced Applications are tightly coupled and become extremely painful to develop after the initial phase
How come this misconception is so broadly accepted?
Very often, I hear as a Pro-Argument for Event Sourcing that it’s possible to simply read the events from other services as you like and build the data structures you want. Honestly, if you hear this argument from a consultant trying to sell Event Sourcing to you, I’d suggest you run. It’s almost certain this consultant does not have any prior experience developing event-sourced systems.
Why do I make this claim?
If you have two microservices, would you allow one service to read directly from the internal tables of the other service? Hopefully not. This is one of the worst forms of coupling you can get. It results in long discussions when you need to make adjustments in the codebase. The service reading data from the database is tightly coupled to the schema of the other service. Every change requires the service to adapt. It’s easy to break something.
Wouldn’t it make much more sense to have an API that contains exactly the data the consumer needs to do its job? No matter how we refactor our internal database, maybe even switch over to a new database, we can keep the API stable and the consumer will never know.
33

UNDERSTANDING EVENTSOURCING
So why would we make it any different using Event Sourcing? If you read the internal events of a service, technically there is no difference if you just read the internal database directly. Well, technically there is because Events are immutable. They won ́t change, databases do. Still, if there are 10 or more clients who depend on the structure of your internal event, you will likely be very cautious to finally rename this field although you wanted to do this for years.
Since persisted Events are immutable, a change in structure will not affect consumers in the same way it would affect systems using a shared database. Changing the structure of a shared database has an immediate effect on all systems. Changing the structure of an event only has an effect on future events.
It’s not Event Sourcing itself that causes the coupling. It’s your architecture and the fact that you allow one service to read internal data from another service. That’s a bad idea in Event Sourcing, but it’s an equally bad idea in CRUD.
So why not make a clear distinction between internal events and external events? Internal events strictly belong to one service and one context. External events (some name them integration events) exist to distribute data from one service or context to another. These events are much more stable than the internal events of a service and are not refactored that often. And again, we will see how that works when we talk about integrations in chapter 24.
34

EVENT SOURCING - WHAT IS IT?
Misconception #4 - Event streaming and Event Sourcing is the same thing
Let’s clarify this right from the start. They are not the same. Not even close. This distinction is so important that we will talk about it in detail in chapter 7.
Event streaming and Event Sourcing are very different things that basically have nothing in common besides the same termi- nology. Nevertheless, they get confused all the time.
If you care about storing information using Event Sourcing, you store all changes in your system as they happen. In Event Streaming, you typically process data in near-real-time as it comes in. However, it is absolutely not required to store the incoming events from a stream as events in an Event Store. Often, it is even a really bad idea. Imagine you are processing IoT data. Some IoT use cases are known for the vast amount of data they send over the wire. We are talking about millions of events per second.
You typically do not store these events to build state out of them long term. Either you process the data live via a stream processing library like Kafka Connect or KSQL, or you aggregate the data on the fly by updating counters and sums as data flows in.
Events in Event Sourcing carry business meaning. Call them domain events if you will. They are carefully crafted and model business processes when combined. Event streaming is often much more technical, and the events are typically much more
35

UNDERSTANDING EVENTSOURCING
fine-grained than in Event Sourcing. Agreed, it ́s not black and white. There are always exceptions to this, but most exceptions prove the rule.
Event Sourcing allows you to replay events to rebuild state from event streams anytime. In Event Streaming this is not practical, as typically the number of events is simply too high to do this efficiently.
I’m tempted to not even use the term “Event” in Event Stream- ing. Most often, they are just notifications to carry information from A to B. The term often used here is “event-carried state transfer” defined by Martin Fowler8 . You use the stream records to transport information between systems in a reliable way.
One tool very often used and discussed when talking about Event Streaming is, of course, Apache Kafka. Kafka is an amazing platform with a vast ecosystem of tools and frameworks to support near-real-time Event Streaming for any platform.
Very often, I get the question, “We have Apache Kafka in place. Can I simply use Kafka as our Event Store?” At first glance, this might look like a good idea. Kafka guarantees the ordering of events to a certain extent, has strong support for serialization using Avro, JSON, or any other format, automatically scales horizontally, and generally operates lightning-fast. And it can work with Events.
So, should we use Apache Kafka as an Event Store?
8 https://martinfowler.com/articles/201701-event-driven.html 36
 
EVENT SOURCING - WHAT IS IT?
The answer is a clear No. Kafka is not built for this.
You can only read all records from one topic. There is no way to only read records for a certain “stream.” Filtering millions of records is not a problem for Kafka, but it simply will not scale to do it in real time.
You would need to configure a new topic per stream or aggregate you want to use. While this could work technically, it comes with a huge organizational overhead to manage the whole process and schemas per topic.
There are event-sourced projects that rely on Kafka and most of the time Kafka Streams, but they are the exception, not the rule, according to my experience.
Misconception #5 - Event Sourcing needs special tools
This is the last of the most common misconceptions and myths we’ll try to debunk in this chapter. And this one is really common. It’s generally assumed that Event Sourcing requires a lot of special tools to work with:
• EventStore
• EventSourcingframework
• StrongCPUandhighmemory
And again, this is not true. Event Sourcing is very simple and basically just asserts that we do not store state but state changes
37

UNDERSTANDING EVENTSOURCING
in the database. Event Sourcing works basically with any kind of database, be it PostgreSQL, MongoDB, or DynamoDB. Some databases are better suited, some are less suited, but basically, as long as you are capable of storing events in an events table and keeping them in order, you are good to go.
Right now, I’m working on a project where we do not use any special Event Store but only have a PostgreSQL database in the background for both our events and our projections. It’s a simple relational database with just a handful of tables.
How we store events is a matter of the serialization format. Oftentimes, events are just serialized as plain JSON, oftentimes backed by a JSON schema. Other options include XML or Avro (which is a special purpose binary-format). And as we defined earlier in this chapter already, you only need a handful of columns to store events.
Since in Event Sourcing you neither modify existing events nor delete any typically, relational databases can be extremely performant in storing and processing these events.
Does it make sense to use a framework? As always, it depends. If you work with a bigger team (> 3-5 people) on a commercial project, I would typically recommend at least evaluating if using a framework like Axon makes sense. It simply has validated solutions to known problems and recipes for known obstacles. You just don’t have to reinvent the wheel time and time again.
Greg Young has stated on several occasions that building Event 38

EVENT SOURCING - WHAT IS IT?
Sourcing frameworks is doomed to fail9. If your company has bet on the wrong horse, you are trapped with an unmaintained Event Sourcing framework as the backbone of your business.
Of course, that can also happen with the bigger ones like Axon, but it’s just much more unlikely. In the end, it’s a project-by- project decision, and I would recommend making an informed decision after a thorough analysis. The good thing, though, is that switching frameworks is not free, but also not impossible. Using some migrations for existing events takes you pretty far.
Conclusion
We did not talk much about technology in this chapter because Event Sourcing is not really a technical topic. There are many ways to implement Event Sourcing, but the important aspect is not the technology itself but the mindset for how to handle data.
One of the most difficult things when adopting Event Sourcing is not the underlying technology but unlearning what you have learned over years of “CRUD-based” software development.
It requires a significant mindset shift.
If you start with your existing data model and just want to “event-source” it, you are very likely to fail. Event Sourcing is about recording data as it flows through the system. Just because
9 https://codeopinion.com/greg-young-answers-your-event-sourcing-que stions/
 39

UNDERSTANDING EVENTSOURCING
you assume you know what a customer looks like in your system does not mean you are correct. Instead of storing a customer using typical attributes like name, surname, contact data, and a list of addresses, you use events like “Delivery Address Added,” “Contact Address Added,” and “Contact Data Updated.” You focus much more on the data and how and where it is used.
This gives you freedom and flexibility in handling the data for other use cases as well. You do not limit yourself to the customer object but see the customer as a natural stream of events (which has nothing to do with streaming) that records what happened to this exact customer in the past.
Whatever happens in the system, you need to record it as an event. If there is no event, it didn ́t happen. All events in sequence make up the story of your system. It should be readable by both developers and your business team, event by event, and it should tell a compelling narrative of your system’s capabilities.
With that being said, there is a great tool that allows you to write this narrative in a highly visible way. It’s called Event Modeling, and we will talk about this in detail right in the next chapter.
40

3
Planning Systems using Event Modeling
How do you reason about systems? What is the best way to discuss what a system does and how a system works? Most developers will have a hard time providing a proper answer to this question because many have had bad experiences in the past with the tools available.
What is a System?
What is a system, anyway? All information systems essentially work the same way: they implement one or more processes, each consisting of multiple steps executed in a specific order.
What’s fascinating is that even the most complex process can be broken down into simple steps. The more complicated the process, the more steps are typically involved. Each step has a trigger, which can come from various sources. It might be a direct user action (“User clicked a button”), an event within the system (“Account was logged in”), or even just the passage of
41

UNDERSTANDING EVENTSOURCING
time (“Process timed out”).
At its core, any process can be described using atomic building blocks. These building blocks either modify the information in the system or somehow use the information already present. Some steps require human interaction via a user interface, while others happen automatically in the background. Additionally, some steps may notify external systems, which in turn again trigger other processes.
Viewing processes as a continuous flow of actions and reactions reveals a “wave”-like structure, as illustrated in Figure 3.1. Each action triggers a reaction, creating a rhythmic pattern of interactions that drive the system forward.
Fig. 3.1 - Typical “Wave Structure” of a Process
Describing processes and how a system works with words is difficult. It’s difficult because words require interpretation, and
 42

PLANNING SYSTEMS USING EVENT MODELING
interpretation is always subjective. It is very easy for two people to have different understandings of how a system is meant to work based on the same words, and they will never know.
The best example I can remember happened in an Event Mod- eling Session in spring 2024 in Vienna conducted by Adam Dymitruk, where 2 people were in a heated debate about how to model a certain aspect of the system. The real problem was, they were discussing completely different things without know- ing. We could resolve that very quickly by drawing the screen mockups. Suddenly everybody realized, that the discussion was pointless and we moved on.
In my opinion, this is the biggest reason why so many IT projects fail.
Let me give you another example that happened just recently in a customer project I am supporting. We had several meetings where we discussed how data can be retrieved from a complex database schema using a technology capable of translating databases to Kafka Records (using Debezium and Change Data Capture, but that is not really important).
In these discussions, the business side mentioned several times that they were working on something they called a “default set.” The developers never really understood what a default set was and how it should work. So they tried to describe that they only needed a “clear defined dataset based on record IDs.”
The business side again did not understand what that meant, so they all agreed that a default set was probably exactly the
43

UNDERSTANDING EVENTSOURCING
record set the developers needed. Of course, it turned out several meetings later that this was absolutely not the case, and for several meetings, people were eagerly discussing how to implement it without ever realizing that they were talking about two completely different things.
Miscommunication, misunderstandings and wrong assump- tions make projects fail. And it ́s almost never intentional. Most people desperately want to see projects succeed. It’s a fundamental problem in human communication we are facing.
One of the oldest approaches which tries to solve the communi- cation gap is UML - the unified modeling language defined by Grady Booch (et. al) around 1990. It’s a great tool even today, but unfortunately it was never really translated to the business side.
Maybe you have also heard about Domain Driven Design (DDD) by Eric Evans. We will talk about DDD in detail in chapter 8. At its core, DDD is all about defining a common language between Business and IT. Eric Evans famously called it the “Ubiquitous Language”.
The idea is simple. Try to find a language understood by all people involved in the project, that allows us to talk about the requirements in an unambiguous way. A language which does not contain technical jargon and only business terms clearly understood and well documented.
As simple as it sounds, it is actually very hard to define this language.
44

PLANNING SYSTEMS USING EVENT MODELING
Alberto Brandolini famously said, “It’s developers’ understand- ing, not your business knowledge, that becomes software”10. And of course, he is absolutely right about it.
So what can we do?
Should we dedicate additional time to precisely clarifying the meaning of our requirements? Would hiring more requirements engineers help bridge the gap between technical and business perspectives? Requirements engineers face the same challenges as we all do; both written and spoken language often fall short when it comes to accurately describing complex technical systems, regardless of one’s job title. We need a more effective solution. Fortunately, there are tools available that can assist us in this endeavor.
If you wanted to explain to a colleague how a system works, the best way is to sit down together and directly show on the screen how it works by going through the real system step by step.
What if we could use a visual language that works similar to “sitting down quickly together at a screen”. A language understood by everyone immediately and detailed enough to not allow any subjective interpretation, but instead discuss only facts?
But what if this even works when the system does not exist yet?
10 https://x.com/ziobrando/status/1347126001340358656 45
 
UNDERSTANDING EVENTSOURCING
Event Modeling
Event Modeling is a visual approach designed to facilitate discussions about data flow within an information system. You can think of it as a collaborative process where you and a colleague sit down and illustrate the system’s operations from left to right using real examples. It resembles the experience of reviewing an existing system together on a screen to clarify its functionality.
Fig 3.2 - a complete Event Model
Event Modeling was invented by Adam Dymitruk11 and combines the best elements from Alberto Brandolini’s Event Storming12, Domain-Driven Design, UML, Figma for UX and many other accepted best practices in the industry in a unique tool to plan
11 eventmodeling.org
12 https://www.eventstorming.com/
  46

PLANNING SYSTEMS USING EVENT MODELING
information systems.
Event Modeling works on a single timeline, and we model the steps a system provides from left to right. You can read “what happens in this system” like a story. It’s a perfect tool to describe requirements, but also very suitable as documentation to support onboarding new colleagues.
The only real tools you need for Event Modeling are a whiteboard and plenty of sticky notes. We use sticky notes in different colors—blue, orange, green, and yellow—to visually capture and organize information on the whiteboard, making it easy to model and communicate the system’s behavior.
Elements of Event Modeling
Event Modeling defines a few key elements we use to describe how systems work. First, we have Events, the foundational building block of all information systems.
Events
47

UNDERSTANDING EVENTSOURCING
 Fig 3.3 - Events in orange
Events essentially represent occurrences within the system, such as “Customer Registered.” By examining these events, you gain a clear understanding of the system’s capabilities. Events reflect what remains of the system when it is powered off and then on again. They document the data that has been stored and the actions that have taken place in the past.
A common objection I often encounter is, “We can’t use Event Modeling because we don’t have an Event-Driven Architecture.” However, any system can be modeled using Event Modeling; it doesn’t need to be event-driven or event-sourced. In this context, an “event” signifies that data has been persisted within the system. It does not make any assumptions about the method of persistence. It’s perfectly acceptable to use an event to indicate that data was saved in a “Customer Table” within our relational database.
48

Commands
PLANNING SYSTEMS USING EVENT MODELING
 Fig 3.4 - Commands in blue
Commands define the input parameters of the system. A command is basically an instruction to the system to carry out a certain action, like “Register Customer.” We could imagine there is a button to register a new customer for an online shop, and as soon as the user clicks that button, the system is instructed to execute the registration step.
How the system does this is a complete black box. We do not really care for the moment. We only want to know that the system has this action “Register Customer”. The result of a successful command execution always is an event that records in the system, that the customer has been registered.
Commands describe what should happen, events describe what 49

UNDERSTANDING EVENTSOURCING
actually happened.
Screens
In Event Modeling, we want to describe how systems work as clearly as possible. To be absolutely sure everybody knows what is being discussed, we additionally work with rough screen mockups to visualize the system behavior. This is the UX part.
Screens or wireframes basically show how a UI for the system might look like and how a User can interact with the system.
This is a point of confusion for many: should we really invest time in designing screens at such an early stage of the process? The answer is yes, but the level of detail should be minimal. The focus isn’t on how the screens appear or creating the perfect user experience; it’s primarily about the data. The extent to which you delve into UX details largely depends on who is involved in the discussion. If UX professionals are present, they will undoubtedly focus on the finer points of user experience. What you want is a highly visual representation that illustrates the data we collect in the system and how that data triggers specific actions through commands.
My screens typically look like this, and it’s clear that they aren’t something you’d want your customers to see in a real system.
50

PLANNING SYSTEMS USING EVENT MODELING
 Fig 3.5 - Screen Mockups
And they never will. But it’s perfectly fine to use a screen like this to discuss how the system basically works.
And it’s important to make sure everybody understands exactly what is being discussed. Highly visual people sometimes get confused if there is nothing visual, but everybody, including business, basically understands a simple screen mockup.
State Change Pattern
So what happens if we combine our screen with the command and the resulting event?
51

UNDERSTANDING EVENTSOURCING
 Fig 3.6 - State Change slice with Command and Event
We receive one of the four main patterns in Event Modeling. This is called a “State Change” and is the only way to trigger change in a system.
A command gets triggered by a click on a button or any other user interaction. The command triggers a business action in the system, and the result is an event, because we need to record that something has happened.
Now we know how to get data into the system. But how do we pull data from the system? Where do we get the data to fill the screens we draw with the necessary information?
That is what a Read Model is used for. 52

PLANNING SYSTEMS USING EVENT MODELING
Read Model
A Read Model (often also referred to as a Query) is displayed as a green sticky note and represents a query against the already stored events in the system.
Fig 3.7 - Read Model
A Read Model can technically be as simple as a SQL query against a relational database. It’s just a way to represent structured data that gets pulled out of the system.
Read Models can be used to feed screens, but also to feed background processes in a system, as they also rely on data.
The only data you can query is data from previously stored events. That’s crucial, as you cannot query things that are simply not present in the system. We will talk about that later in more detail when we discuss the information completeness check.
 53

UNDERSTANDING EVENTSOURCING
State View Pattern
What happens when we combine events with Read Models? We get a “State View”, the second Event Modeling Pattern. With State Views, we allow a screen or a background process to query information from the system ( without describing how ).
Fig 3.8 - State View slice with Event and Read Model
 54

PLANNING SYSTEMS USING EVENT MODELING
Automation
Not all actions in a system are a direct result of a user interaction. Some processes get triggered automatically and then run silently in the background.
Fig 3.9 - Automation Pattern
In Event Modeling, this is called automation and is represented in the model by a gear symbol. Automation is any kind of background process that gets triggered automatically by an Event, a Timer or a User Interaction. One example is an email that gets sent as soon as a user is registered in the system. This
 55

UNDERSTANDING EVENTSOURCING
process typically runs in the background and is triggered by the fact that a user was registered and the presence of the “User Registered” event.
An automation is basically just the combination of a State View, a State Change and a gear symbol for the process.
So far, we have learned how to store data in the system via state changes, how to pull data out of the system via state views, and how to model background processes via automations.
Only one pattern is left: the answer to the question of how we communicate with external systems.
Translations
56

PLANNING SYSTEMS USING EVENT MODELING
 Fig 3.10 - Translation Pattern
Software systems are rarely built in isolation. Typically, we need to communicate with external systems, whether they are third-party or other services in our stack. In the simplest case, for example, just sending emails via AWS SES (which is the E- Mail Service of the Amazon Cloud) is a communication with an external system. For this to work, we need a way to model the communication independently of the underlying technology.
It turns out we already have everything we need to model this. We just have to introduce one more component: the External Event.
57

External Event
UNDERSTANDING EVENTSOURCING
 Fig 3.11 - External Event
The external event represents data that comes from an external system. This can be an API call, an incoming Kafka record, or even a simple CSV file that we load from a network storage. The details do not matter.
We typically need to translate the data from the external event to a format suitable to our needs. The pattern used for this is called “Translation”.
This is the only pattern where there is some variance in how it is modeled in the system. We can either represent the external events directly as a Read Model and do the translation under the cover (Fig. 3.12, right side), or we can model it basically as a State Change where the external event is explicitly translated and stored as a new internal event in our system via a simple State Change (Fig. 3.12, left side). I typically tend to do the latter, but if the external data completely matches the data necessary and is only displayed, sometimes the former is the simpler choice.
58

PLANNING SYSTEMS USING EVENT MODELING
 Fig. 3.12 - Translation Pattern Variations
Information Completeness Check
What Event Modeling provides is a clear and concise language consisting of only four patterns to describe the single steps in a software system in an easy-to-read manner. Not only that, it also automatically provides one crucial tool to verify and vali- date assumptions immediately: the information completeness check.
We learned earlier that Read Models must only ever read data that is already stored in the system in the form of events. This ensures that we never assume data to be available without verifying that this is truly the case. One crucial aspect of why software projects get delayed is false assumptions about data.
59

UNDERSTANDING EVENTSOURCING
We assume data is available and only realize it’s not when we start with the implementation of the system. This is where projects get delayed. It’s too easy to rely on wrong assumptions.
The information completeness check forces you to look at every single attribute in a Read Model and verify the source event of the data of this attribute. Which event delivers this exact data? If you have an “email” attribute in a Read Model that has no corresponding data equivalent in an existing event or a command, you can’t proceed until you clarify this assumption.
The information completeness check not only works for state views but also for state changes using commands. Whenever you model an attribute in an event, you automatically need to make sure that the data is provided by the command that triggers the event. Commands generally have to provide all data necessary to persist an event.
Given / When / Thens
In addition to the four patterns we already learned (State Change, State View, Automation, Translation), we also define business rules in the form of “Given / When / Then” (GWT). This comes from behavior-driven development13 and basically mimics simple business rules in the form of “GIVEN something has already happened, WHEN this new thing happens, THEN we expect the system to be in this new state.”
13 https://en.wikipedia.org/wiki/Behavior-driven_development 60
 
PLANNING SYSTEMS USING EVENT MODELING
This is a perfect match with Event Modeling, as it allows us to describe State Change- and State View Patterns in much more detail than any text ever could.
We typically put the GWT vertically under each of the patterns so you can still read the whole model from left to right. If you want to know more about a certain pattern, you can stop and go down vertically to learn about the business rules of this small part of the system.
Let’s look at an example of a simple state change. Assuming we have a Person, this Person can have up to three addresses at most. Defining the State Change to add an address to a user is simple.
Fig 3.13 - “Add Address” State Change
Now, how would we describe the business rule that a user can 61
 
UNDERSTANDING EVENTSOURCING
have three addresses at most?
Define a GWT with the name “users can have 3 addresses at most” directly below the State Change. We only put it here to the right for better readability. This immediately captures this rule in a way that is understandable for everyone and also directly provides a real example that can be translated into a unit test in the running system.
GIVEN three addresses were already added, WHEN the user tries to add another address, THEN the system should raise an error.
Fig 3.14 - Given / When / Then
 62

PLANNING SYSTEMS USING EVENT MODELING
For some more complicated parts of the system, we could define up to 10 or more GWTs for a single State Change to describe all business rules that apply. Don’t save on GWTs; they are a perfect opportunity to describe business rules in detail.
If you want to describe how a Read Model projects data to a view, you typically do not use GWTs but GTs (Given - Then). Read Models only rely on previously stored events, so there is no “When” part necessary.
The GT for the Read Model used to retrieve the users’ addresses would look like this.
GIVEN an address was added, THEN I expect the Read Model to show the same data as from the event.
Fig 3.15 - Given / Then for Read Models
 63

UNDERSTANDING EVENTSOURCING
The combination of GWT and GT allows you to completely describe any part of any software system visually using Business Rules you would normally try to describe in a JIRA Ticket.
The big difference is that written text allows interpretation, while GWTs provide facts.
It requires discipline and some exercise to always provide meaningful GWTs. I would really recommend defining them together with the business side. Then you get the best of both worlds. You have the real business rules in a form directly translatable to software.
I personally use code generation in combination with Event Modeling to generate running specifications from the GWTs.
When to use Event Modeling?
We will dive much deeper into how to apply Event Modeling to a real world problem in Part II of the book. Now, having learned the basics of Event Modeling, the big question not really answered is when to use it and more importantly when not to use it.
In general, I cannot imagine any software system where it wouldn’t be hugely beneficial to model the system upfront as a base for discussion.
64

PLANNING SYSTEMS USING EVENT MODELING
Documenting an existing system
Event Modeling is a perfect opportunity to document an exist- ing system in a way so that everybody understands the inner workings of the system without ever talking about technology.
This also allows for faster onboarding, as new people in the project do not have to read tons of documentation, often without knowing what the relevant parts and what the outdated parts are. You can be sure that the event model shows the current state of the software, if it is actively used in development. This is automatically the case when you force yourself to work model- first. Whenever you change something in the system, you always start with the model, adjust the model, and then you go to code. This works even better if you use the model to generate parts of the code (or all of it) because then you simply force yourself to start with the model.
You can document any system using Event Modeling. The system does not have to be event-driven or event-sourced in any way. You are basically documenting how data flows through the system. Data flows in any kind of system.
Planning a New System
Sometimes you do not have an existing system, but you are thinking about developing a completely new system to solve a problem that may have been bothering your company for
65

UNDERSTANDING EVENTSOURCING
months, maybe even years.
The big question in the room is: what should this system do?
Instead of endless discussions without ever knowing if the thing being discussed is completely understood, it’s much easier to talk about the facts in the system using Event Modeling.
This allows you to learn about the new capabilities step by step long before writing the first lines of code. It’s even possible to create prototypes directly from the event model to get a feeling of how the new system will behave and simply how it feels.
Planning a system is absolutely crucial and should account for about 60% to 70% of the overall effort of the initial creation of the system and even long-term considering feature develop- ment and maintenance.
Having plans allows your company to be flexible and adjust the plan if necessary. Ideally, you plan out the whole flow without writing a single line of code. This does not mean that the initially modeled flow will be the one that will be implemented in the end; it won’t. But you start discussing concrete problems early and focus on the things at hand from the very first day of the project. Do not underestimate the power of focus.
Do not postpone the details to the end of the project. Getting into the details as early as possible literally pays off every single time.
66

PLANNING SYSTEMS USING EVENT MODELING
Preparing Refinements
Event Modeling is not only suitable for planning complete projects. It’s also a very good tool to plan smaller increments, even if the overall project is not using Event Modeling yet. You can quickly model out single user stories together with the stakeholders and make sure you have all the information necessary to start with the story and plan it for the next sprint.
Discussing Complex Problems
And it does not even have to be a user story. Every time you face a complex problem, I tend to quickly open a Miro Board and sketch out an Event Model for the problem to make sure everybody has the same understanding. And people love it. They love to work on concrete business problems and are typically very receptive to the graphical input they get from using Event Modeling.
Conclusion
Remember the earlier example in the chapter where we ran into confusion over “default sets”? How could Event Modeling have helped in that situation? Instead of focusing on the things in the system and their attributes, assuming everyone shared the same understanding, we would have modeled the data flow and broken down the process into its atomic steps. By doing this,
67

UNDERSTANDING EVENTSOURCING
we would have initiated discussions at each step, allowing us to clarify what “default set” actually meant in the context of the process. When you discuss behavior and processes step by step using a visual language, it becomes much harder to make incorrect assumptions.
Remember the “wave”-structure we used to think about pro- cesses at the beginning of the chapter? Using Event Modeling we uncover the atomic building blocks of each process and can visualize the “wave”-structure in any system.
Fig. 3.16 - Uncovered Wave Structure using Event Modeling
It’s easy to get lost in incomplete or, even worse, missing re- quirements. It’s hard to keep track of every data flow in a system when the knowledge is spread across numerous Confluence pages and JIRA tickets. It’s hard to really understand the data flow in a system when all you have is outdated documentation
 68

PLANNING SYSTEMS USING EVENT MODELING
and legacy source code to study.
Nevertheless, this is the reality we are facing in most projects still in 2024 and beyond. It’s not necessarily Event Modeling that will solve all these problems; we have numerous tools at hand, which all have the same goal: increasing the understanding of system processes and improving communication between all parties involved in a project. Be it Event Storming, Event Modeling, or Domain Storytelling, they all have a common core goal: to improve the status quo in the IT industry. We can do better.
For this book, I chose Event Modeling because, in my opinion, this concept is closest to the actual real implementation of the system. This chapter gave you a brief overview of how Event Modeling works and what the core patterns in it are. This is not enough to make you an Event Modeler. You need some experience, and you need to model some systems and we ́ll do exactly that in Part II of the book.
We will model a system from scratch and step by step. Here is my recommendation - use a free Miro Account and try to model the system yourself chapter by chapter along with me, based on the requirements you ́ll learn in Part II and compare it constantly with how I modeled it. Its completely fine if you model things differently than I do. This is normal and expected, just try to find your way to model things.
Get yourself familiar with the patterns and how to apply them in different situations. Event Modeling is simple, but you need to practice it.
69

UNDERSTANDING EVENTSOURCING
If you want to dive deeper right now, I would recommend you visit https://www.eventmodeling.org or, if your prefer German, https://www.eventmodelers.de. Here you will find plenty of information about the topic and also practical guidance on how to apply the patterns.
If you want to take a quick look at the Event Model we will create in Part II, you ́ll find it here:
https://miro.com/app/board/uXjVKvTN_NQ=/
In Part II of the book, you’ll learn how to apply all the concepts we’ve briefly touched on in this chapter. I hope this introduc- tion has sparked your interest and left you eager to dive into modeling systems right now. But before we get there, we need to explore a few more topics to deepen our understanding of event-sourced systems. One particularly interesting area is concurrency, consistency, and how these concepts relate to something called CQRS. That’s what we’ll be discussing in the next chapter.
   70

4
CQRS, Concurrency, (eventual) Consistency
The one thing that makes reasoning about event-sourced sys- tems more difficult for many developers is not how state gets processed from events, but the combination with CQRS - the Command / Query Responsibility Segregation.
CQS / CQRS
Command-Query Separation (CQS) is a fundamental design principle for object oriented systems introduced by Bertrand Meyer, aimed at improving the clarity and reliability of software systems. CQS revolves around the idea that every method in a system should be classified as either a command or a query, but never both.
A command represents an operation that alters the state of the system. Examples of commands include actions like updating
71

UNDERSTANDING EVENTSOURCING
a user profile, processing an order, or adding an item to a shopping cart. Commands are responsible for changing the internal state of the system but do typically not return data in response to the caller. The focus is purely on side effects— making sure something changes in the system as a result of the command.
A query, on the other hand, is an operation that retrieves and returns data without modifying the system’s state. Examples of queries include fetching a user’s information, retrieving an order history, or displaying a product catalog. Queries are concerned only with data retrieval and have no side effects.
CQS is primarily focused on code elements like classes, inter- faces and objects.
What is CQRS? Isn ́t that the same? CQRS takes the concepts defined by CQS and moves them from the code level to the architectural level. CQRS stands for Command-Query Responsi- bility Segregation, and it defines a pattern that separates how data is written to the system (commands) from how data is read from the system (queries). By using CQRS, these two operations are handled by distinct models, often even using different technologies, though this is not a strict requirement.
This separation makes a lot of sense when you consider how data is typically managed in most systems. Writing data (e.g., updating an order, creating a customer profile) involves differ- ent challenges than reading data (e.g., fetching order history or displaying a product catalog). In fact, for many systems, reading happens far more frequently than writing, so oftentimes it’s
72

CQRS, CONCURRENCY, (EVENTUAL) CONSISTENCY
crucial to optimize the read side for performance.
A few years ago, I encountered a real-world example where CQS (Command Query Separation) wasn’t implemented. The system had no distinction between the read and write sides, both relying on the same normalized relational database schema. Writing data required saving information across numerous tables, while reading involved joining all those tables back together. This led to thousands of queries just to generate a simple page view whenever a customer refreshed the browser. Rather than addressing this inefficiency, many discussions focused on purchasing more powerful hardware. Unfortunately, optimizing only the read side wasn’t an option; we had to refactor the entire system.
Imagine our example we will use later in the book. An e- commerce platform where customers frequently browse prod- ucts, but actual transactions (adding to cart, placing orders) happen far less often. On the read side, we could pre-calculate product availability, pricing, and customer reviews, and store this data in a fast, read-optimized database tables or complete- ley different systems like Redis. Instead of executing complex joins across multiple tables in a relational database, the system can directly fetch the pre-calculated data, providing lightning- fast access to product pages and search results.
On the write side, when an order is placed, it might go through a more complex process involving multiple validations and updates across several tables (e.g., inventory, customer data, and order status). This process doesn’t need to be optimized for instant access since writes happen less frequently than reads.
73

UNDERSTANDING EVENTSOURCING
Thus, the write model can be more comprehensive, ensuring the integrity of the data being persisted.
One of the biggest challenges with CQRS is ensuring that the two models—the write model, which records data, and the read model, which presents data—stay in sync. Since these models are separate, there must be a reliable mechanism to propagate changes from the write model to the read model. One common approach (but by far not the only one) is to rely on event-driven architecture, where the write model emits events whenever data is modified. These events are then consumed to update the read model. For example, when a new order is placed, an event like “Order Submitted” would be triggered, and the read model would update its records to reflect this new order.
In summary, CQRS optimizes both read and write operations by using distinct models for each, allowing systems to fine-tune performance where it’s most needed. This separation of con- cerns enables systems to scale efficiently, particularly in high- read, low-write environments. However, it also introduces the challenge of maintaining consistency between the two models.
In Part III of the book, we will implement a few real-world examples using CQRS, using a strict separation between write side ( storing Events ) and the read side ( projecting data ).
74

CQRS, CONCURRENCY, (EVENTUAL) CONSISTENCY
Consistency
Consistency, as defined by Merriam-Webster, is “agreement or harmony of parts or features to one another or a whole”14
According to this definition, our system is consistent, if all parts operate in harmony and, in our case, show the same data.
For many projects, the simplest solution to update all parts of the system at once is to just use one datastore for write- and read-operations, which is neither forbidden, nor a bad practice.
If you’re utilizing a PostgreSQL database to store your events as well as your projections, it’s often sufficient to update every- thing within the same transaction. This approach guarantees consistency and simplifies the system a lot, making it easier to understand and reason about it. Don’t underestimate the advantages of simplicity. I would only deviate from it if you have very good reasons.
 14 https://www.merriam-webster.com/dictionary/consistency 75

UNDERSTANDING EVENTSOURCING
 Fig. 4.1 - Updating Projections in the same transaction
Keeping everything in sync becomes more complicated when you have different systems that need to be updated simultane- ously.
If we write our data to a relational data store but read our data from Redis for faster access, to keep our system in a consistent state, we need to make sure that every write to the relational tables updates our pre-calculated views in Redis simultaneously.
76

CQRS, CONCURRENCY, (EVENTUAL) CONSISTENCY
 Fig 4.2 - Writing to both datastores
Of course, this is possible, but what happens if Redis is not reachable for a second exactly at the moment the write takes place? Or even worse, what happens if Redis is reachable but very slow because of a short network latency?
What if the write request to Redis times out and we simply do not know if the request was processed successfully? We could retry, as a write to Redis is typically infinitely repeatable for the same key; it would not harm. But what happens to the running transaction for the write to our primary relational system? Should we keep it open? What if we need to retry not once but for some minutes?
Even worse, what if we keep the database transaction open for too long and it times out? This would result in a rollback of the original data, but what if Redis, in the meantime, received the data as the network is reachable again?
These are all typical scenarios that you need to consider if you 77

UNDERSTANDING EVENTSOURCING
want to build a consistent system. It’s hard. Many things can go wrong, and the worst thing you can have in a system is inconsistent data.
And we are not done, it ́s even worse. What if we do not have only Redis as an “optimized system for reads” but also Elasticsearch for reporting, a CSV for the SAP system export, and another relational table to be easily read by the Kafka cluster? Would we try to keep all of them in sync? The more systems we try to keep in sync, the more likely we will run into one of the problem previously described.
Fig 4.2 - Keeping multiple data sources in sync
Once the data is out of sync, it’s typically pretty hard to get it back in sync.
It ́s almost impossible to keep a system like this in sync without a “single source of truth”. If in doubt, you need something that
 78

CQRS, CONCURRENCY, (EVENTUAL) CONSISTENCY
gives you the latest and correct data. If your system gets out of sync, you need a safety net to rely on, that you can use to bring the system back into a consistent state.
In our case, it is the Event Store.
If one of our views gets out of sync, we can just replay the events from the Event Store and hopefully, we are back in a consistent state after that. We will talk about Event-Replays in detail in chapter 28.
Eventual Consistency
The problem is not easy to solve if we try to keep all systems up to date at the same time and writes to one part of the system are immediately propagated to all other parts of the system in real-time. If you try to do that, you have to find solutions to all previously described problems in your system.
The concept of eventual consistency simplifies the problem as we define that the system and all views will reach a consistent state, just not immediately. It relaxes the requirement for all views of the data to be updated at the same time. Instead, updates propagate over time, and the system ensures that all views will eventually reflect the correct state at some point in the future. This trade-off allows for greater scalability and performance in distributed systems, where immediate consistency might be too costly or unnecessary for certain use cases.
79

UNDERSTANDING EVENTSOURCING
For most systems, cost and effort to achieve immediate con- sistency do not justify the result. Most systems actually do not even have the requirement to be immediately consistent, it just feels better. In reality, most of the time it does not really matter if the Redis Cluster receives the data a few milliseconds after the data has been written to the relational tables.
How does this solve the initial issues?
By accepting a delay between the write and read sides, most of the original problems vanish. Instead of coupling them tightly, we can asynchronously sync a view (like Redis) with the relational tables in the background.
Here’s a concrete example of how this might work:
Let’s say we store customer registrations in a “customer_regis trations” table. We can leverage a Change Data Capture (CDC) system, which is an excellent tool for these situations. A CDC system is triggered whenever there’s a change in the source (like a table), and it propagates the update to one or more target systems that are interested in it.
For instance, using CDC, changes in the PostgreSQL “customer_ registrations” table can be propagated to an Elasticsearch index. This index would then automatically synchronize with the newly received data, enabling fast, efficient querying on the read side without burdening the original relational database.
80

CQRS, CONCURRENCY, (EVENTUAL) CONSISTENCY
 Fig 4.3 - Using a CDC to propagate changes in the system
This will happen automatically and the process will ensure that all records are correctly stored in all connected data sources. But it will not happen in real-time. There will always be a small delay.
Why is that important? Most developers are not used to think in eventual consistencies. We are trained for years to think in consistent systems and state. You write something and you can immediately read it again, often using the same transaction.
Think of a simple UI where the user can add items to a shopping cart. If a user clicked the button, you would naturally assume that the data for the shopping cart is immediately available. So it feels natural to just give the complete updated shopping cart back in the same request/response cycle. But in this case, even a delay of a few milliseconds might result in stale data. The user does not see the shopping cart with the updated items, but
81

UNDERSTANDING EVENTSOURCING
the shopping cart as it was before. Only after a reload does the customer see the real data.
The next time you buy an ebook on Amazon, you can see this in action if you go to your “digital contents” page immediately after the buy. The book will only be visible after about thirty seconds. Your digital contents are eventually consistent.
Event Sourcing and CQRS
How do Event Sourcing and CQRS correlate? In most tutorials and articles, CQRS and Event Sourcing are combined and seem like a union, which is misleading. You can implement CQRS perfectly fine without using Event Sourcing. On the other side, when you use Event Sourcing it is almost certain that you will be using CQRS.
A perfect example is our “customer_registrations” table men- tioned earlier, which propagates changes to Elasticsearch using Change Data Capture (CDC). No Event Sourcing here, yet it’s still CQRS.
If you are using Event Sourcing, a big portion of your work building systems is defining use case specific models or pro- jections from the underlying event stream. Imagine we filled the Customer-Registration projection in Elasticsearch directly from the underlying event stream.
The Write-Side handles the Event storage in the system via a clearly defined data path. The Read-Side asynchronously fills
82

CQRS, CONCURRENCY, (EVENTUAL) CONSISTENCY
the Customer-Registration Projection in Elasticsearch in the background.
The key thing to understand is that we can create any represen- tation of our data from the events stored in the system. We’re not limited to static views or predefined tables. Instead, we have the flexibility to define optimized data views tailored to each specific use case, all based on the events within the system.
Another thing we haven ́t touched so far that becomes important in distributed systems is concurrency.
Concurrency
So whenever we deal with multi-user systems or parallel sys- tems, we need to also think about concurrency.
Concurrency: operating or occurring at the same time. 15
If two users want to change the product description of an article in an e-commerce application at the same time without any concurrency control in place, the last write will win. This can have severe consequences and lead to problems that are extremely hard to find.
Most systems have to deal with some kind of concurrency. If you use Git as a version-control system, you are dealing with concurrency daily. If two developers work on the same branch and make changes at the same time, only the first developer is
15 https://www.merriam-webster.com/dictionary/concurrent 83
 
UNDERSTANDING EVENTSOURCING
able to push the changes. All other developers get errors like “non-fast-forward, fetch first.” Developers can ́t push if their data is not up to date. The only way to solve this is to update to
the latest data and retry.
One simple way to deal with concurrency is optimistic locking. Optimistic locking prevents write operations if any other write operation (or push in git) has happened in the background without you knowing.
Every developer needs to update to the latest state, or “git-pull” first to be able to push again.
Optimistic locking is also well-known strategy if you work with relational databases. Here you typically define a “version”- column which gets updated with every write to the table in the same transaction.
Whenever you want to write to a table, you compare the version you received when you initially fetched the data with the version that is currently stored in the system. Only if these versions match do you execute your write operation and commit the transaction. This prevents you from having to lock the table completely and still makes sure, that no data gets accidentally overridden.
Why is this optimistic? Because by default we assume that the table has not changed; we do not technically lock the table and force everybody else to wait. I guess we all heard the horror stories of locked tables by people now on vacation for 6 weeks (or even worse retired).
84

CQRS, CONCURRENCY, (EVENTUAL) CONSISTENCY
 Fig 4.4 - Optimistic Locking preventing accidental overrides ( “Andreas” was not stored )
In event sourcing, we typically use optimistic locking to handle concurrency. You can write new events to the Event Store, but only if no other events have been written to the same stream in the meantime.
But how does this work when multiple developers are writing to the same Event Store? Wouldn’t this create constant conflicts? This is why we apply optimistic locking not on the entire Event Store, but on individual event streams (we introduced event streams in chapter 2). It’s the same principle as avoiding optimistic locking on the entire database and instead applying it to specific tables.
For example, in our customer_registrations scenario, a list of customer registrations could be represented as a stream of events in the Event Store, identified by a specific stream Id like
“customer_registrations_1234.”
Each time we store an event in the Event Store, the version of
85

UNDERSTANDING EVENTSOURCING
the affected event stream is automatically incremented. The optimistic lock here is typically based on the index of the last event written to the stream. Since event streams are append- only, the index automatically increases with every event.
Let’s say we fetch all events from a customer registration stream and find that the last event was “customer registration completed,” with an index of 100. If we now want to add a new event, such as “Customer Registration Notification Sent,” it would be assigned version 101. However, if the Fraud System has updated the customer registration twice in the meantime (for example, flagging the customer as blacklisted due to detected fraud), the stream version would already have advanced to 102..
Fig 4.6 - Customer Registration Process
We will not be able to write to the stream and will need to refetch all data for the customer registration for version 102 and apply our changes again. Instead of 101, our Event will have the index 103. We are optimistic (pun intended) that we will not run into this issue often, which works most of the time, hopefully.
 86

CQRS, CONCURRENCY, (EVENTUAL) CONSISTENCY
Conclusion
Consistency, eventual consistency, and concurrency play crucial roles in the development of information systems, especially in distributed systems like the ones we are dealing with in this book.
Understanding the fundamentals of how these concepts interact is essential. Consistency ensures the reliability of data, while eventual consistency acknowledges the realities of distributed systems where immediate consistency is not always feasible. Concurrency management, particularly through mechanisms like optimistic locking, helps maintain data integrity in multi- user and parallel system environments.
Event Sourcing and CQRS, while often mentioned together, serve distinct purposes. CQRS can be implemented without Event Sourcing, but Event Sourcing almost always involves CQRS due to the natural separation of read and write concerns.
As we delve deeper into the subject, you’ll see how these prin- ciples are applied in practical scenarios in Part III of the book, providing a clear picture of their interplay. Understanding these concepts requires time and practice, but they are foundational to building robust, scalable, and maintainable systems.
In the next chapter, we will briefly discuss the concepts of inter- nal and external data and explain why oftentimes it makes sense to treat them differently to keep your systems maintainable.
87

5
Internal versus external data
In this chapter, we will learn a crucial concept for maintaining your systems: the clear distinction between internal and exter- nal data. We’ll see why being explicit about data usage helps maintain clarity and stability in your system.
Important Notice: This chapter represents my current perspective on best practices for handling external data. I understand that opinions vary—some may see this as added complexity. My goal is simply to share insights gained from over 15 years of experience. I encourage you to critically assess these ideas, consider the specific context of your work, and form your own conclusions.
One argument I often hear in favor of Event Sourcing is, unfor- tunately, misguided.
88

INTERNAL VERSUS EXTERNAL DATA
“The benefit of Event Sourcing is that you have access to all events from all systems, and you can use the events stored in the Event Store as you like and build custom models out of it.”
This sounds too good to be true, and unfortunately, most of the time it is.
Coupling
Accessing the internal events of a system is technically the same as directly accessing the database of a system and reading the records as you like. Why is it a bad idea? Because it creates massive coupling.
Imagine you allowed any system to read your database tables. Anytime you want to change the structure of the database by adding mandatory fields or changing the constraints, you’d have to consult all these clients of your data and basically ask for permission.
The tables in this case define an agreed upon contract between your team and the team(s) consuming the data. The other teams rely on the fact that the data structure won ́t change.
Constantly having to consult other teams decreases the velocity drastically, and sometimes even brings it to a halt. Most often, clients of your data will need some time to make the necessary
89

UNDERSTANDING EVENTSOURCING
adjustments on their side after a change, and only then you’ll be able to make the changes in your system.
And sometimes clients will simply refuse to adjust.
One solution, in general, is of course to never allow direct access to the database and always go via API. This accepted rule also applies to event-sourced systems. It is just not that obvious at first sight.
Fig 5.1 - Integration Events
Leaking Business Logic
If you allow systems to access your internal domain events, they need to understand what every event means in order to use them for projections. In the next part of the book, we will model an
 90

INTERNAL VERSUS EXTERNAL DATA
e-commerce cart system. We will work with events like “Item Added,” “Item Removed,” and “Cart Cleared.”
A system using these events needs to know exactly what it means to “add an item.” What does it mean for prices? What are the rules if there are two items with the same product? Are they merged into one item with increased quantity or not? What if we have a “Voucher Applied” event? Is this applied to the whole cart or to a single item? And how are prices calculated anyway? What about taxes? And even worse, what if some of these rules change over time?
This is all domain knowledge of the cart domain. It should not be distributed across the system. Having the logic defined redundantly will inevitably result in an unmaintainable mess long term, as we constantly need to adjust all places when logic changes. And sooner or later we will forget one adjustment, because we are human.
It’s much better to provide a “Cart Submitted” event that already contains the pre-calculated data necessary for an external system to process it. From the perspective of the cart-system, this is an integration event and serves as a contract with external systems.
External Events
The API of an event-sourced system is clearly defined by the “External Events” or “Integration Events” that your system
91

UNDERSTANDING EVENTSOURCING
provides. This is a different event category than the Domain Events we use internally to capture state changes.
Domain Events are living documents of the business behavior in our context. They will change over time, hopefully, because our system constantly evolves. The external view of our data is a completely different story. It’s like a stable summary of the data, suitable to be consumed and processed.
External Events can and should be versioned and can use an agreed-upon schema between teams. This allows for stable communication and the possibility to evolve data over time.
It does not really matter how integration events are distributed between systems. That’s an implementation detail. You can use some stable middleware like Apache Kafka or RabbitMQ, but it could also be a CSV file uploaded to a network share. From the architectural perspective, it’s basically all the same.
Versioning Events
If we publish data, we agree on a contract about the structure of this data. The structure of our data should be defined in a way that a consumer can rely on the contract.
But if one thing is certain, it is that change will happen. We will have to define new attributes, and we may have to deprecate existing attributes. It might even happen that we’ll have to remove complete data structures from the system if they are
92

INTERNAL VERSUS EXTERNAL DATA
no longer necessary or have been merged with other data structures.
How do we handle this in terms of an event-sourced applica- tion? How do we handle adding a new attribute when this new attribute is required from a business perspective?
Consumers must be aware of the data, and they must be aware when they replay an event stream about this change.
There is a whole book written about the subject of versioning in event-sourced systems by Greg Young that goes into great detail about the cases you need to address16.
But here, let’s quickly define if, when, and how you should version your events.
So, in short - should you version events? Most of the time, the answer is yes, at least whenever you need to make a breaking change.
Let’s define the types of changes that could occur.
Backwards Compatible Change
A backwards-compatible change is a change that allows con- sumers to still consume the new event without any adjustment.
16 https://leanpub.com/esversioning 93
 
UNDERSTANDING EVENTSOURCING
Adding an optional attribute is considered a backwards- compatible change, as the consumer can simply ignore the attribute when processing an event.
Adding a mandatory attribute with a default value is also consid- ered backwards-compatible, as the field will be set by default and the consumer does not need to be aware of the field.
Breaking Change
Breaking changes are changes that the consumer needs to know about.
Removing an attribute is considered a breaking change, as the client would still try to set or read it unless they know about the change.
Renaming an attribute presents the same problem, as the consumer needs to set the new attribute and ignore the old one.
Schema Evolution
Now, how do you handle these types of changes? First, we always try to stay backwards-compatible, and often that works until it doesn’t. At some point, you’ll face a breaking change that you need to handle, and then you better have the right strategies already in place to handle it gracefully.
94

INTERNAL VERSUS EXTERNAL DATA
A simple strategy could be to version the event. Instead of a “CustomerRegistered” event, we can have a “CustomerRegister
ed_V2”.
In the first version, we simply had name, surname, and user- name defined.
 {
"type": "CustomerRegistered",
"payload": {
   "username": "dilgerm",
   "name": "Martin",
   "surname": "Dilger"
   }
}
Later, we had to add the email. Since the email is mandatory, we add the new version “CustomerRegistered_V2”.
 {
"type": "CustomerRegisteredV2",
"payload": {
   "username": "dilgerm",
   "name": "Martin",
   "surname": "Dilger"
   "email": "martin@nebulit.de"
   }
}
95

UNDERSTANDING EVENTSOURCING
But how do you handle this in your system? If you want to build a projection based on the customer-event-stream, at some point, you need to switch from “CustomerRegistered_V1” to
“CustomerRegistered_V2” or simply handle both.
 var name: String? = null
var surname: String? = null
var email: String? = null
when (event) {
    is CustomerRegisteredV1 -> {
        this.name = event.name
        this.surname = event.surname
        // Handle the email
        this.email = "default@example.com"
        // Provide a default email or handle it as
        needed
    }
    is CustomerRegisteredV2 -> {
} }
this.name = event.name
this.surname = event.surname
this.email = event.email
In Chapter 28, you will learn a technique that allows you to only deal with CustomerRegisteredV2 and forget about Customer- RegisteredV1 completely.
96

INTERNAL VERSUS EXTERNAL DATA
Migrating Streams
Another solution to handle breaking changes is to simply rewrite a complete event stream and, during the rewrite, convert all events to the new format. You basically deprecate the old stream and provide a new one. Consumers have to migrate to the new stream in order to handle it.
Most of the time, this is not a viable option for communication with external teams but helps to keep the code base clean internally.
There is no one-size-fits-all solution to migrating schemas, and the devil is often in the details. It will hopefully become much clearer when we start working with it in the next part of the book.
Conclusion
We will see some examples of how to apply the concept of internal/external events in the Event Model in Part II and later also in the implementation in Part III. It’s always a good idea to be very explicit about how data is used in the system. Implicit assumptions can result in subtle misinterpretations and ultimately in hard-to-find bugs in production.
Keeping internal data clearly separated gives the team the freedom to work with the data and evolve it, and also provides
97

UNDERSTANDING EVENTSOURCING
other teams something stable to rely upon.
Breaking Changes will happen. That ́s for sure. So it is important to have the necessary processes in place to handle it gracefully. Versioning events is the strategy that works in most cases.
All concepts discussed in this short chapter will be practically applied in Part II and Part III of the book.
In the next chapter, we will look at the anatomy of an event- sourced application and learn about the building blocks neces- sary to build it from scratch.
98

6
The Anatomy of an event-sourced Application
In this chapter, we will dive deeper into the anatomy of a typical event-sourced application. You will learn how to combine the basic building blocks to form a consistent information system that processes and handles information flexibly and in a maintainable way.
More importantly, we will piece them together in a practical way. I hope you will experience some significant “aha” moments in this chapter, with many more to come in subsequent chapters.
First, let’s quickly recap the building blocks we already know. The foundation consists of Events, which are facts that have occurred in the system, and Commands, which represent actions that may result in Events being stored in the Event Store. We also have Read Models, or projections, that allow us to query the stored information in a flexible way.
This is also visualized in Fig. 6.1 - a user issues a Command, 99

UNDERSTANDING EVENTSOURCING
which results in one or more Events, which again are used by different projections queried again by the user or some automations.
Fig. 6.1 - Information Flow in an event-sourced System
This is important knowledge but not yet enough to build and structure a software system. We will learn some additional building blocks in this chapter.
There is quite some flexibility in how to structure the application, andIwillshowyouonewaytodoit. ThisisalsothewayI
 100

THE ANATOMY OF AN EVENT-SOURCED APPLICATION
typically structure most applications I’m involved in. Here are the building blocks we will learn in this chapter:
1. CommandHandlers 2. QueryHandlers
3. Projectors
4. Aggregates
Let’s assume we want to build a simple system (maybe a Microservice). This system should provide an API that can be called from other services in our stack. Let’s make it simple and assume it’s a system to validate and hold customer addresses.
So the first question is, how will the system be called? And here we directly have to make the first crucial decision about how our system communicates. Do we favor synchronous or asynchronous communication?
Many developers will start with the synchronous approach, simply because it looks easier and more familiar at first sight. And they will even prefer synchronous communication if the backend is inherently asynchronous.
Oftentimes trying to hide the true nature of the backend.
It ́s not a good idea, we’ve seen this before.
One example of an attempt that, in hindsight, wasn’t very successful was the development of Enterprise JavaBeans (EJB) as a standard for building enterprise systems in the Java ecosystem.
101

UNDERSTANDING EVENTSOURCING
In its early days, EJB aimed to simplify remote procedure calls, making them feel as seamless as in-process calls. The idea was straightforward: you could invoke any remote system without worrying about its location or how it processed your request. Whether the system was on the same machine or halfway around the world, the complexity would be abstracted away. This concept is known as location transparency. EJB tried to achieve this through a proxy mechanism that would handle remote calls on behalf of the user.
Fig. 6.2 - Leaky abstraction of Remote Calls in EJB
To be honest, it was absolutely horrible as you had to deal with technical RemoteExceptions and EJBExceptions all over the place. The code felt bloated and technical. The remote nature of the calls leaked into the implementation everywhere. So there was no real benefit, and you had to expect a call to fail anytime. Not to mention the performance impact when you do remote procedure calls like they were local calls.
I personally prefer to fully embrace the asynchronous nature of 102
 
THE ANATOMY OF AN EVENT-SOURCED APPLICATION
event-sourced systems and not even hide it from clients. If the system is asynchronous, why not simply make everybody aware of the fact and then enjoy the advantages it brings?
Now let’s assume our Address Service has to provide a REST API because it needs to integrate with some legacy system that can only handle REST. Of course, you could put in some middleware that translates between HTTP calls and messages, but let’s keep it simple for now and assume what it will most often look like in reality—we neither have time nor budget to use yet another middleware. In most cases, you would simply provide this HTTP API.
Fig. 6.3 - HTTP POST
But what now? What happens when this service receives a POST 103
 
UNDERSTANDING EVENTSOURCING
request to handle the registration of a new address? First of all, we need to instruct the system to take action, in this case, check the address given in the payload. The only way in an event- sourced system to instruct a system to take action is to issue a command.
So the first thing we would do is translate the data from the HTTP request to a command. But who would handle the command? Let’s discuss the Command Handler.
Command Handlers
The Command Handler is basically the only component that knows how to handle a command. You either implement the logic to handle a command directly in the handler, or more often the Command Handler delegates to something called an aggregate.
We’ll talk about aggregates in a minute; it’s more or less the component that makes crucial decisions in our system. And yes, it’s the aggregate you might have heard of probably from Domain-Driven Design.
Assume, for example, registering new addresses is guarded by some business rules. Let’s say we cannot register the same address twice, with address equality being defined by street, house number, floor, and zip code.
So in order to be able to register this new address, we first need to check if the address already exists in the system. If the address
104

THE ANATOMY OF AN EVENT-SOURCED APPLICATION
already exists, the system should throw an Exception if we try to register the same address twice.
The Command Handler typically starts by validating some basic structural rules, such as ensuring mandatory fields are filled and verifying that inputs like zip codes are numeric. Once these checks pass, the Command Handler will either directly execute the business logic or delegate the task.
But who is going to check all these business rules? Who is going to enforce that our model will always be in a consistent state, no matter what? As engineers and architects, our biggest goal is to protect the consistency of data in our system. We do not allow invalid data, if possible, anywhere in the system.
Who is the bouncer in this story?
Aggregates
Here we get to know the aggregate. The aggregate basically defines a transactional consistency boundary protecting busi- ness invariants across a common set of objects. Sounds very technical, but actually it’s pretty simple. The aggregate makes sure no invalid data enters the system. It makes sure everybody follows the (business) rules.
There is a lot of discussion how to handle systems without aggregate boundaries. Most systems right now will still use the aggregate as described in this book. If you want to learn
105

UNDERSTANDING EVENTSOURCING
about alternative methods, I encourage you to search for the term “Dynamic Consistency Boundary”17.
Let ́s introduce an additional business rule. A customer can have one delivery address and one contact address. Both addresses can be the same.
The aggregate checks all invariants and ensures that at no time are more than two addresses assigned to a customer. It also ensures that the customer cannot delete the primary address. Furthermore, the aggregate makes sure that no two customers have the same address assigned.
After a successful command execution, the aggregate is respon- sible for writing the event(s) generated by the command exe- cution to the Event Store. Events are the proof that something has changed in the system, such as an order being submitted. The persisted event is all that remains when the system gets restarted, so it is crucial that every change in the system is recorded as events.
Fig. 6.4 - Command / Aggregate / Event Cycle
17 https://www.axoniq.io/blog/rethinking-microservices-architecture-throu gh-dynamic-consistency-boundaries
  106

THE ANATOMY OF AN EVENT-SOURCED APPLICATION
We’ll take a deeper look at how aggregates are structured and implemented later in the book. For now, let’s define an operation on an Aggregate like this:
 class CustomerAggregate {
    fun handle(command: AssignContactAddressCommand) {
        // Validate the command
        // Example: Validate customer info, etc.
        // Persist the event
        storeEvent(ContactAddressAssigned(...))
    }
}
Projectors
As soon as the event is stored in the system and we know that something has changed, we have components that react to this new information. Here we introduce Projectors.
A Projector reacts to new events and projects them to a different medium, such as a relational table, making it easier to query the data.
107

UNDERSTANDING EVENTSOURCING
 Fig. 6.6 - Data Projector
Instead of going through all events to calculate how many customers moved to new addresses yesterday, we could project every ContactAddressAssigned-event to an entry in a relational table and then perform a simple SQL query:
The data in the contact_addresses table is a projection of the events stored in the Event Store.
A projector is simple and works similarly to an Event Handler.
For every event that happens, the projector translates the data to the necessary SQL statements ( Create, Update or Delete) in case of a database projection. This way, we have the same data both in the events and the table, just in a different format.
 SELECT * FROM contact_addresses WHERE created_date =
'2024-07-21'
108

THE ANATOMY OF AN EVENT-SOURCED APPLICATION
Queries / Query Handler
Now that we know something has changed (Event) and we have prepared the data to be easily queryable (projection), we need to decide how data is queried in the system.
We can either directly query the data using SQL from the database projection or in case of the Axon Framework use the additional Query-Handler-Abstraction.
The Query Handler knows how to retrieve this data:
 val orders = queryHandler.query(
    OrdersQuery(LocalDate.of(2024, 7, 21))
)
The Query-Handler is a specific component of the Axon- Framework, that we will use extensively in Part II of the book. Query Handlers are an essential part of an event-sourced system.
To issue this query, the system that needs this information does not have to know in what format or technology the data is persisted. The Query Handler provides the necessary abstrac- tion. We could change the representation in the background and adjust the Query Handler accordingly without the client even knowing.
109

UNDERSTANDING EVENTSOURCING
 Fig. 6.7 - Query Handler
Query Handlers are the equivalent in code to the Read Model in the Event Model. A Read Model is typically represented either as a data projection and a Query Handler or as a Read Model directly operating on the events as a so-called Live Model ( Live Models are discussed in Chapter 31 ).
Conclusion
These are the basic building blocks you need to build an event- sourced application.
110

THE ANATOMY OF AN EVENT-SOURCED APPLICATION
 Fig. 6.8 - Eventsourced Application Lifecycle
Let’s recap the most important concepts:
1. Westartwitharequesttothesystemtoexecuteacommand 2. The Command Handler processes the command.
3. OfentimesitwilldelegateittotheAggregate.
4. The Aggregate knows all the business rules, validates the
data in the command, and executes the command if all
business rules apply.
5. We then have Event Handlers and Projectors interested in
the data, which handle updates to projections accordingly. 6. Aclientinterestedinthedatacanissuequerieshandledby Query Handlers. The Query Handler knows how to retrieve the data from the projections or the Event Store itself and
returns it to the client.
111

UNDERSTANDING EVENTSOURCING
A client can either be a UI to display information or an automated process running in the background.
In the second part of the book we will dive much deeper into the implementation of an event-sourced system and we ́ll learn step by step how this all is structured in a real-world application.
112

7
Event Streaming , Event Sourcing and Stream Design
Event Streaming and Event Sourcing are not the same thing. They are not even close. So, why did I decide to write a chapter about Event Sourcing and Event Streaming in general?
Unfortunately, these two concepts get confused all the time.
And I know exactly why. They look pretty similar at first sight. And unfortunately, they also use the same terminology. We are dealing with events and streams in both cases. I often speak of records in Streaming and events in Event Sourcing to use clearer language whenever possible, but this is not official terminology.
Events, records and data are the fundamental building blocks of our companies and all of our IT systems, no matter if you physically represent them in your code or not. Both Event Sourcing and Event Streaming can play a crucial role in your system design, often at the same time in the same system.
113

UNDERSTANDING EVENTSOURCING
Confluent, the company behind Apache Kafka, one of the most popular Event Streaming Platforms, has this beautiful name for their advertising road trips. It’s the “Data in Motion” tour.
I like the name because it fundamentally describes what Event Streaming in general is all about and how it’s different from Event Sourcing. It’s about bringing data from Point A to Point B in a fast, reliable, and structured way. It’s really nothing more than this. With Event Streaming, you are basically building the highway for your data between systems. Streams in Event Streaming are infinite and deliver data whenever something happens. It’s hard to find the end of a highway; it feels like they simply go on forever. And it’s the very same thing with your streaming highways.
Fig. 7.1 - Kafka as the data highway
 114

EVENT STREAMING , EVENT SOURCING AND STREAM DESIGN
Apache Kafka Primer
Apache Kafka is an event-streaming platform designed for high- throughput, offering some important guarantees that make it very suitable to be used as message-broker between systems.
I ́ll give a quick primer here about Apache Kafka which is by no means meant to be complete. The goal is more to give you a feeling what a streaming platform looks like in general.
Fig. 7.2 - High Level Kafka Overview
Producer / Consumer
A record in Kafka represents any type of information sent by a producer and consumed by one or many consumers. This record could be data from an IoT device or a notification / event from one of your services.
Kafka differentiates between producers and consumers of 115
 
UNDERSTANDING EVENTSOURCING
records.
• A producer sends records to topics, which act like named queues.
• Aconsumersubscribestotopicstoreceiveandprocessthese records.
Producer and Consumer Communication
Producers and consumers must agree on the topics they use to communicate. Topics allow producers to send data to a defined location, and consumers listen to this location for the data they need. Each topic can be partitioned for scalability and parallel processing.
Scaling with Kafka
When scaling your services horizontally, ensuring that each record is processed by only one instance (and not duplicated across multiple instances) is essential. Kafka solves this through consumer groups. All services of the same type are grouped into a consumer group, and Kafka assigns each partition within a topic to only one consumer in that group. This ensures that only one consumer processes a record at a time.
Topic Partitioning
Kafka topics are partitioned, meaning the records within a topic are split across multiple partitions. Within a consumer group, each consumer instance will be responsible for processing one or more partitions. This ensures that only one instance processes
116

EVENT STREAMING , EVENT SOURCING AND STREAM DESIGN
a particular record at a time, avoiding duplicate processing.
Message Ordering and Guarantees
Kafka guarantees the ordering of messages within a partition. While Kafka doesn’t guarantee ordering across all partitions in a topic, within a single partition, messages are strictly ordered, which is crucial for certain workflows.
Difference between Event Sourcing and Event Streaming
Event Sourcing fundamentally is about state changes and how we capture them, whereas Event Streaming is about motion. We do not store data in tables and relations; we logically store data in events and streams. Unfortunately, we use the same naming in both worlds. A stream in Event Sourcing allows you to group together events that belong together from a business perspective.
In Event Sourcing, all events belonging to a customer could be part of a customer stream under the customer Id. If I want to know what happened to a certain customer in the past, the only thing I need to do is fetch all events from the customer stream and filter by the customer Id.
Streams in Event Sourcing can contain 5, 500 or even millions of events (in theory). In reality it is often something between 10 and 100. Streams fundamentally describe the life cycle of business capabilities, be it a customer, an insurance contract or a car sale. As soon as the insurance contract has been submitted,
117

UNDERSTANDING EVENTSOURCING
we do not expect many changes for the contract. Sometimes we add a new policy, sometimes we re-negotiate the prices but in reality, its maybe two or three changes a year if at all.
In practice, what you will always try is to keep your streams as small and short as possible by defining the “end” of the stream. A stream can end, for example, at the end of the day (like in trading by closing the books), after each month (payroll management), or after a certain event has happened (order was submitted).
Fig. 7.3 - Event Streams
It’s perfectly possible that records from Event Streaming can become domain events for Event Sourcing. Its not exclusive to one or the other.
Now that we’ve defined the basic terminology and explained what separates Event Sourcing from Event Streaming, let’s look
 118

EVENT STREAMING , EVENT SOURCING AND STREAM DESIGN
at a concrete example.
A Story of Two Systems
Assume we have two systems communicating with each other.
System A is a physical coffee store that wants to improve the overall customer experience in the store.
System B is an app a user can install on the phone. Given the appropriate permissions, the app tracks the position of the user. Whenever the user comes close to the store, the app publishes records to a Kafka topic named “user_positions,” containing the user Id, latitude, and longitude.
System A subscribes to the “user positions” topic and detects when the user enters the store. System A translates the records into events: “User entered store area,” “User left store area,” “User entered store,” and “User left store,” which are persisted in the user stream (here we are talking about Event Sourcing).
- “User entered store area” indicates that a user is close to a physical store location.
- “User entered store” indicates the user has finally entered the store.
Whenever a “User entered store” event occurs, it’s displayed on a screen visible only to employees, showing a picture and
119

UNDERSTANDING EVENTSOURCING
the name of the user. This way, when the user wants to order a coffee, the employee can greet them by name and also know their typical order.
Fig. 7.4 - From Record to Domain Event
In the next version, the company plans to give a user an auto- matic 10% voucher and a notification on their phone if they enter the “store area” and do not enter the store within 5 minutes.
It’s a beautiful use case and shows how proper data handling can improve customer experience and boost sales.
But even more importantly, it shows how we can combine the streaming of records with Event Sourcing. The records from the stream (which are technical, containing latitude and longitude) get translated into domain events with real business meaning for processing in System A.
There could also be System C that subscribes to the “user positions” topic, but it ́s only interested in reporting purposes and finding anonymous answers to questions like, “How often does a user enter the store when they have entered the store area
 120

EVENT STREAMING , EVENT SOURCING AND STREAM DESIGN
before?” This could be used as an OKR (Objective / Key Result) to increase this number by 10% in the next quarter. Very specific, very measurable.
I hope this sheds some light on the crucial differences between streaming and Event Sourcing. They are not the same thing, but they can work together beautifully.
In the next section, we will take a deeper look at how to design streams and why in Event Sourcing you typically do not process millions of events to build data objects.
Stream Design
How do you find the boundary of a stream in Event Sourcing? What glues events together? And how do we prevent having to process hundreds or thousands of events just to get the current address of our customer?
Don’t get me wrong, initially you will mess up your stream design, because we all do. It’s impossible to get things right all the time, and it would be foolish to even try. What we should do is think about event streams as the fundamental building blocks of our systems that are in constant flux. They aren’t rigid; they are flexible and adjustable.
You group together what belongs together. Every stream has a unique identifier, and typically you can query all events in a given stream like you would do in a database.
121

UNDERSTANDING EVENTSOURCING
 Fig. 7.5 - Swimlanes
When planning systems using Event Modeling, we use swim- lanes to separate event streams from one another.
You see an example of how swimlanes are applied in Fig. 7.5, where we define 4 swimlanes for each business capability in the system.
Swimlanes define stream boundaries. Typically, all events in one swimlane end up in a physical stream, like in the example above where “Product Selected” and “Product Upgraded” belong to the “Product” swimlane.
122

EVENT STREAMING , EVENT SOURCING AND STREAM DESIGN
A nice little “trick” to validate if the boundaries of a stream make sense is to hide all swimlanes but one and just read the events from left to right in isolation to someone from the business side who cannot see the model. The events should form a compelling narrative and a consistent story. If not, the business person will tell you immediately.
Another nice little “trick” is to again hide all streams and start from the right. Uncover one Event at a time. For every event, check that the preceding events deliver all information necessary to create the current event. If yes, move one event to the left until you reach the beginning.
If you find an attribute that cannot be filled from the previous events, make sure the model clearly defines where this data is coming from. For example via a call to an external System
Going from right to left forces you to look at it from a different angle and focus on the data.
If the streams get too big, it’s often simply a hint that you might have missed the opportunity to structure your streams more clearly. Sometimes streams are simply big with a few thousand events. It ́s neither forbidden nor a problem. The only thing that is important is, the stream should mimic the lifecycle of the underlying business capability.
You can process a big chunk of events in real-time before you hit any kind of performance bottleneck (processing 10,000 events is still doable in milliseconds), but sometimes it’s crucial to keep performance steady. So how do you handle these big streams?
123

UNDERSTANDING EVENTSOURCING
Keeping streams short
The absolute best way to keep performance high is not by optimizing the loading of big chunks of data, but by preventing these big chunks in the first place. The pattern here is “Closing the Books,” and it comes from accounting, where you typically close an accounting period periodically.
The accounting period of a company is the year. If the year is over and taxes are filed, you typically do not care anymore what happened in this year taxwise unless the financial authority is asking for an audit.
The accounting period of a cash register in a café is typically for a single day. You write down the amount of money you started with, calculate the difference to the amount you ended the day with, and then you know what you made that day. For the next day, the only important thing is the amount in the cash register.18
We will talk about the implementation details of “Closing the Books” in Parts II and Part III of the book. But essentially, it’s a way to keep your streams short by partitioning them according to business (not technical) rules.
The stream for the cashier could be “cashier_{cash-register- id}_{day}” as mentioned in the article linked above. Since we
18 https://www.eventstore.com/blog/how-to-model-event-sourced-system s-efficiently
 124

EVENT STREAMING , EVENT SOURCING AND STREAM DESIGN
take the day into account, there will be a new stream every day automatically, which is a natural way to enforce short streams.
Summary Events
Instead of creating new streams every day or year and keeping them separate, we could also leverage summary events to close the books. If you wanted to keep all events for the cash register in one stream, we could leverage a “Register Closed” event per day that keeps track of the final cash amount in the register.
This stream will get big. In this case, it’s not feasible to load all events but only to load the events since the last “Register Closed” event.
How do you know where to start in event stream then? We need to keep an index of all “Register Closed” events in a separate stream and load all events from the index.
 eventstore.readEvents("aggregateId",
startSequenceNumber)
Snapshots are the technical equivalent to the summary event mechanism. At regular intervals, we create Snapshot- Summaries of our streams to keep information short. We will look at Snapshots in detail in chapter 34. Using snapshots
125

UNDERSTANDING EVENTSOURCING
typically requires you to load the latest snapshot and from there all events on top. Snapshots can be taken periodically after each hour or day, or if the number of events exceeds a certain threshold, for example, 1000 new events since the last snapshot was taken.
Conclusion
From my experience, using snapshots is really the last option I’d consider when designing streams as it’s a purely technical and a performance optimization (very much like a cache), that is very often not necessary if the streams are designed in the right way. I’m not saying snapshots are a bad practice in general. I just want you to think about the business capabilities of your software first before you attempt to solve a business problem with a technical solution.
It’s always better to solve problems in a way that aligns with how the business works. Talking to business about snapshots will almost always result in puzzled faces and question marks. Whereas mentioning that the cash register has been closed for the day will be understandable by the business people involved.
Stream design requires some practice and some understanding of how event-based systems typically work, but it sounds much harder than it actually is. One of your sharpest weapons here is common sense. And the good thing is, errors here can be corrected later by creating new streams and migrating old ones. As already mentioned, it’s impossible to get everything right all the time, and you will mess up certain decisions. Instead
126

EVENT STREAMING , EVENT SOURCING AND STREAM DESIGN
of trying to prevent this, let’s embrace it and try to mimic the business in our software systems the best way possible.
In the next chapter, we will look a bit deeper into exactly this when we widen our horizon and look beyond the technical measures with a brief discussion of concepts from domain- driven design and how they apply to event-sourced systems.
127

8
Domain Driven Design
If you work in IT, it’s very likely you’ve heard about Domain- Driven Design (DDD) recently. It all started with the “Blue Book” by Eric Evans, which almost all of my colleagues (including me) have read. Some multiple times. It is insane, looking at my order history in Amazon, I bought the book back in 2009. Feels like it has been much more than 15 years.
 128

DOMAIN DRIVEN DESIGN
But at the same time, it remains a mystery to many, how to apply the concepts in real-world projects. Books like Implementing Domain-Driven Design by Vaughn Vernon help, but still many concepts remain unclear.
If I had to provide a short explanation of what Domain-Driven Design is all about, it would be basically this:
“Domain-Driven Design is a software development ap- proach that emphasizes collaboration between technical and business teams to model complex systems using a shared language and clearly defined domain concepts.”
In this chapter, I will provide a simple overview of what concepts of Domain-Driven Design play a crucial role for us in this book and how it fits together with Event Sourcing and Event Modeling.
This is neither a comprehensive description of DDD in general, nor will we touch all necessary areas of it. If you want to dive deeper into this topic, I ́d suggest the Book by Eric Evans and from there progress through the books mentioned in the recommended readings on eventmodelers.de 19
You will hopefully come to the same conclusion as I did: in combination, the tools from the books and the tools we use in this book are a match made in heaven.
19 https://eventmodelers.de/faq/fachliteratur 129
 
UNDERSTANDING EVENTSOURCING
Having explored these books extensively I consider myself a fan of Domain-Driven Design for its emphasis on collaboration across team boundaries and its focus on clear communication among all stakeholders. While I am not an expert in DDD and this is not specifically a book about DDD, I will provide a clear definition and share my current understanding of it. This definition is one perspective among many. As Adam Dymitruk famously said at a meetup in Munich in October 202320, “By working with Event Modeling, we achieved all of DDD without doing DDD.”
Bridging the Gap with Domain Driven Design
One of the biggest challenges that Domain Driven Design ad- dresses is the gap between business and IT. The translation between these domains often works like the famous children’s game in Germany named “Stille Post” (known as “Telephone” in English).
In this game, children stand in a row, and the first child whispers a word or sentence into the ear of the next child. This process continues until the sentence reaches the last child, often resulting in a completely different and amusing phrase.
Most people know this game from their childhood, and it’s likely been played for hundreds of years. Interestingly, we seem to play a version of this game in most IT projects with absolute
20 https://youtu.be/MbkDDnqUm90?t=2995 130
  
DOMAIN DRIVEN DESIGN
passion.
Someone defines a requirement and puts it into a JIRA ticket. Someone else reads the requirement and translates it into a specification. This specification gets picked up by another person who splits it into even more tickets. These tickets are discussed in weekly meetings, and based on the outcomes, budgets are allocated. We discuss and meet every two weeks in refinements to discuss these requirements, most of the time without involving the business stakeholders directly. “Stille Post” in action.
The most important rule, it seems: nobody is allowed to talk to the person who wrote the original requirement.
It’s hilarious when you think about it, yet this is what happens in most projects.
DDD provides tools to tackle this communication gap, and Event Modeling allows you to work with these tools in a practical and easily accessible way. It ́s a perfect match.
If you ask me, the most important idea in DDD is the ubiquitous language as defined by Eric Evans.
Ubiquitous Language
The ubiquitous language tries to minimize or even eliminate the translations between business and IT by leveraging a common
131

UNDERSTANDING EVENTSOURCING
language spoken and used by all sides. Not only that, it provides a clear language definition of all domain concepts that is used throughout tickets, discussions, and code.
It sounds much easier than it actually is. Finding this language is a process and can only happen with a lot of collaboration. People have to actually speak to one another in meetings and also during coffee breaks.
If you learn a new language, the best and fastest way is to speak it.
And the best way I have found to speak this common language is collaborative modeling, and in my case, I prefer using Event Modeling as my dictionary. Event Modeling allows us to talk about data flows in a system that is both close to the business and close to the implementation, offering a common ground for everyone involved.
In Event Modeling, we do not focus on technical concepts but only discuss how data flows through our system. By defining the domain events on the whiteboard, we define all the important concepts and terms we need to handle.
132

DOMAIN DRIVEN DESIGN
 Fig. 8.1 - Domain Concepts captured
This is where the ubiquitous language starts to appear, where people begin to discuss the meaning of different terms. We discover that one term in the language can have different meanings depending on the context. This is where common understanding starts to emerge.
Indirect communication via tickets is always “Stille Post.” It’s a losing game. And we typically pay a pretty high fee to play it. Direct communication is where we find value and understanding. Direct communication is what saves time and money in the end.
Since our collaborative model not only serves as a base to gather requirements but also for documentation and code generation in many cases, the common language naturally propagates itself to all corners of the system.
133

UNDERSTANDING EVENTSOURCING
Bounded Context
The second most important concept I see is the use of bounded contexts, which goes hand in hand with the previously defined ubiquitous language.
As it turns out, it’s almost impossible in the context of a company to define a clear language that is universally applicable to all areas. The same term can and will have very different meanings in different departments or divisions of a company, even between different teams of the same department, some- times even within a team.
So it’s critical to make sure we draw the boundaries of language and meaning very clearly. This is what bounded contexts are all about. We define the ubiquitous language for a bounded context, and it is only valid and applicable within that context.
Very often, bounded contexts immediately get translated to the technical side and correlated with something like microservices. That’s a big mistake, in my opinion. You can have many different contexts within one system. You can have one system for one context, and a context can of course span many systems. It’s simply not defined. If you need to make a decision early, most of the time it ́s the best conscicous decision to not split but keep everything in one system until you know more.
What has to be clear and defined is the area in which our language, terms, and models are valid. So we do not make as- sumptions but can rely on the clear definitions and boundaries.
134

DOMAIN DRIVEN DESIGN
Again, collaborative modeling and especially Event Modeling help us here, as we can cluster our defined events into areas of interest which typically become bounded contexts in the sense of DDD.
For example, the term “Order” in an e-commerce system has a very different meaning in the “Order Context”, where we discuss an order the customer placed, compared to the “Payment Context”, where the payment placed order has to be processed.
In the Payment Context, the order might have attributes like “Payment-Id,” “Payment-Provider,” and “Payment-Status,” which simply have no meaning in the Order Context. Conversely, in the Order Context, there might be attributes like “Order-Id,”
“Order-Items,” or “Address-Details.” 135
 
UNDERSTANDING EVENTSOURCING
It would be futile and silly to even try to define a global model for an order. It simply won’t work. The concept of an order always has to be defined within a clear context so everybody knows what is being discussed at the moment.
Aggregates
In the context of Domain-Driven Design, an aggregate is a cluster of domain objects that can be treated as a single unit. Its first clear definition, to my best understanding, appeared in the blue book by Eric Evans. An aggregate ensures the consistency of changes being made within the model. Here are the key points that define an aggregate:
 136

DOMAIN DRIVEN DESIGN
Root Entity
An aggregate has a root entity, known as the aggregate root, which is the main entry point for accessing other entities within the aggregate. Only the aggregate root can be referenced from outside the aggregate.
Boundary
The aggregate defines a boundary around related entities and value objects. Within this boundary, the entities and value objects are encapsulated and can interact with each other.
Consistency Rules
Aggregates enforce consistency rules only within their boundary. Any changes to the entities within the aggregate must ensure the aggregate’s invariants are maintained. Invariants are simply things that are always true in the system like “Customers must never order more than 3 items” or “Blocked Customers must not order online”.
Transaction Scope
Operations that change the state of an aggregate must be atomic, meaning they should be completed in a single transaction to maintain consistency. This implies that the aggregate is a unit of consistency and also a transactional boundary.
137

UNDERSTANDING EVENTSOURCING
References
Other aggregates or entities outside the aggregate should not directly reference entities within the aggregate, except for the aggregate root. This helps maintain the encapsulation and integrity of the aggregate’s boundary.
This explanation is fine, but in practice, if you have never worked with aggregates, you might not know how to implement them correctly. What does a “cluster of objects” even mean? How do you enforce “consistency rules,” and what about the
“transaction scope” of the aggregate?
It sounds much more complicated than it actually is. Let’s take a closer look.
Eric Evans defines a cluster of objects as consisting of both entities and value objects. Entities have an identity, basically an Id that uniquely identifies an entity throughout the lifecycle of the system. One example might be a User or an Order in an e-commerce system.
Value objects do not maintain their own identity but are defined by the values of their fields. An example is an address. Several persons can live in the same house and can have the same address, but not all of them will reference the same address by its primary key in the database. Addresses might not even have a primary key.
What about order items? Is an Order-Item a value object or an entity? It depends on the context. Let’s say an Order-Item only
138

DOMAIN DRIVEN DESIGN
exists in the context of an Order. It might be enough to reference the order item as a value object, identified only by the index of the list of order items in the order.
In the end, it does not matter too much as we try to keep our model flexible. You will mess up the design of your model more than once, and so did I. Don’t spend too much time trying to find the perfect model; just find something that works and get going.
Finding Aggregates
The easiest way to find aggregates is to use the collaborative modeling technique that works best for you. You will start to collect numerous events from all involved parties, and naturally, these events will start to form clusters.
These clusters are almost always good candidates for a first set of aggregates.
An “Order Item Added,” “Order Item Removed,” and “Order Submitted” event naturally belong together and form a group of things that can happen in the system. In discussions with your business experts, you should try to find out what the real name of the “cluster” is. What term do they use in their meetings for this?
If you can ́t find a good name for it, most probably it ́s not (yet) detailed enough to become an aggregate.
139

UNDERSTANDING EVENTSOURCING
 How does an Aggregate enforce Business Rules?
Let’s say in a discussion with the business side, we find out about the rule that a user must only have a maximum of 3 items in an order and never more. That’s a business rule and an invariant. Invariants must not be broken.
We can express this rule in the event model with a “Given / When / Then.” (We discussed GWTs in Chapter 3 already).
Given: The user already has 3 items in the order. When: The user tries to order another item.
140

DOMAIN DRIVEN DESIGN
Then: Expect an exception to be thrown as the invariant is broken.
The implementation is straightforward. Let’s forget that we work with Event Sourcing for a second and just look at how the aggregate enforces this rule.
 class OrderAggregate {
var orderItems = mutableListOf()
   fun addOrderItem(orderItem: OrderItem) {
      if (this.orderItems.size >= 3) {
         throw InvalidStateException("only 3
         orderitems allowed")
       }
      this.orderItems.add(orderItem)
   }
}
The addOrderItem function in the OrderAggregate is the only way to add an Order Item to the Order. The Order Aggregate is aware of the business invariants and implements the necessary checks to protect them. In this case, it checks that there are never more than 3 order items in the list and throws an exception otherwise.
We will see in Part II how this looks like with a so-called “event- sourced aggregate”, but the concept stays the same no matter the implementation.
The Aggregate ensures that only valid data enters the system at 141

UNDERSTANDING EVENTSOURCING
any point in time.
How does the Aggregate enforce Transactional Boundaries?
Every operation in an aggregate forms a business transaction. The result of a business transaction is persisted in the system, whether in a relational table or an Event Store. To store the data, we need to check if the data we are trying to persist is still valid and consistent.
We also need to make sure that nothing has been persisted in the meantime without us knowing that would invalidate our locally valid transaction.
The most common way to approach this is using an optimistic locking approach on the aggregate’s version. We discussed optimistic locking in detail in Chapter 4, when we talked about concurrency. Whenever we change something in the aggregate, we increase the aggregate version. If we try to persist the result of a business transaction in the aggregate, we compare the aggregate version before the change with the aggregate version in the database. If someone else persisted an operation while we were processing ours, the aggregate version was already increased, and we cannot proceed. We have to start over by loading the latest state of the aggregate again.
Since the aggregate coordinates all changes in the object cluster, it ensures that all objects remain in a consistent state. For
142

DOMAIN DRIVEN DESIGN
example, by adding an order item to the order, we add the item to the list and could also recalculate the order total based on the new item.
Conclusion
This chapter only scratches the surface of what DDD is all about. This book cannot and does not even try to replace the wonderful books written on the topic. I wanted to provide a short and concise summary of the most important concepts as they affect us in the next part of the book. Nothing more, nothing less.
I really encourage you to dive deeper into the topic and have a look at some of the books mentioned. It really widens your horizon to look at systems from the DDD-perspective.
In the next chapter, we will touch on a topic that most developers fear the most. Designing distributed transactions and the infamous Saga Pattern. Let ́s see if we can shed some light into these interesting concepts.
143

9
Handling transactions in distributed systems using Sagas
It would be possible to write a book about Sagas, Long Running Processes and Distributed- and Compensating Transactions. It feels like nobody really knows how to handle them correctly. I can very well remember how I felt 10 years ago - completely lost.
Sagas are not a new thing. And it also did not come up in around 2010 with Microservices as many developers think. There is rarely something truly new, not even in our industry. The term saga comes from a paper by Hector Garcia-Molina and Kenneth Salem. It is well worth a read. 21
Sagas typically come up when you start using Microservices and you suddenly realize, that you need to coordinate processes across Service Boundaries. Maybe even with other teams. Un- fortunately, as soon as you leave your system and communicate with other systems, everything becomes more complicated at
21 https://www.cs.cornell.edu/andru/cs711/2002fa/reading/sagas.pdf 144
 
HANDLING TRANSACTIONS IN DISTRIBUTED SYSTEMS USING SAGAS
first sight. No database transactions unless you are using a common Database (hopefully not).
Whenever you try to find out how to do this process- coordination, you get bombarded with terms you don ́t understand, sometimes from people who don ́t understand them either. You read about orchestration, and choreography and compensating transactions. Most people get lost within minutes.
In this chapter, I try to give my point of view on the Saga- Pattern in simple words. As I would explain it to my children if I had to. Because I think, one thing that is missing nowadays is simplicity. I neither claim that everything here is correct, nor do I try to tell you how to do things, because simply I don ́t know what works best for you. I don ́t even claim that I ́m using the terms correctly, as my understanding of distributed processes constantly changes and evolves.
As always, there are many ways to understand a problem. I will not present a solution to distributed transactions in this chapter, but I ́ll try to describe the problem in an understandable way. If you want to learn about a simple approach to implement these kinds of processes, feel free to jump right to Part IV and Chapter 35 - and read about the “Processor TODO-List Pattern”.
We will not dive into the implementation of Sagas in this chapter, but only provide a high level overview of what they are and why they exist. We will talk about the implementation in Part II of the book, where you will learn that I typically don ́t use Sagas at all.
145

UNDERSTANDING EVENTSOURCING
Sagas
Let ́s collect some common information widely available on the internet.
A LLT (long-lived transaction) is a saga if it can be written as a sequence of transactions that can be interleaved with other transactions. The database management system guarantees that either all the transactions in a saga are successfully completed or compensating transactions are run to amend a partial execution. 22
“Implement each business transaction that spans mul- tiple services as a saga.” - Chris Richards talks about Transactions that span multiple Services.23
“The Saga design pattern is a way to manage data con- sistency across microservices in distributed transaction scenarios. A saga is a sequence of transactions that updates each service and publishes a message or event to trigger the next transaction step.” (Microsoft Azure Reference Architecture)24
The saga pattern is a failure management pattern that helps establish consistency in distributed applications,
22 https://www.cs.cornell.edu/andru/cs711/2002fa/reading/sagas.pdf)
23 https://microservices.io/patterns/data/saga.html
24 https://learn.microsoft.com/en-us/azure/architecture/reference-architect ures/saga/saga
 146

HANDLING TRANSACTIONS IN DISTRIBUTED SYSTEMS USING SAGAS
and coordinates transactions between multiple microser- vices to maintain data consistency. (AWS Documenta- tion)25
In the AWS Documentation, you also find this hint.
The saga pattern is difficult to debug and its complexity increases with the number of microservices. The pattern requires a complex programming model that develops and designs compensating transactions for rolling back and undoing changes.26
Whereas in the Saga Paper, we find this sentence right at the beginning.
Both the concept of saga and its implementation are relatively simple, but they have the potential to improve performance significantly. 27
As always, it is best to discuss a theoretical concept using a practical example. Everyobody knows how an online-shop works, at least in theory.
25 https://docs.aws.amazon.com/prescriptive-guidance/latest/modernizatio n-data-persistence/saga-pattern.html
26 https://docs.aws.amazon.com/prescriptive-guidance/latest/modernizatio n-data-persistence/saga-pattern.html
27 https://www.cs.cornell.edu/andru/cs711/2002fa/reading/sagas.pdf 147
 
UNDERSTANDING EVENTSOURCING
 Fig. 9.1 - Online Shop Flow (wrong)
This all looks fine. The only problem - it is wrong. The different process steps don ́t happen in order but in parallel, somtimes in different systems.
Fig. 9.2 - Online Shop Flow (correct)
So we collect the payment and update the inventory at the same time. This all is fine, unless something goes wrong.
What if “Collect Payment” fails because the Credit Card was 148
 
HANDLING TRANSACTIONS IN DISTRIBUTED SYSTEMS USING SAGAS
rejected, but the inventory is already updated in the meantime?
Fig. 9.3 - Failure to collect payment
Who or what will tell the inventory System, that the Update needs to be rolled back? Who will tell the Order-System, that the order could not be fulfilled. And who will tell the customer in the end, that the delivery might not happen?
If all these Systems were implemented in one monolithic appli- cation, you could simply wrap it all in a giant transaction and if anything goes wrong, just roll back the transaction and show the problem to the customer.
In most systems, it is not that easy. Collecting payments is typically done by a payment provider, as handling Credit Card Data is complex and requires compliance with security standards.
 149

UNDERSTANDING EVENTSOURCING
This is where it starts to get complicated.
First, you need to make the decision if you want to coordinate this process in a central place using something like a “Process Coordinator” or if you want to keep the systems as independent from each other as possible.
Process Coordinator
Whenever you read the term “Orchestration”, it is basically talking about a Process Coordinator as a central instance to handle a complete process. We could place the coordinator in the order service for example, we could also even implement it as a standalone service instance.
Fig. 9.4 - Process Coordinator
 150

HANDLING TRANSACTIONS IN DISTRIBUTED SYSTEMS USING SAGAS
The Process Coordinator is responsible for handling specific system events. For example, it knows to act when a payment fails, notifying the inventory system that the related inventory update must be rolled back. Similarly, if the system runs out of inventory, the coordinator instructs the Payment System to issue a refund. Initially, this approach seems intuitive, as it resembles a familiar “local” process that we’re accustomed to managing.
Fig. 9.5 - Process Steps
The “Process Coordinator” can create a massive amount of coupling, if not used carefully, as it needs to have knowledge about all these systems. What if we needed to also notify the Fraud Management System, the Tracking System, and the Customer Service?
And even worse, whenever any of these Systems changes the API or suddenly requires additional information, the “Process
 151

UNDERSTANDING EVENTSOURCING
Coordinator” needs to be adjusted. And who owns this process in the end? Is it the Order Team? Is it the Payment Team? If this process changes, who will implement the changes?
Don ́t get me wrong, I ́m not judging this approach. There are situations, where orchestrating a process is a perfectly valid solution.
But what is the alternative?
Service Autonomy
The other type of Saga-Implementation is typically called “Choreography”. I personally don ́t like this term at all, as it is hard to understand what it actually means for your system, but
that ́s a personal opinion.
If you want to prevent the coupling introduced by the central “Process Coordinator” we discussed in the previous section, you need to find a way to keep the services independent from each
other.
Instead of modeling the process in a central location, you simply notify the other systems that something happened and you trust, that all other systems know how to act accordingly.
If the payment-service emitted a “Payment Failed” Event, there is no central place where you could look up what that means to the process. The process steps are decoupled and both systems react to an “Order Submitted”-event.
152

HANDLING TRANSACTIONS IN DISTRIBUTED SYSTEMS USING SAGAS
 Fig. 9.6- Order Notification
The Order System is unaware and unconcerned about how other systems respond to the fact that an order has been submitted. If the payment fails, the Order Service can handle this by emitting an “Order Failed” event. The Payment System, in turn, is not involved in what actions the failed payment triggers; it simply recognizes that the payment has failed.
153

UNDERSTANDING EVENTSOURCING
 Fig. 9.6- Order Failed Process
The inventory system knows how to react to submitted and failed orders and updates the Inventory accordingly. Every Service is focused on its own Context and area of responsibility. If each service is owned by a Team, it would feel natural to go for this approach, as Teams work independently.
Now as we all know, there is no free lunch. Debugging these decoupled workflows and finding problems is much more com- plicated than in a central process manager instance. Therefore, deciding between one of these approaches always requires careful analysis, and some experience, in order to make the right call.
154

HANDLING TRANSACTIONS IN DISTRIBUTED SYSTEMS USING SAGAS
Conclusion
Let ́s face it - Distributed Processes are complex. You need some experience to get them right. And most probably as stated earlier, you likely won ́t on the first attempt anyways. How you design your processes is a little bit art, a big portion experience, combined with some pain you had from earlier attempts.
It is easy to overcomplicate your process design. Simplicity is always a good starting point. What is the easiest way to implement a process without creating a massive amount of coupling?
The only one who can give the answer for your system is you and your team.
I personally don ́t use Sagas most of the time but rely on a simple “Processor-TODO List Pattern”. In simple terms, the Processor-TODO-List-Pattern uses the current state of the system to determine the next action to execute.
Only because I like to use this approach doesn ́t mean it is the best approach.
Make your decisions on a case by case basis, and make sure you don ́t make things more complicated than they have to be.
In the next and final chapter in this part of the book, we will go into much more detail on the topic of “Vertical Slice Architec- ture” and how to prevent and manage coupling. We could also name the chapter “Modularity done right” or “Microservices
155

UNDERSTANDING EVENTSOURCING
done right”, as we ́ll explore some of the benefit (and potential drawbacks) of this special way to structure your code base.
156

10
Vertical Slicing
As of my best knowledge, the term “Vertical Slice Architecture” was mainly coined by Jimmy Bogard in a series of articles around 201928. The main statement is: “Minimize coupling between slices, and maximize coupling within a slice.”
If you hear “maximize coupling” all alarm bells should ring, and we will dive into the concept and find out what it actually means in a second.
The system we will build in Part III of the book will be based on a vertically sliced architecture.
There are many ways to structure a codebase, but all have in common that they try to isolate certain aspects of the system and shield them from changes in other parts of the system. In
28 https://www.jimmybogard.com/vertical-slice-architecture/ 157
 
UNDERSTANDING EVENTSOURCING
the end, the goal we try to achieve with all our architecture work is minimizing coupling between different parts of the system.
The leading Principle here is the well-known “Open-Closed- Principle”(OCP)firstdefinedbyBertrandMeyerin1988.29 The Open-Closed-Principle basically states, that any part of the system should be open for extension, but closed for modification. When we extend the system, we typically do not modify existing code if possible.
About two decades ago, most projects I was involved with were using the well-known layered architecture, which isolates the presentation layer from the domain/service layer and again from the persistence layer.
Fig. 10.1 - Layered Architecture
Reading the documentation of Spring Integration for example, layered architecture is advocated as a best practice.
29 https://en.wikipedia.org/wiki/Open%E2%80%93closed_principle 158
  
VERTICAL SLICING
“From a vertical perspective, a layered architecture fa- cilitates separation of concerns, and interface-based contracts between layers promote loose coupling. Spring- based applications are typically designed this way, and the Spring framework and portfolio provide a strong foundation for following this best practice for the full stack of an enterprise application.”30
I personally don ́t necessarily agree, but I ́m not here to judge any architecture approach. Architecture is always a tradeoff and the way to find the best solution for a problem at hand.
How does layering decouple our system?
Back in the days when this was still widely used, it was common practice to have a “UI” team and a “Service” team. I even worked on a project where there was a complete team named “Lacosa,” taking care of the “Layer for Common Service Access”. Quite funny looking back and thinking about it nowadays.
These teams worked in isolation and only met during planning meetings, where they agreed on the next features to build. This approach created a massive amount of coupling between all teams involved, as a change in one part of the system had to be implemented in all other layers by different teams. This was only possible with a massive communication overhead.
Coupling is also an issue even if one single team is responsible for all layers. A change in the persistence layer might spread
30 https://docs.spring.io/spring-integration/reference/overview.html 159
  
UNDERSTANDING EVENTSOURCING
through all layers and have a huge impact on the entire codebase. I very well remember a user story I was assigned (pull principle didn’t really exist back then), where I had to add a single field to the “Address Entity” and the “Address Data-Transfer-Object” (DTO) that was used as an exchange mechanism between layers.
Guess what—the Address DTO was used everywhere in the service layer. The effect was crazy. Adding this simple attribute affected all modules and all places where the DTO was used, and there were plenty.
Even worse, the effect bubbled up to the presentation layer, where there were even more changes to be made and more mappings to be adjusted. The initial task resulted in almost a week of meaningless work and delivered almost zero value. The worst thing was that everybody accepted it, because that’s how software development worked.
Fig. 10.2 - Bubbling Effect of Changes
 160

VERTICAL SLICING
This real-life story beautifully illustrates the disastrous effects of coupling in a project.
There are many ways to define coupling and there are many words used to describe it, most of which no one can remember (afferent coupling , efferent coupling are just 2 examples - feel free to look them up).
I’ll define coupling in very simple terms:
Every time you change one thing, you always have to change this other thing.
If coupling becomes too widespread and too strong, one change requires a plethora of follow-up changes that have to be done in the right order. And let’s just hope you don’t forget one.
All the architectural styles like Hexagonal, Onion, Ports & Adapters and Clean Architecture have the same goal: to min- imize the impact of changes by minimizing direct coupling between components. We can’t get rid of coupling altogether. A system without coupling is a system that does nothing.
“Vertical Slice Architecture” is no exception. We just use a slightly different approach. Instead of separating different concerns in the system from one another, we divide the system into features or slices and fully embrace the coupling within one slice of the system.
 161

UNDERSTANDING EVENTSOURCING
 Fig. 10.3 - Vertical Slices
One of the best analogies I’ve found that immediately resonates with developers is treating a vertical slice like a microservice, just on a much smaller scale. Vertical slices are what mi- croservices should have been from the beginning—an isolated functional building block for one specific feature or business capability.
Vertical slices contain all the previously discussed layers, includ- ing persistence, service logic, controllers, and the UI - just all in the same module.
If your UI is based on a component framework like Apache Wicket, putting the UI components directly into the slice’s
162

VERTICAL SLICING
package makes total sense. If there is a technology gap, such as using React for the frontend, we might physically place the UI in a different location, but the folder structure typically still mimics the slice structure.
This makes it very easy to work within one slice, as everything is directly accessible. That is the type of coupling we want.
But how do we organize the codebase?
We will do the important work in Part II of the book when we model the system. It turns out that systems planned with Event Modeling is a perfect fit for Vertical Slice Architectures, as we naturally divide the system into write operations (“State Changes”), read operations (“State Views”) and automations. Each of these elements will be implemented as vertical slices in one monolithic system.
So our vertical slices will be on a very small scale. Every write operation, every read operation will be a dedicated slice. If we break down a business process and model it using Event Modeling, every process step can essentially be implemented as a dedicated vertical slice.
One question that often comes up is whether this approach is maintainable with so many packages. And it’s true—there are a lot of slices and a lot of packages. However, when combined with an event model, it becomes very easy and natural to navigate the codebase. There is always exactly one place where the code belongs, making it simple to manage.
163

UNDERSTANDING EVENTSOURCING
 Fig. 10.4 - “State Change” slice
A slice like “Add Item,” modeled in Fig. 10.4 basically defines the data we need to perform the write operation and also what data is persisted. The slice can be implemented independently from other slices in a dedicated package.
164

VERTICAL SLICING
 Fig. 10.5 - Package Structure in a Vercial Slice Architecture
Each operation, like “add item,” “archive item” or the view for “cart items” will be placed into a dedicated package. We prevent direct dependencies between slices as far as possible using Spring Modulith31. You will learn more about Spring Modulith in chapter 19.
I personally do not enforce a predefined structure for slices internally. Basically, there is only one rule: everything that resides in the top-level package of a slice is exposed and can be used by other slices. Everything else is hidden by default and must only be used within the slice.
However, I do allow the common dependency to the domain package containing the aggregates for example. So slices are not fully independent, but all dependencies are strictly defined and controlled.
31 https://spring.io/projects/spring-modulith 165
 
UNDERSTANDING EVENTSOURCING
Working without coupling means freedom. I ́m constantly refining the structure to reduce coupling, so this structure might evolve in the future.
166

VERTICAL SLICING
 Fig. 10.6 - Controlled “common” dependencies
Should you always use Vertical Slice Architectures?
167

UNDERSTANDING EVENTSOURCING
I stopped using the word “always” a long time ago. You should just look for the best tool for the job, and while vertical slices can help tremendously, they’re not ideal for every situation for sure.
Benefits of using Vertical Slices:
Clearly structured codebase
Vertical Slice Architectures typically result in a very structured codebase and clear responsibilities for slices. Every class has a clearly defined place and responsibility. In combination with the Event Model, there’s typically no question about where a functionality has to be placed.
Minimizing coupling
We fully embrace coupling within a slice and minimize coupling between slices. Slices expose a limited set of interfaces that others can use.
One typical dependency between slices is an automation that uses the corresponding Read Model or “State-View” to query data. For this the Read Model exposes a specific query interface. In Fig. 10.7 - Slice B depends on the exposed Read Model of Slice A. You will see a practical example of how to apply this in code in chapter 22.
168

VERTICAL SLICING
 Fig 10.7 - “allowed” dependency
Scalable development
With this approach, adding new developers increases velocity as slices can be implemented mostly in isolation, requiring little onboarding. The event model fully specifies how a slice should behave and defines the test cases. This is typically enough to implement the slice.
Testable codebase
We define the behavior of vertical slices using “Given / When / Then” and treat slices as Black Boxes from a testing perspective. Using GWTs, we fully describe slices at their edges (“what data comes in as GIVEN”, “what data goes out as THEN” ).
Example:
169

UNDERSTANDING EVENTSOURCING
- GIVEN: Item Added Event
- WHEN: We remove an item
- THEN: We expect the Item Removed Event to be
persisted.
This makes slices very stable against refactorings, as these test cases describe the behavior, not the implementation. Even if a slice is completely deleted and rebuilt from scratch, these tests will typically continue to work.
Drawbacks of using Vertical Slices
Code duplication
Since we try to minimize coupling between slices, we need to be careful when we have common code between slices. Code reuse is not a bad thing in general, but it reintroduces the coupling we try so hard to prevent. Sometimes it’s easier to just copy some code and accept the redundancy in favor of preventing coupling and keeping slices independent.
Tooling
You need some tools to handle the dependencies and enforce rules. Without these guardrails, it’s too easy to let one wrong dependency slip through, followed by the next. We use Spring Modulith, but there are many other tools you can use to enforce module/slice boundaries.
The right tools make it hard to do the wrong thing. 170

VERTICAL SLICING
Uncle Bob Martin ́s “Clean Architecture”
Chapter 34 in Uncle Bob Martin’s book Clean Architecture, titled “The Missing Chapter” 32 ( contributed by Simon Brown ) describes “package by component” as a preferred architecture style. This approach essentially mirrors a vertically sliced architecture but with a slightly different perspective from the one previously described. There is a talk by Simon that addresses this topic quite well33.
The chapter focuses on proper encapsulation and the impor- tance of providing a clear interface for each component.
Simon discusses concepts like “order” or “payment,” which can be encapsulated within a single package by using appropriate Java access modifiers like private or, at the very least, package- private, and hiding all implementation details behind a clean interface.
If you want to access the persistence layer of a component, the only way to do that from outside the component is via the provided API, which can be a REST-Controller or something completely different.
Simon Brown’s definition of a component:
“A grouping of related functionality behind a clean inter-
32 https://www.amazon.de/Clean-Architecture-Craftsmans-Software-Struct ure/dp/0134494164
33 https://www.youtube.com/watch?v=5OjqD-ow8GE 171
  
UNDERSTANDING EVENTSOURCING
face, which resides inside an execution environment such as an application.”
This is essentially the same concept we’ve been discussing in this chapter, except that we further break down each concept into individual process steps that are implemented as vertical slices.
Conclusion
Using a vertical slice approach isn’t a silver bullet, but it’s a valuable tool in the developer’s toolbox. Try it and see if it fits your needs. In my experience, it works extremely well in combination with Event Modeling, as both complement each other perfectly.
For me, the greatest advantage of a vertically sliced architecture is that it avoids the crippling effect of changes, which is common in a layered approach. I will never ever do this kind of “DTO- Refactoring” mentioned in the introduction again. Changing requirements typically impact only one or a few slices. Since slices are small and focused, it’s often feasible to rewrite a slice from scratch rather than refactoring it. My goal is to keep each slice to about a day’s worth of work, at most.
You’ll see in Part III of the book how slices are implemented. This chapter concludes Part I. In Part II, we’ll model the system using Event Modeling—slice by slice.
172

II Modeling the System

11
Brainstorming
Finally, we are here to start the real work. We have learned a lot in the foundations part of the book. We have learned what it means to work with event-sourced applications in theory, how they differ, and what benefits and challenges they offer. We have learned about Domain-Driven Design, and what it actually means for event-sourced applications.
We have learned a good portion about consistency and eventual consistency. We have learned CQRS and how it fits into our architecture. We have learned the anatomy of an event-sourced application and, last but not least, we have learned why all this is important to us.
Now it is time to put everything we have learned into practice and move directly into the trenches.
In this part of the book, we will start with a blank canvas. We will plan our system from scratch together with the business experts. We will collect requirements, and we will model the
175

UNDERSTANDING EVENTSOURCING
system like we do in real-world projects.
This is how we work; this is the essence of the last 15 years of experience. I’m not saying it’s the best way; I’m just saying it’s my way.
I call this whole process “accelerated”. Using a consistent approach for planning, implementing and testing systems is such an edge for your software development.
The worst thing you can do is immediately start with the technology in mind, drawing UML diagrams, finding “the Nouns” in the systems, and trying to craft a perfect model right from the start. This is not how it works and this is the path that leads most projects into early problems.
Project Paradox
We tend to make the most important decisions at the beginning of a project. What technologies are used? What architecture is used? Do we start directly with microservices to save the later effort?
All these decisions are brutal and they require a lot of under- standing and knowledge. All of which we don’t have when the project starts. This is also called the Project Paradox and I learned about it the first time when I saw a talk by Peter Gfader34 .
34 https://beyond-agility.com/project-paradox/ 176
 
BRAINSTORMING
 Fig. 11.1 - Project Paradox by Peter Gfader
The earlier in the project, the bigger the decisions you are forced to make. Since there is not yet a lot of knowledge about the project, you will make decisions based on assumptions, which later might turn out to be false. But many of these decisions are not easily reversed. So the best approach is to defer these decisions as far as possible into the future to the point when there is much more knowledge to make educated decisions based on data and not assumptions.
We will neither start right with the code, nor will we jump right into domain-modeling sessions using UML and Noun-based Modeling. We will start by simply trying to list the facts we know about the system to be built. This is something we can
177

UNDERSTANDING EVENTSOURCING
do without making too many assumptions. This means we will bring together all people who know something about the domain to get as much collective knowledge as possible, as early as possible.
The only thing we have right now is a loose set of requirements we need to implement. Nothing more, nothing less. We are to build an e-commerce system. Sounds easy at first, right? Adding items, removing items, submitting orders. Nothing special.
But as we’ll see during the first few sessions, there is a lot of complexity lingering under the surface. And we have quite a few systems involved in the process for checking inventory, payment data, address data, processing payments, and so on. We simply pick one part of the system and start to find out more about it.
The first part of the system we’ll take a close look at is the shopping process. We already received some requirements but we do not yet know what they mean in detail.
This is what a customer should be able to do according to the business side. They gathered these requirements during a call a few days ago.
- Adding Items to a Cart
- Removing Items from a Cart
- Upgrading/Downgrading items in the Cart - Submit the Cart
- Clear the Cart
- Inventory of products can change anytime - Prices can change anytime
178

BRAINSTORMING
- If the price of an item changes, it must be removed from the cart
- Customer will receive a voucher to buy the item for the old price (Marketing)
We schedule a meeting for the next morning. We have about 15 participants from development, business, and the CEO will also take part. The more people, the better. Everybody is anxious and nobody really knows what we will be doing the next day.
Brainstorming Session
The next morning, all the people come together. We offer coffee and enough water. Everybody seems to be a bit nervous; this is not the way most people are used to work. We hand out a ton of orange sticky notes and we now try to find the “facts” of the system, which we call Events. The task is simple:
“Write down what could have happened in the system. Assume it already happened.”
There is only one rule. The sticky notes have to be formulated in the past tense. So instead of “Add Product”, we write on the sticky note “Product added”. That’s basically all the instruction we give for the first session and then we just start. At first, most people seem hesitant. So we just start and put on the first sticky notes to the board.
179

UNDERSTANDING EVENTSOURCING
 Fig. 11.2 - Starting with some Sticky Notes
It doesn’t take long for people to get used to it. This phase is actually a lot of fun. You cannot make any mistakes. It’s just about providing all the knowledge you have and putting it on the board.
After about twenty minutes into the session, everybody is involved, and some people are already eagerly discussing certain events.
Soon enough, we have many more events on the board, some are duplicates and it starts to get chaotic. That’s great!
180

BRAINSTORMING
 Fig. 11.3 - Brainstorming the domain
We probably still missed some events, but that doesn’t really matter. In the next step, we will try to bring all events in chronological order to check if they truly form a consistent story. This also happens in the first session and will be done collectively. Here, some important discussions occur as there may be disputes about the order of events.
The goal is to read the system from left to right. It should be a story that makes sense to everybody. So, let’s see how this could look:
181

UNDERSTANDING EVENTSOURCING
 Fig 11.4 - Storyboard - Bringing Events in order
Here is the story. Typically, I’ll ask one of the participants to read from left to right.
“First, an item is added to the cart. An item in the cart can be upgraded. The customer can remove items at any point in time. There is also the possibility to apply a voucher. The cart needs to continuously monitor the inventory and prices of the items. Whenever a price is changed, the item must be archived. In this case, the customer will be notified that something has changed. The customer can also clear the cart. As long as any items are in the cart, it can be submitted.”
That’s a good enough story that seems to make sense for now. We typically end the first session when we have a rough story in place. This is a perfect opportunity to pause and wait for the next session to pick up again. Most participants will remember
182

BRAINSTORMING
the story when they come back, as the human brain is pretty good at remembering stories.
In the next chapter, we will directly start to draw some first UI sketches to deepen the understanding and make discussions more approachable.
183

12
Modeling Use Cases with Wireframes
For the next session, we will limit the number of participants to a smaller group of about 4-6 people. While it’s important to have as many people as possible in the room for brainstorming, once that phase is complete, we can work more effectively with smaller groups. We need to ensure we have enough people with the necessary domain knowledge.
In this session, we will start sketching some simple wireframes for the UI. Participants are typically hesitant about this because it is time-consuming and the immediate benefits are not always apparent. Some even argue that screens are unnecessary. I personally disagree; screens help foster understanding and ensure everyone knows exactly what is being discussed. The sketches don’t need to look like the final product; they are not about perfect UX and screen design. That’s the work of experts later. Our goal is to sketch how data is captured in the system and ensure a consistent data flow throughout the storyline.
184

MODELING USE CASES WITH WIREFRAMES
Modeling Approach
The products we are selling are “Premium Coffee Brands” online.
We always model from the view of one system or context. Since we are now modeling from the view of the cart system, we do not care how the coffee-products initially come into the system; That is a topic for a later session focusing on the product- catalogue. For the cart system, we can simply assume the products to be available.
We will now be looking at some of the system functionalities in detail.
Item Added:
Items can be added to the cart. If that happens, the products will be displayed. For now, we assume the products in the cart are simply displayed one item after the another.
Sketching helps us see and discuss what data is needed to make the system usable. It’s essential to include an image, description and pricing, for example.
185

UNDERSTANDING EVENTSOURCING
 Fig. 12.1 - simple Wireframe
We can gain valuable insights into the system by examining this simple wireframe. Here are some key features:
• Thetotalpriceisobviouslycalculatedasthesumofallitems. • The quantity for each item can be adjusted.
• There is an indicator showing the current inventory status.
Item Removed:
We’ll need a button to remove an item.
186

Submit Cart:
MODELING USE CASES WITH WIREFRAMES
 Fig. 12.2 - “remove Item” button
Additionally, in Fig. 12.2 you can see the button to submit the cart. Users can add items, remove items, and submit the cart.
Let’s take these simple use cases and work on them in detail. We can switch between wireframing and detailed modeling on a per-session basis or even within a single session as needed.
187

UNDERSTANDING EVENTSOURCING
Commands & Read Models
In the next phase, we’ll define the command & read models in our system. Remember, commands are the actions that can be executed in the system. Every event stored in the system is a result of a command that has been processed successfully.
Commands:
Ask for any event: “what command must have been processed for this event to be stored in the system?”
Example:
- Event: “Item Added”
- Command: “Add Item”
188

MODELING USE CASES WITH WIREFRAMES
 Fig. 12.3 - Defining the first commands
If the “Add Item” command is processed successfully, the “Item Added” event is stored.
Read Models:
Looking at the screens in Fig. 12.4, we work backwards to de- termine what data is necessary to display the marked elements like name and price on the screen.
189

UNDERSTANDING EVENTSOURCING
 Fig. 12.4- using screens to define data
I often mark the elements on the screen in green, as shown in Fig. 12.5, so everyone knows exactly what data we are discussing at the moment.
190

MODELING USE CASES WITH WIREFRAMES
 Fig. 12.5 - making it very clear what is being discussed
It ́s immediately clear, what is the necessary data for the screen in Fig. 12.5: image, description, price and totalPrice
We define these fields on the read model.
191

UNDERSTANDING EVENTSOURCING
 Fig 12.7 - defining necessary data
Where does this data come from? Ask backwards again: What data must have been stored in the event(s) to populate the read model?
There is a special case here. What about the total price?
For now, we don’t focus on the price calculation logic. We only care that all information is available. We know that the total price can be derived from the single item prices. In the simplest case by just summing up all item prices. We can model the totalPrice on the Event or assume it is derived from the itemPrice in the read model. Right now we don ́t have enough information to decide, so either way is fine. Our goal is to make sure that all data is mapped.
192

MODELING USE CASES WITH WIREFRAMES
We decide to define the totalPrice only on the read model for now.
Fig 12.8 - Defining the Event Data
Finally, ask: What data must be provided in the command to populate the event correctly? The command must provide all necessary data to populate the event. So, it’s the same data as defined in the Event.
 193

UNDERSTANDING EVENTSOURCING
 Fig. 12.9 - Defining Command Data
Backwards thinking is powerful as it focuses on the solution rather than the problem. There is no distraction. We only focus on the data. This approach often leads to simpler solutions to achieve a desired outcome.
This is absolutely essential, so let me rephrase it again - through- out the session, we focus on data flow rather than technologies like databases, REST, or messaging. We are only interested in how information flows through our system, something both developers and business stakeholders understand well.
From here, we can either model the entire flow in several sessions or model the flow part by part in detail and start implementing some of the modeled parts as a Proof of Concept (PoC).
What works best highly depends on the project plan and timeline. Generally, the more time we spend on the model, the better.
194

MODELING USE CASES WITH WIREFRAMES
The model guides the implementation and serves as our spec- ification. Therefore, every minute spent refining the model is well-invested and saves costly development time later.
Providing a big picture of the whole system gives valuable insights, so let’s stay in the model a little bit longer and do some more upfront work.
Modeling Use Cases
We already have a good imagination of how the basic flow actually works. We decide to spend some time modeling more use cases, before we start to implement the first parts.
We ́ll now go use case by use case and repeatedly apply the four Event Modeling Patterns we learned in chapter 3.
Requirement: Remove Item
Removing an Item is straight forward, as we already sketched the button in the UI. To model this use case, I typically mark the area on the screen that is in focus in blue. This means, only the button marked blue is relevant for the moment.
195

UNDERSTANDING EVENTSOURCING
 Fig. 12.10 - Use Case “Item Removed”
Again ask the backwards-question: What command needs to be triggered for the “Item-Removed” event to have happened?
196

MODELING USE CASES WITH WIREFRAMES
 Fig. 12.11 - Mapping out the Storyline
It is “Remove Item”. But how do we know, which item was removed?
197

UNDERSTANDING EVENTSOURCING
 Fig. 12.12 - Connecting elements
Obviously we need some kind of identifier in the event. Of course, by using the information completeness check again, we can add the appropriate attribute. However, we currently have no idea where this identifier is coming from. The red arrow from “Remove Item” to “Item Removed” in Fig. 12.13 clearly indicates that information is missing.
198

MODELING USE CASES WITH WIREFRAMES
 Fig. 12.13 - Information Completeness Check
It ́s clear that the command needs to deliver the “itemId” to the event, so let ́s add it there. But this does not really help, since we just moved the red arrow one hierarchy further up to the UI. How can the UI provide the “itemId” to the command?
199

UNDERSTANDING EVENTSOURCING
 Fig. 12.14 - Defining the data trail
What we ultimately need to do is find the source of the data. Where does it really come from? And in this case, the source is the “Item Added” event. When adding an item, we need to pass the “itemId” along, so we can work with it in later steps.
200

MODELING USE CASES WITH WIREFRAMES
 Fig. 12.15 - Uncovering the data source
Its important to show, how the system behaves. Event Modeling is like sitting with your teammates in front of the screen and clicking through the already available software. Even if only on a whiteboard. Here it makes sense to show the empty screen after the item removed event happened.
201

UNDERSTANDING EVENTSOURCING
 Fig. 12.16 - Working with examples
And here we also see, that we are reusing the “cart items” read model for both screens. I typically try to make it crystal-clear that these models are the same by linking the elements in the Event Model. This is indicated by the blue arrow in the upper corner on the left “cart items”-read model. Currently the cart is empty. To model the next use cases in the upcoming chapters, we again need an item in the cart. Instead of just assuming it in the cart, we copy the “add item”slice again, so we know exactly where the item in the cart came from. Working with examples, like we would with the real software.
Using the element links here proved beneficial to me and helps to keep the model structured, but its not mandatory. It ́s just a visual indicator to keep track of duplicated elements. Feel free to
202

MODELING USE CASES WITH WIREFRAMES
define your way how to mark elements as being the duplicates. Maybe its enough to just use the name, although that proved to be problematic, when elements are renamed later.
Fig. 12.17 - Duplicating elements to improve readability
So we are going use case by use case and work with a lot of examples. Don ́t be afraid to copy elements in the model if it improves the understanding by adding additional workflow steps. It ́s always better to provide more examples. More examples means less assumptions.
In the next chapter, we will be talking about some specific business rules for adding items and how to model them in the Event Model using “Given / When / Thens”.
 203

13
“Given / When / Then” Scenarios
Now that we’ve implemented the ability to add and remove items from the cart, it’s time to define our first scenarios to capture key business rules. This is a critical step in using Event Modeling and one of its most significant advantages.
A recent experience illustrates the value of this approach: we were working with a customer to model a system and clarify the intended functionality. A business meeting was unexpectedly canceled, giving one of the business experts some extra time. He used it productively by revisiting the Event Model and adding several important Given-When-Then (GWT) scenarios. This was a perfect example of the model’s power.
Instead of scattering important business rules across multiple people, JIRA tickets and workflow descriptions, we define them directly within the model—precisely where the system’s behav- ior is outlined—in an easily readable format.
Let’s quickly recap a key concept from chapter 3: 204

“GIVEN / WHEN / THEN” SCENARIOS
Given / When / Then, originating from behavior-driven development (BDD), helps express simple business rules in this form: “GIVEN something has already occurred, WHEN a new event happens, THEN we expect the system to transition into a specific state.”
To effectively define scenarios, it’s crucial to break the system into its smallest functional units—the building blocks. In Event Modeling, we refer to these as “slices”. Slices are either State Changes (where a user or an automated process modifies the system’s state) or State Views (where the system’s stored data is used to generate views or projections). As we discussed briefly in Chapter 4 on CQS/CQRS, changing the system and reading the system should remain separate.
In Event Modeling, we naturally define systems using slices, as each step in a process is laid out consistently along a timeline from left to right. Process steps translate into slices in the system. I usually represent slices with simple boxes that encapsulate the relevant commands or read models, making it easy to name and track them. These slices may eventually evolve into stories for implementation, though at this stage, the focus remains on modeling, not implementation.
205

UNDERSTANDING EVENTSOURCING
 Fig. 13.1 - defining and naming slices
For each slice, we can now define “Given / When / Then” scenarios independently. This is crucial because we define these scenarios during workshops alongside business stakeholders (or, as in the earlier example, sometimes they define them themselves). These stakeholders possess the necessary knowl- edge and understand the business rules, making them the ideal collaborators for visualizing these rules within the Event Model.
Given
A set of events that brings the system into a specific state. For example, to verify that an item can be removed from the cart, we need to ensure that the scenario starts with at least one item in the cart. This can be easily simulated by defining an “Item Added” event as a precondition for the scenario. If the system doesn’t require a specific state, the “Given” step can be omitted.
206

“GIVEN / WHEN / THEN” SCENARIOS
 Fig. 13.2 - Example - Defining a GIVEN for a scenario
When
“When” always defines a command. Once the system is in the state described by the events in “Given,” executing the com- mand defined in “When” should lead to the outcome specified in “Then.” For read model and automation tests, the “When” step is typically omitted, leaving a “Given / Then” scenario. In such cases, it’s sufficient to put the system into the desired state and verify that the read model shows the correct information.
Then
This step either defines one or more events resulting from the command execution, or it specifies an error if the system was intentionally placed in an invalid state during the execution. Now, let’s consider the “Add Item” slice. We can define the following scenario:
207

UNDERSTANDING EVENTSOURCING
“Given an empty cart ( GIVEN omitted ), when I execute the ‘Add Item’ command, I expect the ‘Item Added’ event to be stored.”
Fig. 13.3 - Add Item Scenario
We usually place scenarios directly below each slice to maintain readability as the model progresses from left to right.
 208

“GIVEN / WHEN / THEN” SCENARIOS
During the session, we asked the business stakeholders if there were any additional rules regarding adding products to the cart. They immediately pointed out a crucial restriction: a customer can never add more than three items to the cart.
This limitation stems from the Fulfillment System, though the exact reasons weren’t clear at the time. However, we’ll retain this as a key business rule for now. All known business rules should be documented as Given / When / Then (GWT) scenarios. Let’s define the following scenario:
“GIVEN there are already 3 items in the cart, WHEN the ‘Add Item’ command is executed, THEN we expect an error, as the system is in an invalid state.”
209

UNDERSTANDING EVENTSOURCING
 Fig. 13.4 - Business Rule: Max 3 items per cart
It is immediately clear what this rule means. Reading GWTs is very expressive and visual. And again, we put the scenario
210

“GIVEN / WHEN / THEN” SCENARIOS
under the slice it belongs to. I typically also link the elements in the scenario to the elements in the Event Model, which allows to easily navigate from a scenario to the corresponding model elements.
We can define as many scenarios as necessary - don ́t save on them. They are the real treasury in Event Models.
Fig. 13.5 - Stacking scenarios vertically below slices
Since we have the Read Model, we can also directly define the “Given / Then” scenarios for the “Cart Items” slice.
• GIVEN an ‘Item Added’ event, THEN we expect the Read Model to show one item.
• GIVENan‘ItemAdded’eventfollowedbyan‘ItemRemoved’ 211
 
UNDERSTANDING EVENTSOURCING
event, THEN we expect the Read Model to show no items.
In this case, we can even extend the scenario with clear example data. For instance, we need to ensure the price calculation in the Read Model is accurate.
• IfIaddanitempricedat“5,00€”Iexpectthetotalpricein the Read Model to be “5,00 €”
• IfIthenremovethisitem,thetotalpriceshouldreturnto zero.
The key is to provide clear, concrete examples so it’s easy to understand which rules apply to different parts of the system.
Fig. 13.6 - Defining example data in scenarios
 212

“GIVEN / WHEN / THEN” SCENARIOS
Additional Context Information for “Given / When / Then” Scenarios
Sometimes providing additional context information can be helpful and provide some more documentation for a scenario.
If I want to provide additional information, I place them on a white sticky note directly in the scenario. ( And if you wonder, where the “Cart Created”-event in the scenario came from, you ́ll find the answer in chapter 15 )
213

UNDERSTANDING EVENTSOURCING
 Fig. 13.7 - additional context information for scenarios
Conclusion
The combination of slices and “Given / When / Then”-scenarios enables us to fully describe a system from a purely behavioral perspective. By breaking the system down into its smallest func- tional units (slices), we can define precise business rules that require no interpretation. This is done using simple, accessible
214

“GIVEN / WHEN / THEN” SCENARIOS
notation that anyone in the company will understand easily. These scenarios are typically translated into unit tests during implementation to ensure the system functions as expected, ideally using the example data provided. Any functional block of the system can be comprehensively described through a list of GWT scenarios.
In the upcoming chapters, we’ll explore various use cases and how to model them effectively. We’ll also provide extensive GWT definitions, which will serve as a foundation during the implementation phase.
215

14
Use Case: Clear Cart
The customer should be able to clear the cart at any point in time using a simple click on a button. The event we use to record that the cart was cleared is “Cart Cleared”. First, we’ll define the simplistic UI for this process step using a new button marked in blue in Fig. 14.1.
216

USE CASE: CLEAR CART
 Fig. 14.1 - Clear Cart
Since we now set the context for this slice, let ́s define the “Clear Cart” command including the necessary attributes.
217

UNDERSTANDING EVENTSOURCING
 Fig. 14.2 - identifying the missing session-identifier
We need to identify which cart was cleared. So far, we haven’t defined the concept of a “Cart Identifier” or “Cart Session”. Since this identifier uniquely ties together all events within a shopping-cart session, we’ll call it “aggregateId” for now. I use the term “aggregateId” as an identifier of a business concept like the shopping-process or the cart-session.
If using terms like “aggregateId” is too technical and
218

USE CASE: CLEAR CART
causes confusion for business stakeholders, it’s best to set them aside for now and stick with something simpler, like “cart-id”.
Fig. 14.3 - Using the information completeness check to extend the model
Again, the information completeness check forces us to look at 219
 
UNDERSTANDING EVENTSOURCING
the data flow. Just defining the “aggregateId” attribute isn’t enough. We need to define it consistently throughout the Event Model.
Remember - using the information completeness check, for each element and each attribute we ensure a clear data-path.
We now also realize that the “cart-items” Read Model gets its data not only from the “Item Added” and “Item Removed” Events, but also from the “Cart Cleared”-event. They all have an impact on the items in the cart.
220

USE CASE: CLEAR CART
 Fig. 14.5 - clearly defining data sources
What is a Read Model actually? Most people new to Event Modeling (especially developers) far too often think of Read Models as something technical in the first place. It ́s too easy to drift into implementation details.
If you are a developer, you could imagine a Read Model to be a table in a relational database system, and very often this will be the case. But “how” the Read Model is implemented later does not have any relevance for the Event Model itself. It ́s just data we query from the system.
221

UNDERSTANDING EVENTSOURCING
Staying away from the implementation details oftentimes is confusing at first. It can be helpful to additionally visualize Read Models using a table representation with example data as showcased in Fig. 14.6. This approach gives developers something more familiar to work with, especially if an already existing system is modeled.
However, you need to be cautious when introducing these “technology” hints in the Event Model. By doing so, we may unintentionally suggest a specific implementation, even though the final solution might differ. For example, the system may not
end up using a single relational table but instead several tables, or it might even utilize a document database, which could be a better fit for this use case.
Fig. 14.6 - Additionally providing table representations to foster understanding
Still - it is crucial to understand that the Read Model just defines the data that is necessary for a certain use case and where this
 222

USE CASE: CLEAR CART
data is coming from. In this case, the Read Model for “cart items” projects the data from three different Events (and maybe even more later) to a data view that can be used for further processing.
We can define the behavior of the slice by providing “Given / When / Then”-Scenarios for the new slice “clear cart”.
“GIVEN an item was added to the cart, WHEN the cart is cleared, THEN we expect the “cart cleared” event to be stored.”
223

UNDERSTANDING EVENTSOURCING
 Fig. 14.7- Defining scenario for “clear cart”
In addition to that, we can also provide an additional “Given / Then”-scenario for the “cart-items” read model. It makes sense to provide example data on each element to describe the use case in as much detail as possible.
“GIVEN an item was added to the cart, and GIVEN the cart
224

USE CASE: CLEAR CART
was cleared, we expect the cart to be empty.”
Fig. 14.8 - Given / Then for Read Models using examples
Conclusion
Modeling this behavior was straightforward, since we could sim- ply reuse the already existing Read Model and just provide some additional information for it. We also learned that oftentimes we don’t have all the information in the model from the beginning, like the “aggregateId” for the cart session. The information completeness check helps us and shows us exactly where we need to put the additional attributes. In the next chapter, we’ll be modeling the cart submission.
 225

15
Use Case: Submit Cart
In this chapter, we will model how the cart is submitted. Since the “cart submission” is something other systems might be interested in, we will also apply the concept of internal and external events discussed in chapter 5.
Typically, we strictly differentiate between events in our internal system, which allow us to capture state changes, and external events, which allow us to communicate between systems. As always, we first mark the UX elements that are important for this slice of functionality. In this case, it is the ‘Order now’ button to submit the Cart.
226

USE CASE: SUBMIT CART
 Fig. 15.1 - Extending the UI
The “Submit Cart” functionality consists of the ‘Submit Cart’ command and results in the ‘Cart Submitted’ event being persisted.
227

UNDERSTANDING EVENTSOURCING
 Fig. 15.2 - Defining the “State Change” Slice
In the “Submit Cart” command, we maintain a list of “ordered products.” In Fig. 15.3 you see an example of how I typically model complex data structures in the Event Model. An ordered product contains the “product id” and the “product price” at the time of the order:
I often provide the structure of the data as an example in the form of simple JSON. It’s not always easy to keep the exam- ples easily readable, so feel free to experiment with different approaches until you find something that works for you.
 {"productId": UUID, "price": Double}
228

USE CASE: SUBMIT CART
 Fig. 15.3 - example complex data structure
If you look closely, the “Cart Submitted” event contains the attribute “totalPrice,” which does not come with the command. This attribute is calculated during the cart submission process. That’s just one possible design decision. Another option is to calculate the total price whenever an item is added or removed from the cart, and so to maintain the current total price as part of the cart session. Before we look at how to externalize the functionality, let’s define another set of GWTs for the cart submission:
GIVEN an item was added to the cart, WHEN the cart is submitted, THEN we expect the Cart Submitted event to be stored.
229

UNDERSTANDING EVENTSOURCING
 Fig. 15.4 - GWT for “Cart Submission”
Now let’s talk to our business experts for a second. Are you aware of any rules that must always be fulfilled when a cart is submitted?
“Well, obviously, we should not submit an empty cart.”
Good point! Of course, it makes no sense to submit an empty cart. Let’s define the GWT for that as well:
GIVEN no item was added to the cart, WHEN the cart is submitted, THEN we expect an error: empty carts must never be submitted.
Later the button to submit the cart should not even be enabled if the cart is empty. If somehow the command gets executed anyway, the system should throw an error as we are clearly in an invalid state.
230

USE CASE: SUBMIT CART
 Fig. 15.5 - Error Case for empty Cart
Anything else?
“We already discussed the rule that we must never have more than three items, didn’t we?”
Good point again, but we already considered this while adding items. Checking it here again would be redundant and wouldn’t bring any additional benefit. That’s an important point— generally, we trust the information stored in the system, as we ensure the business rules are applied when data is written, not when it is read.
External Events for the Order System
231

UNDERSTANDING EVENTSOURCING
We already know there is at least one other system—the order system—and potentially many more that are interested in the “Submitted Cart”. Since we learned that we should strictly distinguish between internal and external events when possible,
how would this look like for the cart system?
We certainly do not want the order system to have to rebuild the cart from scratch using all the low-level events from the cart internals, like ‘Item Added.’ The order system should ideally receive a prebuilt cart that it can directly use and process. The external event should contain all the information necessary to process the order based on this cart and, if possible, also the delta, so external systems can make easier decisions.
How to design external events is a topic of its own, and I can only provide personal suggestions from my experience.
In my opinion, external events should not be sparse; they should contain all the data a system needs to act and make decisions. For example, in the case of the submitted cart, the event should include all the necessary data for the order system to process a cart submission, including prices, products, etc.
To model this, we will need the Event Modeling “Automation Pattern” we discussed in Chapter 3. In this case, the automa- tion takes care of translating internal events into an external representation.
We define a read model that provides the necessary data for export, an automation processor that handles the translation, and a command that processes the data to create the external
232

USE CASE: SUBMIT CART
event.
Let’s start with the read model. We’ll name it ‘submitted cart data.’ It will be populated from all the internal events, including the ‘Cart Submitted’ event previously stored.
Fig. 15.6 - Providing the Automation Read Model
Let’s also define the GWT for this right away:
GIVEN an item was added to the cart, and GIVEN we submitted the cart, THEN we expect the submitted cart data to be in the read model.
 233

UNDERSTANDING EVENTSOURCING
 Fig. 15.7 - Given / Then Scenario
Now, who takes care of the processing? Since we use an “Au- tomation Pattern” whenever we interact with external systems, we will now be adding an automation processor, represented by a gear symbol.
234

USE CASE: SUBMIT CART
 Fig. 15.8 - Providing the Automation Processor
The task for the automation processor is straightforward. It retrieves all the information from the “submitted cart data” Read Model, prepares the data for publishing, and then issues a “Publish Cart” command into the system using the prepared data.
235

UNDERSTANDING EVENTSOURCING
 Fig. 15.9 - an Automation-Processor issues a Command
Oftentimes, when we talk about processors, developers imme- diately start thinking about how it will be implemented. At this point in time, it’s not really important—though that’s usually not an acceptable answer for most developers! The automation processor could either periodically poll for new “Cart Submitted” events or simply be notified whenever a new “Cart Submitted” event is emitted.
You will see different types of processor implementations in Part III and also in Part IV of the book.
The “Publish Cart” command results in an “External Cart Published” event ( Fig. 15.10 ), stored in another swimlane ( Fig. 15.11 )
236

USE CASE: SUBMIT CART
 Fig. 15.10 - External Events in yellow
We clearly separate the internal from the external events. Again, how this is implemented does not really matter for the process itself. We could store the external events in the event store as a different stream, which could be named “external_cart_{cartI d}”. Alternatively, and just as an example, we could decide to publish external events directly to Apache Kafka to make them accessible to other systems. The implementation might change multiple times over the lifetime of the project, but the process itself will likely remain pretty stable.
237

UNDERSTANDING EVENTSOURCING
 Fig. 15.11 - Publishing Process
Of course, we can also provide a GWT for this slice:
GIVEN an item was added to the cart, and the cart was submitted, THEN we expect the ‘External Cart Published’ event to be stored in the system.
238

USE CASE: SUBMIT CART
 Fig. 15.12 - Given / Then for Automations
For Automation test cases, we omit the “Then” block (same as we did for the Read Model test cases), as it gets triggered automatically by the processor. I often provide another test case for the “State Change” part of the automation, which is somehow redundant, but gives some more context in the Event Model.
239

UNDERSTANDING EVENTSOURCING
 Fig. 15.13 - Given / When / Then for the “State View” Part
We will look at how this is implemented in detail in chapter 27.
Conclusion
We achieved a lot by modeling this small slice of functionality. We touched on Automations for the first time. Automations are small pieces of functionality that run in the background and are triggered by certain events being processed automatically.
We have already prepared everything for our cart system to communicate with the order system, which will handle the complete order processing, payment processing and ultimately ensuring proper fulfillment of the customer order.
In the next chapter, we will model what happens when the 240

USE CASE: SUBMIT CART
inventory of a product suddenly drops below a given threshold. The customer should be notified about this, especially if the current cart cannot be submitted due to insufficient inventory.
241

16
Use Case: Inventory Changed
In this chapter, for the first time, we will encounter a scenario where we are being notified by an external system. Together with the business team, we will analyze in this chapter how a change in inventory will affect our system.
The inventory system is a completely different system that notifies us whenever a change in inventory occurs. Beyond that, we don’t know much about the system—it’s a black box for our department.
The business rules clearly state that a product can only be ordered if there is enough inventory available. We must not sell items that are not in stock. Again, if you’ve worked in the e- commerce space as I did, you might shake your head in disbelief. This is meant to showcase how to model certain aspects of a system, not necessarily a realistic scenario.
“So what should happen if a customer has this item in the cart when it goes out of stock?” someone asks.
242

USE CASE: INVENTORY CHANGED
These are the business rules that apply:
• Youcannotadditemstothecartthatareoutofstock.
• We do not actively remove items that are out of stock.
• Thecustomershouldseeanotificationthatthecartcontains
items that are out of stock.
• The customer must not submit a cart containing out-of-
stock items.
Typically, that’s all a developer receives in a JIRA ticket to implement this feature, without much more detail. Let’s try to provide more context by modeling the use case in the Event Model.
First, we’ll receive a notification from the inventory system. We’ll model this in the event model using an external event. Technically, this could be anything from an API call to a Kafka record or an integration database. For the model itself, the tech- nical details don’t matter—incoming data is always modeled as an external event.
We can also use any information provided by the external system. From the provided API specification (perhaps a simple Word document?), we know that the system will notify us of any changes in inventory using a “product-id” and the updated
“inventory.”
243

Translation
UNDERSTANDING EVENTSOURCING
 Fig. 16.1 - External Event
Here we have a classic use case for the “Translation Pattern” in Event Modeling. We already discussed this in chapter 3. In order to work with the external event, we’ll translate it into something internal for us. Since we have a direct translation of the event, I typically skip the read model definition and directly map the external event to an automation processor responsible for the translation step.
244

USE CASE: INVENTORY CHANGED
 Fig. 16.2 - Translation
The first crucial step is getting the external data into the cart system. This is done by the automation processor, which sends the “Change Inventory” command to the cart system (Fig. 16.2 ). From this point on, the cart system is aware of the latest inventory update.
Let’s add a new swimlane for inventories. Oftentimes, it makes sense to give things a name early. The “inventory” swimlane makes sense for now and allows us to group all inventory- specific events. Later, it might turn out to be the wrong name, but that’s not a big deal. We can adjust the model at any point based on our improved understanding.
245

UNDERSTANDING EVENTSOURCING
 Fig. 16.3
For the UI, we need a way to display the inventory information. We also need this information when adding items to the cart or when submitting the cart.
Let’s start by ensuring we can display the inventory information first.
Inventory State View
First, let’s make sure we understand why we need this informa- tion. The best way to do this is to provide a simple UX element. There is a small inventory indicator in the UI that displays how
246

USE CASE: INVENTORY CHANGED
many items are currently in stock.
Fig. 16.4 - Inventory
To display this information in the UI, we define a new state view for “Inventories” and the appropriate read model to provide the inventory information.
 247

UNDERSTANDING EVENTSOURCING
 Fig. 16.5 - Information Completeness
It’s essentially a simple list of “product-id” to inventory map- pings, such as: “product-id” 4711 has 17 items in stock.
248

USE CASE: INVENTORY CHANGED
 Fig. 16.6 - Inventories Read Model
For each item in the cart, the inventory indicator should display how many items are in stock. The “information completeness check” guides us here (indicated by the red arrow in Fig. 16.2). We need the ability to match the “product-id” from the inven- tory to an item in the cart.
We haven ́t modeled the “product-id” in the system yet. This is important. What we’ve just discovered is a mismatch in the information available long before starting the implementation. That’s one of the major benefits of using Event Modeling! The earlier we uncover these gaps in the requirements, the better.
To simplify this matching, we decide to extend the “cart items” read model with the “product-id” as additional information. Let’s add it right away.
Of course, it’s not enough to simply add the “product-id” to the read model. The information completeness check again ensures, we don’t forget to also add it to the “Item Added” event and the corresponding “Add Item” command.
249

UNDERSTANDING EVENTSOURCING
 Fig. 16.7 - filling the information gap
Since we now have the “product-id” available and a read model to provide the inventory information, the UI can simply query the data and display it. This is also enough information to notify the customer about items that are out of stock. If the inventory is zero, the UI could display a hint or notification.
250

USE CASE: INVENTORY CHANGED
Validating Inventories for ‘Add Item’ and ‘Submit Cart’
Now we need to take care of the validation logic. An item must not be added to the cart if it is out of stock. In the special case where an item goes out of stock while it is already in the user’s cart, the cart must not be submitted.
Later, the business might decide to remove the item from the cart, similar to how we will handle price changes in chapter 17. For now, we’ll keep the item in the cart. This is also an interesting case for implementation later, but for now, we are only focused on modeling how the process works.
The great thing is, we already have the information flow com- pleted. The only thing we need to add to visualize these business rules in the model is to provide the proper “Given / When / Then” definitions for the “Add Item” and “Submit Cart” slices.
GIVEN an item was added to the cart, and GIVEN the quantity of this item was decreased to zero, THEN we expect the item to stay in the cart.
We can achieve this by defining a “Given / Then” for the “cart items” Read Model. Notice in Fig. 16.8, how the inventory in the read model shows zero.
251

UNDERSTANDING EVENTSOURCING
 Fig. 16.8 - Given / Then
We define an additional test case for the “Add Item” command using our improved understanding of the business rules for the inventory case:
GIVEN the quantity of an item was decreased to zero, WHEN a user tries to add the item to the cart, THEN we expect an error to be thrown.
252

USE CASE: INVENTORY CHANGED
 Fig. 16.9 - Add Item
The last business rule we need to add is for the “Submit Cart” command:
GIVEN an item was added to the cart, and GIVEN the quantity of this item was decreased to zero, WHEN the user tries to submit the cart, THEN we expect an error.
253

UNDERSTANDING EVENTSOURCING
 Fig. 16.10 - Inventory change prevents submission
From a usability standpoint, this is not ideal, and we might want to add a visual indicator for the user. However, let’s keep it simple for now. It also might be a better idea to model it using a “Cart Submission Failed”-event instead of an error, but again, we can adjust this later.
Conclusion
In this chapter, we used the “Translation Pattern” to convert data from external systems to internal information understand- able to our system. Working with translations protects our system from changes in external systems. Developers and architects typically refer to this as the “Anti-Corruption Layer”.
A change in the external API will only immediately affect the translation slice, not the rest of the system. Consistently using
254

USE CASE: INVENTORY CHANGED
translations also allows us to integrate systems using different technology stacks and protocols. It doesn’t really matter for our event model if the external event at runtime comes as a simple HTTP call, a Kafka record, or even as a human manually entering information into a database. The “external event” is the same for all use cases.
We also looked at the validation part of the system and provided more “Given / When / Then”-scenarios to describe how the system behaves under certain circumstances. You are probably eager to see how this is implemented, but bear with me for just two more chapters. It’s crucial to clearly distinguish between how the business works and how it is implemented. I’ve stated it multiple times already: the implementation might, and will change multiple times during the lifetime of an information system. Business processes, however, tend to be much more stable and typically won’t change much (exceptions prove the rule).
Anyone reading the Event Model will know that with this slice, external information is entering the system. To understand the business process, it doesn’t matter how that actually happens.
In the next chapter, we will look at an even more interesting case—how prices change and how they affect the cart system at runtime.
255

17
Use Case: Price Changed
Whenever the price of an item changes, we must ensure the product is removed from the cart. This requires the ability to adjust all active cart sessions in the system and remove the affected product. The challenge is even more interesting by the need to track which carts contain which products, and this information must be readily accessible for the system to query.
In a typical e-commerce system, you likely wouldn’t implement such a feature unless absolutely necessary. Actively adjusting a user’s cart session is not a decision to be taken lightly. I was once involved in an e-commerce system where this exact requirement was implemented (though, as far as I know, the feature was later removed due to significant confusion for the customer).
Let’s consider an example: if a customer has a product in their cart priced at €9.90, and the price changes to €10.90, the product must be actively removed from the cart and archived. While we might use the archived products later for marketing purposes,
256

USE CASE: PRICE CHANGED
for now, archiving the product is the only requirement. It must never happen that a customer submits a cart with incorrect prices. Those are the rules; now let’s explore how to apply them.
Communication with the Pricing System
Price changes are handled by the pricing system. Similar to the inventory system, both the pricing and product systems function as black boxes from the perspective of the cart system. What we typically receive is either an API call or a notification from the pricing system indicating that the price of a product has changed. For the purposes of the Event Model, the exact mechanism doesn’t matter as long as we understand that the updated information enters the system.
The first step is to introduce the external event “Price Changed.”
Fig. 17.1 - External Price Changed Event
The event contains the “product-id,” the old price, and the new price, allowing us to easily determine if the business rules apply.
 257

UNDERSTANDING EVENTSOURCING
To work with this information, the first step is translating the external event to an internal event, which we will also call “Price Changed” for now.
Other than this translation, the cart system remains unaware of the external event.
Fig. 17.2 - Translation Pattern applied
Now that we know a price has changed, we need to determine which, if any, cart sessions are affected by the price change.
In discussions with business experts, they often use terms like “When the Cart Session was started” and “checking all active Cart Sessions.” This is an important signal that something may be missing. If people in workshops are using terms that are not
 258

USE CASE: PRICE CHANGED
reflected in the model, it is something to investigate. I usually take note of these terms and focus on them to identify any missing concepts in the Event Model.
Looking at the model, we realize we haven’t yet really modeled the concept of a cart session. While we can add items to a cart, we haven’t explicitly modeled the life cycle of a cart session, particularly we are missing how it is created.
This presents an opportunity to learn more about the system. So, let’s ask the experts: “When is a Cart Session started?”
Since no one seems to have a strong opinion on how a cart session begins, we’ll keep it simple. We can assume a cart is created when an item is added to a non-existent cart. This allows us to define the “Cart Created” event as a side effect of the “Add Item” command.
259

UNDERSTANDING EVENTSOURCING
 Fig. 17.3 - A command can have multiple outcomes
We can, of course, describe this behavior using a “Given / When / Then” definition.
GIVEN nothing has happened, WHEN an “Add Item” command is issued, THEN we expect a “Cart Created” event to be persisted, along with the “Item Added” event, if the cart session does not yet exist.
When there are multiple events in a “Given / When / Then” scenario, the order of events is important and follows a left- to-right sequence. In Fig. 17.4, the “Cart Created” event occurs before the “Item Added” event.
260

USE CASE: PRICE CHANGED
 Fig. 17.4 - ordered events in Given / When / Then
Now that we have a “Cart Created” event for every cart that gets created, it would be easy to build an “Active Cart sessions” projection, tracking all cart sessions by following these events. However, this isn’t exactly what we need. Our goal is to build the right model for our specific use case, not a general-purpose model for every use case. What we need is something that allows us to quickly map from a product with a changed price back to the affected cart sessions.
This illustrates the power of events: it’s not the model that dictates our use cases, but rather the use cases that shape our model. The model we need evolves naturally from the use cases we implement.
261

UNDERSTANDING EVENTSOURCING
For now, we don’t need to worry about the implementation details. We’ll address that later. What matters now is defining the simplest model could possibly work. So, let’s create a simple Read Model that provides the necessary information. We’ll define the Read Model “Carts with Products”, which includes a product-id and a list of cart-ids.
Fig. 17.5 - defining a use case specific Read Model
First, it’s crucial to determine whether all the necessary data is available and, more importantly, where it comes from.
The main source of this data appears to be the “Item Added” event. This event already includes a reference to the product through the “product-id” attribute. Using this, we should be able to construct the required data projection.
 262

USE CASE: PRICE CHANGED
 Fig. 17.6 - Being explicit with dependencies
We must also account for the “Item Removed” event. If an item is removed from the cart, any subsequent price changes for that product will no longer be relevant to this cart-session.
Modeling with Pen & Paper
Who takes care of removing the item? This is handled by an automation process running in the background, which reacts to the “Price Changed” event.
It’s important to emphasize that we are not modeling how this automation works—that’s the developer’s responsibility. What we focus on in Event Modeling sessions is ensuring the correct
263

UNDERSTANDING EVENTSOURCING
flow of information. I know I’m repeating this point often, but one of the biggest mistakes when starting with Event Modeling is focusing too much on implementation details.
For this automation to function, we only need:
• Thecartcontainingtheproductwiththechangedprice(cart- id)
• The product-id affected by the price change
When considering how to model this, I often ask: how would I approach it if I had only pen and paper?
With pen and paper, I would manually go through the list of active cart sessions and mark the relevant ones for later processing. This would become my “to-do list” for the task. Then, I’d go through the list from top to bottom, adjusting the cart items by erasing the affected items.
The first step is to build a list of items that need adjustment. We introduce a new command “Archive Item” (Fig. 17.7), which adjusts a single cart by removing the item. This command only requires the cartId (we called it “aggregateId”) and the affected product-id.
264

USE CASE: PRICE CHANGED
 Fig. 17.7 - Archive Item Command
The Command-Execution will result in an “Item Archived” Event.
Fig. 17.8 - Item Archived Event
By introducing this new event, we can now make use of the archived items and offer affected users a voucher to purchase the item at the old price. This is a smart marketing tactic and definitely something we should add to the backlog.
 265

UNDERSTANDING EVENTSOURCING
One crucial point remains: since the item is archived, it will no longer be displayed in the cart. To ensure this, we need to adjust the “Cart Items” Read Model we defined earlier. The Read Model will now account for the “Item Archived” event, treating it in the same way as the “Item Removed” event.
This use case, where a later event impacts an earlier defined Read Model, is quite common. There are various ways to handle this in Event Modeling. If only the data is affected (such as the number of cart sessions provided by the Read Model) and not the flow itself, I prefer to represent this by using a dotted arrow from the event pointing back to the affected Read Model to indicate the dependency.
Some other Event Modelers prefer to copy the Read Model and show these dependencies pointing forward. It all depends, and I strongly suggest finding what works best for you. Don’t treat my best practices as the only approach—there is a lot of flexibility in Event Modeling.
The dotted line indicates that this connection does not impact the modeled flow itself but only affects the data in the Read Model.
266

USE CASE: PRICE CHANGED
 Fig. 17.9 - (optional) Backlinks to visualize data flow
Conclusion
With the price change feature now modeled, we have a fairly complete understanding of how the cart system functions. We can walk through the system using the Event Model, moving from left to right, much like reading a book. This model not only helps onboard new employees but also serves as a key source of requirements for the upcoming implementation phase, which we will begin in the next section of the book. The time invested in modeling will undoubtedly prove to be one of the best investments in the project.
267

UNDERSTANDING EVENTSOURCING
 Fig. 17.10 - price change process modeled
In the next and final chapter in Part II, we’ll take a brief look at practical extensions to Event Modeling and explore some useful tools that help structure models as they begin to grow.
268

18
Structuring an Event Model
In this chapter, we will explore additional methods for structur- ing Event Models using tools not mentioned in the original arti- cle35 but that are particularly useful for maintaining structure in more complex, developed Event Models. These standards are optional and can be applied as long as they support your workflow.
Chapters and Sub-Chapters
To better understand the flow of information in the system, I tend to logically group slices together. This makes it easier to capture the big picture in an Event Model. I use blue arrows and arrange them in two layers.
35 https://eventmodeling.org/posts/what-is-event-modeling/ 269
 
UNDERSTANDING EVENTSOURCING
 Fig 18.1 - Chapters and Sub-Chapters
I started to name them chapters following a suggestion out of the Community. Another term often used is “Workflow”. I personally think, “Workflow” is a bit overloaded. Basically a chapter defines kind of a context for a given slice. I place “chap- ters” directly above the Event Model, so my eyes automatically capture the current context while reading along the timeline.
In the Event Model, we logically have the chapters for “Shop- ping” and within “Shopping” four sub-chapters for “Items,” “Inventory,” “Price Change,” and “Submission”. Very much
like the chapters in a book.
270

STRUCTURING AN EVENT MODEL
 Fig. 18.2 - Grouping Slices in chapters and sub-chapters
I like this simple way of grouping elements in the model, as it is non-invasive, does not harm readability, and can be easily adjusted when slices are added or removed. I learned this structuring method from Adam Dymitruk, who also uses this notation.
Multiple models per board
It is perfectly fine to have more than one model on a board. In fact, this is the rule rather than the exception for me. I prefer having many smaller models over one large model, as it allows more flexibility when adjustments are necessary.
The size of your model depends on your personal preference. I aim to capture one business context in each model, so I can read it from left to right without any visual interruptions. So in
271

UNDERSTANDING EVENTSOURCING
this case, I use a pink sticky note placed on the left side of each model to properly name it.
Fig. 18.3 - Model Context
Alternative Flows
In Event Modeling, we focus on one use case at a time along a single timeline. This often confuses developers new to Event
 272

STRUCTURING AN EVENT MODEL
Modeling, as it can feel like a limitation rather than a feature. Not all software follows a linear timeline — we have conditions and loops to consider. How do we model them?
The short answer: we don’t. Instead, pick one flow and model it. Model each use case as a separate flow, and keep it simple.
What does this mean in practice?
Typically, we model the “good case” first as a starting point, and all “error cases” as separate flows or simple “Given / When / Thens” if sufficient. For example, we’ve already modeled the “Submit Cart” flow, but we haven’t examined any errors that could occur.
• Dowedisplayanerrormessage
• Doweredirecttheusersomewhereforhelp?
From the current model, we can’t answer these questions, and neither can the developer who needs to implement the solution.
While discussing this issue with domain experts, we might even discover a new rule that hasn’t been mentioned before. For instance, if a customer fails to submit a cart three times due to technical issues, the cart process is aborted.
We could add this rule to the current model, but it would disrupt the flow. Most of the time, it’s easier to define a dedicated model for this.
273

UNDERSTANDING EVENTSOURCING
 Fig. 18.4 - Alternative Flows
If there are alternative flows for a certain slice, I place a marker below the slice with a link to a different model on the board. Whenever you see a sticky note like this in the model, it indicates that there are alternative flows from that point.
The link takes you to a different flow on the board named “Submit Cart Error” (Fig. 18.5). While we won’t go into detail on this flow, it essentially demonstrates how the cart behaves
in the event of an error, including the “Given / When / Then” scenarios.
274

STRUCTURING AN EVENT MODEL
 Fig. 18.5 - Providing separate flows
This approach allows you to easily jump and navigate between different models. Each model is also annotated with the context annotations, ensuring that we always know which flow we are currently discussing.
Conclusion
The tools discussed in this chapter are not part of the original Event Modeling definition. Instead, they can be seen as practical “extensions” to an Event Model. It’s important to note that I didn’t create these notations but rather selected what worked
best for me from discussions with various practitioners.
Questions about how to structure developed models arise re- peatedly, so I hope these tools prove as helpful to you as they have been for me.
This practical chapter concludes Part II of the book, and we can 275

UNDERSTANDING EVENTSOURCING
now move on to the implementation in Part III.
276

III
From zero to running software

19
Technology Stack
I struggled with how to approach Part III of the book—whether to dive deep into a specific framework and technology stack or to focus purely on the concepts without using any particular frame- work at all. In the end, I chose to stick with the stack I’m most familiar with to give you a real-world look at how I structure systems and implement certain patterns after modeling them. Even if it’s not your preferred tech stack, I hope you still find value in the principles and techniques shared in this chapter.
We finally get started and write our first code for our event- sourced application. The great thing is, in Part II of the book, we already mapped out all requirements in a way that allows us to implement them directly.
In theory, we don’t need ticket systems and constant backlog refinements anymore to clarify open questions. We could even skip daily scrums. This is a bold statement, and for most projects you will of course not throw everything out of the window immediately. But it ́s crucial to understand, that we already did
279

UNDERSTANDING EVENTSOURCING
the most important work upfront and designed and planned the system. The only thing left is implementing what was planned.
Project Tools
You will find the source code here:
https://github.com/dilgerma/eventsourcing-book
The repository will have different branches for each chapter; the main branch will always contain the latest version of the system.
Let’s briefly lay out the technologies and architecture we will be using. Don’t expect anything fancy or exotic. I typically try to keep it as simple as possible and work with mature technologies that simply work.
You will learn now about some of the technologies used in the project. Don’t worry if you are not familiar with all or any of them; we’ll learn some basics of these tools in this chapter and the rest on the way.
Tech Stack
This is not a complete introduction to each of the frameworks and tools, but mainly provides a quick overview and some links to get more insights.
280

TECHNOLOGY STACK
The concepts laid out in this chapter should be transferable to any tech stack.
Kotlin
Kotlin is our programming language of choice. You probably know or heard of Java. Kotlin is a statically typed language that runs on the Java Virtual Machine (JVM) and can be used anywhere where Java is used today. It has a very concise syntax, built-in null safety, and full interoperability with Java. Kotlin is my preferred choice for developing server-side applications due to its simplicity. I use it in most projects.
Learn More: https://kotlinlang.org/docs/home.html Spring Framework
Spring is a powerful framework for building enterprise-level applications. I ́m personally working with Spring for more than 15 years now.
It provides comprehensive infrastructure support for develop- ing Enterprise applications, allowing developers to focus on business logic rather than boilerplate code. The framework manages the whole application lifecycle and comes with de- pendency injection, transaction management, persistence and MVC support (and much more).
Learn More: https://spring.io/projects/spring-framework
281

UNDERSTANDING EVENTSOURCING
Spring Modulith
Spring Modulith is a newer addition to the Spring ecosystem designed to help developers build modular applications. It promotes a modular architecture by allowing you to divide your application into distinct modules, each with well-defined boundaries and responsibilities. This helps in maintaining and scaling large applications more effectively. We will be using Spring Modulith heavily to structure our Vertical Slice Architecture.
Learn More: https://spring.io/projects/spring-modulith Spring Data
Spring Data is another sub-project of the Spring Framework that makes it easier to work with data access technologies like relational and non-relational databases. It provides a consistent and easy-to-use data access layer by leveraging the repository pattern. We will be using Spring-Data to access and populate our database-projections. More on that in chapter 25.
Learn More: https://spring.io/projects/spring-data Axon
Axon is the framework to implement the principles Command Query Responsibility Segregation (CQRS), and Event Sourcing (ES) discussed in chapter 4. It helps in building event-driven, distributed systems by providing the necessary infrastructure for handling commands, events, and aggregates. Axon inte-
282

TECHNOLOGY STACK
grates well with Spring, making it easier to build robust and scalable applications. We will go into much more detail on Axon in the next chapter.
Learn More: https://docs.axoniq.io/ Test Containers
Test Containers is a Java library that provides lightweight, throwaway instances of common databases, Kafka, or anything else that can run in a Docker container. It enables you to write tests that interact with real instances of external dependencies, improving the reliability and accuracy of integration tests. We will be using Testcontainers in our Integration-Tests, but also to start the application locally with all dependencies automatically.
Learn More: https://www.testcontainers.org/ JUnit
JUnit is a widely-used testing framework for Java applications. It provides a simple and efficient way to write and run repeatable tests. JUnit supports annotations to identify test methods, setup and teardown methods, and assertions to test expected results. Since we will mostly work Test-First by generating our Tests, JUnit is a crucial Tool primarily for development, not for Quality Assurance.
Learn More: https://junit.org/junit5/
283

UNDERSTANDING EVENTSOURCING
PostgreSQL
PostgreSQL is an open-source and well-known relational database system. It is a popular choice for a wide range of applications from small projects to large-scale enterprise systems. PostgreSQL supports advanced data types and performance optimization features, offering flexibility in data modeling and query performance.
Learn More: https://www.postgresql.org/docs/
These components together form a robust and flexible tech stack, enabling us to build, test, and maintain scalable applica- tions effectively.
We will implement the system as a single repository and deploy it as a well-structured monolith. We’ll be using the vertical sliced architecture approach laid out in chapter 10. Let’s briefly discuss what that actually means in practice.
Vertical Slice Architecture
Now with the tech-stack in place, let ́s discuss how we will structure the codebase using a vertical sliced architecture (VSA) we introduced in chapter 10. In our system, we will fully utilize the VSA-approach. It worked amazingly well for me over the years not only in the Event Model but also in code.
284

TECHNOLOGY STACK
 Fig. 19.1 - Vertical Slices
The main idea is simple. We define isolated blocks of function- ality, which are independent from each other. We put every functional block into its own module or package, including everything that is necessary for this functionality to work, like a REST-API Controller, Services, Repositories and also the UI (if applicable).
We already defined the functional blocks in Part II by slicing the application.
If you have to change an existing feature, your changes never affect the whole codebase but only a very limited set of slices typically.
There are no direct dependencies between slices; they work in isolation, and only expose well-defined interfaces.
Since we already modeled the slices in the event model, divided into “state change”- and “state views”, we can directly trans-
285

UNDERSTANDING EVENTSOURCING
late each modeled slice to a package in our codebase.
If you closely follow the package names here like “cartitems”, “additem”, “removeitem”, you ́ll see that they directly mimic
the slices in the Event Model.
Fig. 19.2 - Package Structure
The VSA approach offers a significant advantage by eliminating dependencies between slices, making it an ideal tool for scaling development efforts. Since each slice is independent, they can be implemented in any order, with the events defining the APIs between them. This independence allows us to achieve what was previously impossible: adding more developers truly increases velocity, rather than slowing things down. Each slice can be developed autonomously, and with the event model providing a clear blueprint of the system, onboarding new developers becomes far more efficient and straightforward.
 286

TECHNOLOGY STACK
Developers can get productive within a few hours, if not in- stantly.
This all sounds great, but we’ll need to see how it looks in a real project setup to really understand what it means to work in this kind of architecture. It requires a small shift in thinking to work this way, but in the end, it hopefully will also start to make sense to you.
In the next chapter, you ́ll learn about the Axon Framework, which we will be using to build our system step by step.
287

20
Brief introduction to Axon
From this chapter onward, we’re finally diving into the imple- mentation phase, where we’ll explore key concepts in a tangible, practical way. This shift is a big reason why I wanted to write this book. Most books tend to focus on the fundamentals and remain theoretical, which isn’t wrong—but when it comes to event-sourced applications, what’s really missing are examples that show how to implement these systems in practice.
In this book, I’m using the tools that I currently rely on in my projects. These tools are simple enough to grasp in a matter of days or weeks, yet powerful enough to handle a wide range of use cases. If, in the future, the tools become outdated or I switch to new ones, I plan to update or even rewrite entire chapters to ensure the content remains useful. For now, what you’re about to read reflects how we work in my company today.
Earlier, I mentioned that Event Sourcing can be done without any framework—and that’s absolutely true. I even considered writing this book without using a framework to make it more
288

BRIEF INTRODUCTION TO AXON
universally applicable. But in doing so, I would have had to sacrifice the concrete, real-world examples that I believe are crucial. For me, providing practical, detailed insights into how I approach things was more important than broad applicability.
So, let’s take a quick tour of one of our key tools: the Axon Framework.
Axon Framework
Before we dive right into the implementation of our first slice, we will take a short tour through the Axon Framework, as the framework forms the backbone of our system. This tour is by no means complete in any way, but gives you a brief overview of the building blocks in the framework.
Also studying battle-tested event-sourcing frameworks used by thousands of teams to build systems gives you a pretty good overview of the patterns necessary that repeat over and over again, even if you will be using a different framework later.
At the time of this writing, I ́m not involved in any way with AxonIQ, the creators of the Axon-Framework.
The Axon documentation is pretty good, so I would suggest heading over to the reference guide and reading it twice from top to bottom.36
36 https://docs.axoniq.io/reference-guide 289
 
UNDERSTANDING EVENTSOURCING
The great thing is, you will see most of the concepts we learned in Part I of the book again here, as the Axon Framework provides a possible implementation for many of these patterns. It might be a good idea to jump back to Part I occasionally to be able to connect the fundamentals to the implementation.
Let ́s start to look at some of the building blocks we will be using.
Event Store
Since our application will be working with Event Sourcing, we will need an Event Store to store our events. AxonIQ, the company behind the Axon Framework, has a commercial offering for the Axon Server, which is a powerful Event Store, operated and managed either in the cloud or on premise.
In this book, we will not be using any commercial tools but a simple PostgreSQL database as our Event Store. Axon also has support for this setup. We could migrate to the Axon Server at any point in time later.
The basic interface to access the Event Store in Axon is of course “EventStore”.
 interface EventStore {
    fun readEvents(
        aggregateIdentifier: String
290

BRIEF INTRODUCTION TO AXON
     ): DomainEventStream
    fun readEvents(
        aggregateIdentifier: String,
        firstSequenceNumber: Long
    ): DomainEventStream
}
The fundamental operation of the Event Store is to read all events belonging to an aggregate or stream. In Axon, aggregates and streams are basically the same concept. Currently, Axon focuses on aggregates to group events. The Aggregate Identifier used here is basically just a string, like the “customer-id” or the “shopping-process-id”, to efficiently find all events that belong together.
Besides the possibility to read events, the Event Store allows you to append events but not modify persisted events.
When appending events, the Event Store requires an instance of a “Serializer” to handle event serialization. Event serialization is a crucial concept that certainly warrants a deep dive. However, for the purposes of this book, we will keep things simple and use plain JSON for serializing and deserializing events. This approach ensures clarity and ease of understanding while demonstrating the core concepts without getting bogged down
 protected void appendEvents(List<Event> events,
Serializer serializer)
291

UNDERSTANDING EVENTSOURCING
in serialization specifics.
Aggregates
You learned about aggregates in chapter 8 as a domain concept to ensure transactional consistency across a group of objects under the umbrella of the aggregate.
In Axon, an aggregate is essentially a simple class with a few annotations. For example, in a cart system, we might define a CartAggregate. We would create a class named CartAggregate, add the annotation @Aggregate, and ensure it has a field annotated with @AggregateIdentifier, which represents the unique ID of the aggregate. In this case, we could use a UUID, though it can be just about anything, as Axon handles it as a simple string in the background. This gives us flexibility in choosing how to identify aggregates.
With these few lines of code we defined that there is a business concept like “Cart” which is identified by a UUID.
 @Aggregate
class CartAggregate {
    @AggregateIdentifier
    lateinit var aggregateId: UUID
}
292

BRIEF INTRODUCTION TO AXON
This is enough to make Axon aware of the aggregate. Its lifecycle becomes managed by the framework. The only job of the aggregate is to ensure that any action processed in the system is valid according to our business rules.
Whenever we want to change something in the system, most of the time, an Aggregate will approve or deny our action.
Aggregates are not data holders. You do not request an aggregate if you want to know how many items a user has in the cart. The aggregate takes care of validation, not data. If you recall what we learned in Chapter 4 talking about CQRS, the aggregate lives on the “Write-Side” of things and takes care, that only validated changes come into the system.
If data is not necessary for validation, it should not be present in the aggregate.
Command Handlers
We already learned that actions in the system are represented by commands. Whenever I want the system to process an action or change its state, a command needs to be issued. The command is handled by a command handler.
Command handlers can be standalone, or they can be defined directly on the aggregate, because ultimately, the aggregate will decide if a command can be processed or not. You will see plenty of examples for this in the upcoming chapters.
293

UNDERSTANDING EVENTSOURCING
So if we wanted to add an item to our CartAggregate, we could define a simple command handler directly on the aggregate.
 @Aggregate
class CartAggregate {
    @AggregateIdentifier
    lateinit var aggregateId: UUID
    @CommandHandler
    fun handle(command: AddItemCommand) {
//.... }
}
Now, whenever a “Add Item” command is issued anywhere in the system, Axon takes care to load the aggregate, find the corresponding command handler and execute it.
Within the command handler, we will implement validations and business rules, such as checking the maximum number of items allowed in the cart. If a validation fails, we have two options: we can either throw an exception, which will immediately stop the command execution, or we can store a business event to record that the action failed. This is something that is typically modeled within the Event Model, allowing us to capture and track the failure as part of the process.
294

BRIEF INTRODUCTION TO AXON
 // example of using exceptions in the process
@CommandHandler
fun handle(command: AddItemCommand) {
    if (cartItems.size > 3) {
        throw CommandException(
} }
    "Cannot have more than 3 items"
)
If the aggregate decides to process the command, we need to store the fact that the processing took place and the item was added by storing the “ItemAddedEvent”.
 @CommandHandler
fun handle(command: AddItemCommand) {
    [...]
    AggregateLifecycle.apply(
) }
ItemAddedEvent(
    command.itemId,
    command.productId
)
We use the powerful AggregateLifecycle, which obviously controls the lifecycle of an aggregate and allows us to apply events, delete the aggregate or get information about the current state of the aggregate.
295

UNDERSTANDING EVENTSOURCING
After we call AggregateLifecycle.apply(...), the event is stored in the Event Store and is available for later processing.
It is crucial to define which command can create new instances of an aggregate. There are two possibilities to do that: ei- ther annotating the constructor of an aggregate or defining a @CreationPolicy-annotation on the Command Handler.
I personally prefer the latter, as it is easier to understand and more readable, but this is just personal preference. There is no functional difference between the two approaches to my best knowledge.
In this case, we define that every time this command handler is executed, a new instance of the Cart-Aggregate is created. An aggregate cannot be created twice with the same aggregateId. If the instance with the given aggregateId already exists, an exception is thrown.
 @CreationPolicy(AggregateCreationPolicy.ALWAYS)
@CommandHandler
fun handle(command: CreateCartCommand) {
    AggregateLifecycle.apply(
        CartCreatedEvent(command.aggregateId)
) }
event-sourcing handlers
296

BRIEF INTRODUCTION TO AXON
There are two ways to work with events: event-sourcing han- dlers and Event Handlers.
With an event-sourcing handler, we get directly to the core of event sourcing. An event-sourcing handler is typically also placed directly on an aggregate alongside the Command Handler and allows sourcing the attributes of the aggregate from the previously stored events.
Since we previously applied the ItemAdded-event, the aggregate needs to know about the items already in the cart.
 @EventSourcingHandler
fun on(event: ItemAdded) {
    this.items.add(
        CartItem(
) )
}
event.itemId,
event.productId,
event.price
Each time we load the aggregate to make a decision, all event- sourcing handlers are triggered in the order of the events in the stream, updating the aggregate to its most current state. This process is known as “hydrating.” Once all events have been processed, the aggregate reaches its “latest” state and has all the information it needs. This happens every time the aggregate is loaded. While there are also state-based aggregates,
297

UNDERSTANDING EVENTSOURCING
which persist the relevant information directly as state in a database table, in this book we will focus solely on event- sourced aggregates.
Event Handlers
Event handlers are the backbone of Axon and allow reacting to events. Technically, Axon provides the Event Processor abstraction. Event Processors are essential tools for building any kind of data-projection.
Axon mainly differentiates between subscribing and streaming Event Processors (Fig. 20.1). The key difference is that subscrib- ing Event Processors handle events in the same thread as the Command Handler, whereas streaming Event Processors are decoupled and use a dedicated thread.
Fig. 20.1 - Streaming / Subscribing Event Processor
 298

BRIEF INTRODUCTION TO AXON
Because a subscribing Event Processor uses the same thread, projections are populated immediately. Again thinking in CQRS, both the write side and the population of the read side happen atomically in the same thread (Thread-1 in Fig. 20.1). As everything happens in the same thread, our projections are either updated within the same transaction or rolled back if something goes wrong. We can rely on our database system to ensure consistency.
There can be more than one subscribing Event Processor sub- scribed to an event type. However, if any of the subscribed Event Processors fail, the entire transaction (including command processing) enters an error state, which might cause the entire business transaction to roll back.
It’s important to note that scaling subscribing Event Processors can be challenging because they block a single thread for the entire duration of execution. For most use cases, the streaming Event Processor is a better option, which is why it’s the default in Axon. It decouples command processing from building projections and handling events, which helps with scalability. However, with streaming processors, we need to consider eventual consistency, as there will be a delay between executing a command and reflecting that data across all projections.
While subscribing Event Processors aren’t inherently a bad practice, they do have some limitations. As always, it’s essential to weigh your options carefully and choose the one that best fits your needs.
The Tracking Event Processor (TEP) is a special kind of stream- 299

UNDERSTANDING EVENTSOURCING
ing Event Processor. In this book, we will primarily focus on TEP. It tracks the events that have been processed and ensures that even after a system restart, it picks up from the last processed event in the stream. Axon also manages parallelism by splitting the event stream into segments, ensuring that no two processors handle the same event simultaneously—a feature that is notoriously difficult to implement manually in a distributed system.
If you want to know more about this process and the different types of event processors, you can find a detailed explanation in the Axon Documentation.37
Conclusion
There is much more to Axon than these basic building blocks, such as Sagas, Deadline Managers to control process timeouts, serialization, concurrency, and the concept of queries. All of these concepts will be practically applied in the next few chapters.
The goal of this chapter was to give you a brief overview of what it looks like to work with the Axon Framework. I hope you feel prepared to get into the trenches and start working on our project using Axon and Event Sourcing.
In the next chapter, we will start to implement our very first
37 Axon Documentation - https://docs.axoniq.io/reference-guide/axon-fram ework/events/event-processors/streaming#tracking-tokens
 300

BRIEF INTRODUCTION TO AXON
slice ( a “State Change” slice ). At the beginning of each chapter, you will find the branch in the repository where you find the implementation of the chapter.
301

21
Implementing the first slice - “Add Item”
In this chapter, we will start implementing the first features of our application using the Event Model as our source of truth and requirements. We have laid out everything in advance through several Event Modeling sessions, allowing us to begin the implementation directly. The work can also be distributed among multiple developers to scale efforts, as the order in which slices are implemented is not important. All dependencies and contracts between slices have already been well-defined by the Events.
The Skeleton Application
In the first step, we will implement the Skeleton Application, which basically means setting up the project and providing the typical configurations necessary to be able to work.
In the following steps, you will see how we gradually build the system step by step. To get familiar with the code-base quickly,
302

IMPLEMENTING THE FIRST SLICE - “ADD ITEM”
I would recommend to check out the book repository38 from github in parellel.
You ́ll find the directly startable skeleton application in the branch “01-skeleton”. The skeleton Application is a preconfig- ured Spring Boot Service using Spring Modulith for modularity and Axon for Eventsourcing / CQRS.
Let’s quickly explore the structure of the project.
 38 https://github.com/dilgerma/eventsourcing-book 303

UNDERSTANDING EVENTSOURCING
 Fig. 21.1 - Project Structure
(01): The project uses Maven39 to manage dependencies. In the pom.xml file, you’ll find all required project dependencies listed. They will automatically be downloaded during the first build. Some developers will argue that Gradle40 is the better tool, but
39 https://maven.apache.org/ 40 https://gradle.org/
 304

IMPLEMENTING THE FIRST SLICE - “ADD ITEM”
in the end I personally don ́t really care and just need something that works.
In the pom.xml file, dependencies are defined using artifact coordinates, which include the groupId and artifactId. The following example shows how we declare a dependency on spring-boot-starter-web, allowing us to build a REST interface for the system.
  <dependency>
              <groupId>org.springframework.boot</groupId>
              <artifactId>spring-boot-starter-web</artifactId>
  </dependency>
(02): Contains some generic interfaces used throughout the application like Command or Event.
(03): The domain folder contains the central domain logic of the application. This is where we will place commands and aggregates.
(04): The Application Starter can directly run and start the application, including all dependencies like databases. Its is used to locally start and test the application.
The first slice - “Add Cart Item”
 305

UNDERSTANDING EVENTSOURCING
 Fig 21.2 - Add Item Slice
The first slice we will implement is “Add Cart Item”. This slice is based on the “Cart” aggregate, so the first step is to implement the aggregate. You will find the implementation of the first slice in the branch “02/slice-add-item”.
The Aggregate is basically a simple Kotlin-Class with only a handful of Annotations.
 @Aggregate
class CartAggregate {
   @AggregateIdentifier
   lateinit var aggregateId: UUID
}
It does not contain any logic yet. For our current slice we now basically need commands, events and ideally the test cases
306

IMPLEMENTING THE FIRST SLICE - “ADD ITEM”
derived from the “Given / When / Then”-Specifications.
I personally use a lot of code generation in my daily routine and generate code directly from the Event Model. Most of the code you will see in this book will actually be generated. For this example, the code generator generates commands, events, REST API, and the Test-Cases we defined in the Event Model. We will not go into the details of the code generation, as this is absolutely not a required tool.
Looking at the code, we see the new classes “CartAggregate”, “AddItemCommand” and “AddItemResource” marked in red. The “package-info.java” is used to declare modules using Spring Modulith. The “Add Item Resource” is a simple HTTP
adapter to submit a command to the system.
307

UNDERSTANDING EVENTSOURCING
 Fig. 21.3 - Code Structure
What you see here directly reflects our discussions in chapter 10 about Vertical Slice Architectures. We just implemented our very first slice in the package “additem”. Packages here basically form modules and define an isolated functional block in the system.
308

IMPLEMENTING THE FIRST SLICE - “ADD ITEM”
Events
Let ́s quickly review the Events necessary for the slice which are CartCreated- and the ItemAdded-event.
Events are simple POJOs (Plain Old Java Objects), which basically means plain classes without any infrastructural concerns. In our case they are Kotlin Data-Classes ( we could name them POKOs ). They define the shape and structure of our data. Kotlin Data- classes are very handy as they automatically generate getters / setters and further helper-functions we would otherwise have to write ourselves.
In Java we would most probably define a record-type which behaves similar to Kotlin Data-classes.
We will see in a few minutes, how these Events will be used in Command Handlers.
 data class CartCreatedEvent(var aggregateId: UUID) :
Event
data class ItemAddedEvent(
    var aggregateId: UUID,
    var description: String,
    var image: String,
    var price: Double,
    var itemId: UUID,
    var productId: UUID
) : Event
309

UNDERSTANDING EVENTSOURCING
Commands
Let ́s have a look at the “AddItem” command
 data class AddItemCommand(
    @TargetAggregateIdentifier override var
    aggregateId: UUID,
    var description: String,
    var image: String,
    var price: Double,
    var totalPrice: Double,
    var itemId: UUID,
    var productId: UUID
): Command
The command again is just a simple Kotlin Data-Class. The command includes all fields defined in the Event Model.
310

IMPLEMENTING THE FIRST SLICE - “ADD ITEM”
 Fig. 21.4 - Add Item Command
The AddItem command implements the Command interface, which is not Axon-specific but simply defines that all commands should provide an aggregateId of type UUID. Axon itself assumes the Aggregate-Id to be a string.
 interface Command {
   var aggregateId: UUID
}
The Aggregate-Id is marked with @TargetAggregateIdentifier, so the Axon framework knows which attribute should be used to find the corresponding aggregate in the system.
Next, let’s look at the generated REST adapter, which shows how easy it is to submit commands to the system. We won’t dive
311

UNDERSTANDING EVENTSOURCING
into the details of the Spring Framework here, but this section illustrates the process:
 @PostMapping("/debug/additem")
fun addItem(
   @RequestParam aggregateId: UUID,
   @RequestParam description: String,
   ...
   ): CompletableFuture {
   // Send command into the system
return commandGateway.send(
    AddItemCommand(
        aggregateId,
        description,
... )
) }
This is a Rest-Adapter that could be generated from a de- scription language or directly from the Event Model. The @RequestParam-Annotation indicates a request parameter.
The interesting part here is the use of the CommandGateway and how it is used to send a command:
As soon as you submit this command to the system, the Axon framework will try to find the corresponding command handler. The term “command handler” was defined in the chapter 6 as the “first component that knows how to handle a command”.
 commandGateway.send(AddItemCommand(...))
312

IMPLEMENTING THE FIRST SLICE - “ADD ITEM”
The Axon framework takes care of locating the appropriate command handler in the background.
We will review the generated test cases in a minute, but running them now will show that they fail with this error:
The Axon framework cannot locate a responsible command handler for the submitted command, simply because we haven’t implemented one yet. The tests are crucial and act as our safetey net. They reflect business rules from the Event Model translated to a executable specification. For most feature I typically start with red test cases. Let’s explore how these tests work.
Testing Aggregates
 org.axon[..].NoHandlerForCommandException: No handler
was subscribed for command [...].AddItemCommand.
313

UNDERSTANDING EVENTSOURCING
 Fig. 21.5. GWT for “Add Item”
Axon provides easy-to-use testing capabilities to test state change slices in combination with the command handling of aggregates. In our tests, we use the AggregateTestFixture:
 private lateinit var fixture:
FixtureConfiguration<CartAggregate>
@BeforeEach
fun setUp() {
    fixture =
    AggregateTestFixture(CartAggregate::class.java)
}
Thankfully we can now directly translate the Given/When/Then steps into executable code.
GIVEN for this test case is an empty list of events, meaning we are testing the scenario where the aggregate does not exist yet:
314

IMPLEMENTING THE FIRST SLICE - “ADD ITEM”
 // GIVEN
val events = mutableListOf()
WHEN is the command or action that should be executed. In this case, it’s the AddItem command filled with random data.
Most Tests are using one of our huge time savers - the RandomData-utility, that can fill any data with random values instantly. This speeds up the writing of test cases in code significantly and reduces the possibilities of accidentally rely on fixed data. You can find the implementation of it in the Github Repository41 .
 // WHEN
val command = AddItemCommand(
    aggregateId = UUID.fromString("41bc31..."),
    description = RandomData.newInstance { },
    image = RandomData.newInstance { },
    price = RandomData.newInstance { },
    totalPrice = RandomData.newInstance { },
    itemId = RandomData.newInstance { },
    productId = RandomData.newInstance { }
)
THEN defines the expected events that should be stored in the system when the command is successfully executed. Since no GIVEN events were defined, we expect the CartCreatedEvent and
41 https://github.com/dilgerma/eventsourcing-book 315
 
UNDERSTANDING EVENTSOURCING
the ItemAddedEvent with the data from the command:
 // THEN
val expectedEvents = mutableListOf()
expectedEvents.add(RandomData.newInstance {
   this.aggregateId = command.aggregateId
})
expectedEvents.add(RandomData.newInstance {
   this.aggregateId = command.aggregateId
   this.description = command.description
   this.image = command.image
   this.price = command.price
   this.itemId = command.itemId
   this.productId = command.productId
})
Finally, we hand all this data to the AggregateFixture provided by Axon, which simulates a real command handling cycle and allows to test the business rules for writing data to the system.
 fixture.given(events)
   .when(command)
   .expectSuccessfulHandlerExecution()
   .expectEvents(*expectedEvents.toTypedArray())
This is quite powerful, as the test cases we defined with the business are directly translated into executable code in a test first manner. This Test-Case could as well have been defined by a business person alone. They guide us toward successful
316

IMPLEMENTING THE FIRST SLICE - “ADD ITEM”
implementation.
I encourage you to review the test cases in the project. For example, in our second test case, we wanted to verify that a user can never have more than 3 items in the cart.
Fig. 21.6 - Max 3 Items per user
We define the GIVEN state by adding exactly 3 items to the cart by applying the corresponding “ItemAdded”-events.
  // GIVEN
val events = mutableListOf()
events.add(RandomData.newInstance<CartCreatedEvent> {
    aggregateId = UUID.fromString("f9602c89-...")
317

UNDERSTANDING EVENTSOURCING
 })
events.add(RandomData.newInstance<ItemAddedEvent> {
    aggregateId = UUID.fromString("f9602c89-...")
})
events.add(RandomData.newInstance<ItemAddedEvent> {
    aggregateId = UUID.fromString("f9602c89-...")
// Omitted for readability
})
events.add(RandomData.newInstance<ItemAddedEvent> {
    aggregateId = UUID.fromString("f9602c89-...")
// Omitted for readability
})
We will not review this test case in its entirety, but it shows the ability to set up any state for the aggregate and ensure that the correct state-changing events occur.
Since this is a test first approach, the tests will naturally fail because there is no code in our aggregate to handle the command and produce the expected events. Let’s change this now.
Implementing the Command Handler
In chapter 20 we learned that there are several places where a command handler can be defined, one of them being directly on the aggregate itself.
Let’s define the command handler on the Cart-Aggregate to handle the AddItem command.
We already know what the command handler should do, as we’ve modeled it in our “Given / When / Then” tests for the slice.
318

IMPLEMENTING THE FIRST SLICE - “ADD ITEM”
A simple command handler implementation directly on the Cart- Aggregate might look like this:
 @CommandHandler
@CreationPolicy(CREATE_IF_MISSING)
fun handle(command: AddItemCommand) {
   AggregateLifecycle.apply(
      ItemAddedEvent(
          aggregateId = command.aggregateId,
          description = command.description,
          image = command.image,
          price = command.price,
          productId = command.productId,
          itemId = command.itemId,
      )
) }
@CommandHandler Annotation: This annotation makes the method discoverable by Axon as a command handler.
@CreationPolicy(AggregateCreationPolicy.CREATE_IF_M ISSING): This allows the command handler to create a new instance of the aggregate if it does not already exist.
We start by applying the ItemAddedEvent directly using the AggregateLifecycle#apply method, which we’ve also discussed in chapter 20.
As soon as the event is applied, it will be stored in the Event Store and available for further processing.
319

UNDERSTANDING EVENTSOURCING
Test First Development
Now that we have implemented the command handler, let’s run the tests again to verify if they pass. However, we encounter an exception again:
You will see this Exception often when you start to work with Axon. Let ́s try to understand why.
Event Sourcing and Aggregate Initialization
Just applying the event is not enough; we need to ensure that the necessary data from the event is also applied to the aggregate.
We need to define the event-sourcing handler that takes the event that was just applied and “hydrates” the aggregate with the data from the event(s). All “event-sourcing handlers” will be called automatically with the corresponding events everytime an aggregate is loaded from the Event Store.
But first, let’s ensure we’re implementing the correct logic. Which event defines the aggregate-id of the aggregate?
The CartCreatedEvent, not the ItemAddedEvent, defines the 320
 [...].IncompatibleAggregateException: Aggregate
identifier must be non-null after applying an event.
Make sure the aggregate identifier is initialized at
the latest when handling the creation event.

IMPLEMENTING THE FIRST SLICE - “ADD ITEM”
identity of the cart.
Handling Cart Creation
How do we know when to apply the “CartCreated”-event?
It’s actually simple: as long as the aggregate-id is not set, the aggregate will apply the event.
 if (aggregateId == null) {
   AggregateLifecycle.apply(
       CartCreatedEvent(
           aggregateId = command.aggregateId
) )
}
It’s perfectly fine to apply more than one event in a command handler, although most often there is a one-to-one relationship.
We need to ensure that this event is also used to set the ID of the aggregate. This does not occur in the command handler. The command handler never changes the state of the aggregate; it only validates commands and applies events. To populate the aggregate’s data attributes, we need the event-sourcing handler.
321

UNDERSTANDING EVENTSOURCING
 @EventSourcingHandler
fun on(event: CartCreatedEvent) {
   this.aggregateId = event.aggregateId
}
This makes the first test pass. However, the second Test-Case “user can add max 3 items” still fails.
Implementing Business Logic
We need to implement some business logic to prevent the customer from adding more than three items to the cart. The test validates this business logic and acts as our safety net.
A simple implementation could be:
 if(cartItems.size >= 3) {
   throw CommandException("Can only add 3 items")
}
But how do we know how many cart items are already in the Cart? We can easily determine this by counting the number of ItemAdded-events applied. This is straightforward because we can define a second Event Sourcing for this.
 val cartItems = mutableListOf()
322

IMPLEMENTING THE FIRST SLICE - “ADD ITEM”
 @EventSourcingHandler
fun on(event: ItemAddedEvent) {
   // add an item-id to the list for every event
   applied
   this.cartItems.add(event.itemId)
}
The number of items in the cart will always be accurate and is directly derived from the events applied to the aggregate. That’s Event Sourcing in action.
Finalizing the Command Handler
Now that we know how many cart items are in the cart, it’s straightforward to implement the actual business logic (which in fact we’ve already done just a few lines before). We just need to add this check to the command handler function. You see the full implementation of the command handler in the code- snippet below.
 @CommandHandler
@CreationPolicy(CREATE_IF_MISSING)
fun handle(command: AddItemCommand) {
    if (cartItems.size >= 3) {
        throw CommandException("Can only add 3 items")
}
    if (aggregateId == null) {
        AggregateLifecycle.apply(
            CartCreatedEvent(
323

UNDERSTANDING EVENTSOURCING
 ) }
) }
    aggregateId = command.aggregateId
)
AggregateLifecycle.apply(
    ItemAddedEvent(
    aggregateId = command.aggregateId,
    description = command.description,
    image = command.image,
    price = command.price,
    productId = command.productId,
    itemId = UUID.randomUUID(),
)
With this logic in place, both tests should pass, confirming that we’ve implemented the business logic correctly.
Don ́t underestimate the power of this approach. As we defined the test cases long before we started with the implementation, they will be our safety net through the whole process. I generally implement systems test first, means I typically generate test cases directly from the Event Model. All the generated test cases will fail (which is typically just a handful, as we go slice by slice). So typically, when I start with the implementation of a slice, I have 2-10 failing test-cases. When all of them are green, I ́m done with the slice implementation.
324

IMPLEMENTING THE FIRST SLICE - “ADD ITEM”
Conclusion
Congratulations! We have just successfully implemented our first slice using Event Modeling and Event Sourcing with the Axon Framework.
We achieved this through a simple test first approach, where we defined all the tests before writing any code. Since all business rules are already documented in the Event Model, it’s natural to use this logic to create executable specifications, ensuring that we don’t overlook anything.
I typically try to reflect the progress also in the Event Model directly by marking all implemented slices in green. The blue arrow in the top-right corner of the slice is an indicator for the slice belonging to a model-context “Cart”. We introduced the concept of a model-context in chapter 18.
Fig. 21.7 - Done Slice assigned to a Model Context
 325

UNDERSTANDING EVENTSOURCING
We covered important building blocks such as aggregates, com- mands, and command handlers. We also explored hands- on event sourcing concepts by implementing event-sourcing handlers to rebuild an aggregate from the already persisted events in our event store. Additionally, we used the built-in test support for aggregates to verify our business logic.
In this chapter we learned, how we get new information into the system. In next chapter, we will implement our first state view slice and get data out of the system to display it in the UI.
326

22
Implementing state view slices using Live-Projections
In the last chapter, we implemented the first slice in the system. Instead of issuing commands and instructing the system to change and execute an operation, in this chapter we will im- plement a state view slice. For instance, whenever we add an item to our cart, we might want to display the list of items in the UI. This was also part of our rough UI sketch during one of our Event Modeling sessions.
327

UNDERSTANDING EVENTSOURCING
 Fig. 22.1 - querying data from the system
But how do we get this information? Do we process all events live for each and every request? Or do we persist the data and access a cached version, for example, from a database table?
The answer is—it depends. Both approaches are possible.
When appropriate, I implement state view slices as “Live Re- ports”. A Live Report is a projection of the events stored in the Event Store, calculated in real-time for each and every request.
Is Live Reporting Too Slow?
The first question is often, “Isn’t that too slow?” This was also discussed in chapter 2 as one of the common misconceptions. Most of the time, the answer is no. You can process a significant number of events within milliseconds, so the simplicity often justifies a small additional CPU usage.
In this chapter, we will implement our projection as a Live Re- 328

IMPLEMENTING STATE VIEW SLICES USING LIVE-PROJECTIONS
port, processing all added cart items on each and every request, without persisting the projection. In the later chapters, we will implement another Read Model using a database projection, which will continuously be updated for every new event.
You will find the implementation of this chapter in the branch ‘03/live-report-read-model.’
Implementing a Live Report
The steps to implement a Read Model are straightforward:
1. Load all events from the Event Store for the given aggregate-id
2. Extractallinformationfromtheeventstobuildtheprojec- tion
3. Delivertheprojecteddatatotheclient
Axon provides a great abstraction for implementing queries in the form of query handlers.
Query handlers allow information to be queried in a type-safe way without needing to know how the query projection is implemented.
We annotate a method with @QueryHandler, and it receives the 329
 @QueryHandler
fun handleQuery(
    query: < Query Type >
): < Query Result >

UNDERSTANDING EVENTSOURCING
query object as a parameter. For example, if we want to query all cart items by the cart session or aggregate ID, the query object could be named “CartItemsReadModelQuery” and would simply require the “aggregate-id” as a query attribute.
The query handler then returns the Read Model filled with data from the event stream.
To issue a query, we use Axon’s Query Gateway.
 class CartItemsReadModelQuery(
    val aggregateId: UUID
)
 val readModel = queryGateway.query(
    CartItemsReadModelQuery(aggregateId),
    CartItemsReadModel::class.java
)
The Query Gateway receives the query object, including the filled query parameters, and the type of the Read Model that should be returned. The Query Gateway uses these two types to find the corresponding query handler in the system. The Query Gateway will only select query handlers that accept queries of type CartItemsReadModelQuery and return objects of type CartItemsReadModel.
This is a nice approach from the client’s perspective, as the client does not need to know how the query is actually implemented in the background. It doesn’t matter if there is a database
330

IMPLEMENTING STATE VIEW SLICES USING LIVE-PROJECTIONS
projection and the query results in an SQL statement, or if the query simply calculates the projection live from the event stream.
I’m typically hesitant about over-abstracting a system. Abstrac- tions can sometimes make a system harder to understand— often harder than necessary. Too much abstraction can also introduce hidden and unnecessary complexities. However, the query abstraction hides the implementation details of a slice, which is crucial and allows us to keep slices independent from one another.
Let’s first define the Read Model and the data we need to project from the event stream. The great thing is, we already defined exactly what is necessary in the Event Model.
Fig. 22.2 - read model definition
 331

UNDERSTANDING EVENTSOURCING
We define a simple class for “CartItems”. Querying the Read Model will deliver a list of Cart Items.
 data class CartItem(
    var aggregateId: UUID,
    var description: String,
    var image: String,
    var price: Double,
    var itemId: UUID,
    var productId: UUID
)
So the “CartItemsReadModel”-Implementation could look like this for example:
 class CartItemsReadModel : ReadModel {
    var totalPrice: Double? = null
    val data: MutableList<CartItem> = mutableListOf()
}
Projecting Data from Events
How do we now project the data from the events into this object? We simply load the events from the Event Store for the given Cart Aggregate-Id and iterate over all events to build up the projection on the fly. We do this everytime a query for this Read Model is issued.
I typically define a function applyEvents on the object, which receives the list of all events necessary for this Read Model.
332

IMPLEMENTING STATE VIEW SLICES USING LIVE-PROJECTIONS
 fun applyEvent(events: List<Event>):
CartItemsReadModel {
    // same read: project event data
return this }
Pay close attention here: this approach works well when dealing with a single event stream and aggregate. It can also work with multiple event streams, but you need to be mindful of the order in different event streams. I typically use database-projected read models when multiple streams are involved, which we’ll discuss in Chapter 25.
Now, let’s implement the method by iterating over the list of events in a simple for-each loop and match on the event type:
 class CartItemsReadModel : ReadModel {
    var aggregateId: UUID? = null
    var totalPrice: Double = 0.0
    val data: MutableList<CartItem> = mutableListOf()
    fun applyEvent(events: List<Event>):
    CartItemsReadModel {
        events.forEach {
            when (it) {
                // handle specific events
                is CartCreatedEvent -> {
                    this.aggregateId = it.aggregateId
                }
                is ItemAddedEvent -> {
                    // Add cart item to list
333

UNDERSTANDING EVENTSOURCING
 } }
return this }
}
    this.data.add(
        // build cart items on the fly
        CartItem(
            itemId = it.itemId,
            aggregateId =
            it.aggregateId,
            ...
) )
    // Sum total price
    this.totalPrice += it.price
}
First of all, the state of the read model is mutable. As we iterate through all the events, we gradually populate the instance variables with data from the events or even perform simple calculations, such as summing the prices of all cart items to calculate the total price.
Whether calculating prices in the read model is a good idea is something to discuss with the team. Personally, I find it acceptable as long as the state can be directly derived and calculated from the events.
First, we load all events and pass them to the read model’s “applyEvents” function. In a moment, we’ll see how to load all the events. The Read Model receives the events and simply
iterates over them using a for-each loop. 334

IMPLEMENTING STATE VIEW SLICES USING LIVE-PROJECTIONS
For each event type, we define the business rules directly in the loop while processing the events. For example, recalculating the total price every time we receive a new item:
The beautiful thing is, we can now define, event by event, how our Read Model uses the data.
What happens in the case of an ItemRemovedEvent, for example? We could already implement the Read Model for it even if though we haven’t implemented the “Remove Item” slice yet.
We simply do the exact inverse of what we do for ItemAddedE- vent:
 this.totalPrice += it.price
 is ItemRemovedEvent -> {
    val item = this.data.find { item -> item.itemId
    == it.itemId }!!
    this.totalPrice -= item.price
    this.data.remove(item)
}
We find the item in the cart, reduce the total price, and remove the item afterward.
The really great thing about this approach is that we don’t have to deal with concurrency or eventual consistency. Any client using this read model gets a dedicated instance that is hydrated from scratch and always reflects the latest state in the system.
335

UNDERSTANDING EVENTSOURCING
This read model instance is used by exactly one thread at any given time and discarded afterward. You can always get a fresh copy by requesting it. Another thread will use a separate instance of the read model, so concurrency issues are avoided.
Since we already validated the command when the data entered the system, we can trust the correct order of these events, as long as we’re working within a single stream. In general, we trust the data stored in the system, so there’s no need to validate it for correctness at this point.
Implementing the Query Handler
Implementing the Query Handler is straightforward:
 @Component
class CartItemsReadModelQueryHandler(
// (1)
val eventStore: EventStore ){
    @org.axonframework.queryhandling.QueryHandler
    fun handleQuery(
        query: CartItemsReadModelQuery
    ): CartItemsReadModel {
        val events = eventStore
             // (2)
            .readEvents(query.aggregateId.toString())
            .asStream()
             // (3)
            .map { it.payload }
            .toList()
336

IMPLEMENTING STATE VIEW SLICES USING LIVE-PROJECTIONS
 } }
return CartItemsReadModel()
    // (4)
    .applyEvent(events)
We work directly with Axon’s Eventstore implementation here, which gets injected by the Spring Framework in (1). The Eventstore allows us to query all events of an aggregate by using the readEvents-function (2) . We extract the payload of each event (3) and simply pass it to a fresh instance of CartItemsReadModel in (4). That’s all we need to do. The Query Handler is already functional at this point.
Testing Queries
Now that we have implemented the read model and the query handler, how do we use them? As we saw earlier, we use the query gateway to issue a query and retrieve the result. The client doesn’t need to know any of the implementation details of the read model.
Unfortunately Axon itself does not provide any special test support for queries as it does for aggregates. I typically test
 queryGateway.query(
    CartItemsReadModelQuery(),
    CartItemsReadModel::class.java
)
337

UNDERSTANDING EVENTSOURCING
queries by starting the application along with its dependencies using Testcontainers. Testcontainers ensures that all defined dependencies of the application are bootstrapped with the test case and shut down when the test case finishes.
Now we simply have to fire the corresponding command into the system and assert on the fields in the read model. Using commands in tests is not ideal, as we would rather just set the system to a certain state by applying Events to it in the correct order. In this book, we will stick to the command approach, as this works without any custom implementation.
Here is an example of how to test projections using 3 simple steps:
 @Test
fun CartItemsReadModelTest() {
    val aggregateId = UUID.randomUUID()
    val addItemCommand =
        RandomData.newInstance<AddItemCommand> {
           this.aggregateId = aggregateId
        }
    // Step 1: Fire command
    val addItemCommandResult = commandGateway
        .sendAndWait(addItemCommand)
    awaitUntilAsserted {
        // Step 2: Query read model
        val readModel = queryGateway.query(
338

IMPLEMENTING STATE VIEW SLICES USING LIVE-PROJECTIONS
 } }
    CartItemsReadModelQuery(),
    CartItemsReadModel::class.java
)
// Step 3: Assert on Read Model data
assertThat(
    readModel.get().data
).isNotEmpty
In Step 1, we fire the AddItemCommand into the system.
In Step 2, we query the Read Model using the Query-Gateway.
In Step 3, we can assert on the fields in the Read Model and en- sure, for example, that the total prices are calculated correctly.
This Test is using some helper-functions like “waitUnti- lAsserted” that continuously run the code provided until the assertion is fulfilled (or the test times out).
Conclusion
In this chapter, we implemented a simple read model to query all cart items of a specific cart instance. We used a live report model, calculating the cart items directly from the events in the event stream. This is a straightforward approach, simple to
339

UNDERSTANDING EVENTSOURCING
understand and I’ll use it whenever appropriate.
I typically mark all implemented slices green, as we want the progress in the code to be also reflected in the Event Model.
Fig. 22.3 - another slice marked ‘done’
As mentioned, we will implement another Read Model type in chapter 25 using a database projection. This shows you two of the most popular approaches to implement read models. If you are curious, feel free to quickly skip some chapters and jump forward for a sneak peak into database projections.
In the next chapter, we will implement two more state change slices namely “Remove Item” and “Clear Cart”.
 340

23
Implementing Remove-Item and Clear-Cart
Removing an item is already clearly defined in the Event Model. We carefully planned this feature in advance and now just need to follow the plan. For me, this is what good software engineering is all about.
341

UNDERSTANDING EVENTSOURCING
 Fig. 23.1 - Remove Item Slice
You’ll find the implementation of “Remove Item” on the branch “04/remove-item.”
The “Remove Item” command is straightforward; we only need the “aggregate-id” to identify the cart-session and the “item- id” to identify the item to be removed.
 data class RemoveItemCommand(
    @TargetAggregateIdentifier
    override var aggregateId: UUID,
    var itemId: UUID
) : Command
The entire scenario to verify this functionality is only about 30 lines of code. I’ll provide the whole test here once more, so you
342

IMPLEMENTING REMOVE-ITEM AND CLEAR-CART
get a clear picture of how these tests can be structured.
Fig. 23.2 - Given / When / Then
Using the “Given / When / Then”-approach allows us define the business rules very clearly and in a readable way even for non-technical people. The test can be 100% generated from the Event Model, but writing by hand will also be very quick, if you have some experience doing it. The patterns repeat over and over again.
  @Test
fun RemoveItemAggregateTest() {
    // GIVEN
    val events = mutableListOf()
    val itemId = UUID.randomUUID()
343

UNDERSTANDING EVENTSOURCING
 }
events.add(
    RandomData.newInstance<ItemAddedEvent> {
        aggregateId = UUID.fromString("4bcf-...")
        itemId = itemId
    }
)
// WHEN
val command = RemoveItemCommand(
    aggregateId = UUID.fromString("4bcf-..."),
    itemId = itemId
)
// THEN
val expectedEvents = mutableListOf()
expectedEvents.add(
    ItemRemovedEvent(
        this.aggregateId = command.aggregateId
        this.itemId = command.itemId
    }
)
fixture.given(events)
    .when(command)
.expectSuccessfulHandlerExecution()
.expectEvents(*expectedEvents.toTypedArray())
Let’s examine the implementation of the command handler and the aggregate. Here, we see how the cart aggregate utilizes data from prior steps to validate command execution. Naturally, an item can only be removed if it’s already in the cart. In earlier steps, we populated a list of “cartItem-Ids” in the cart aggregate
344

IMPLEMENTING REMOVE-ITEM AND CLEAR-CART
whenever an “ItemAdded” event occurred.
This setup proves useful now, as it allows us to verify that the item about to be removed indeed exists in the cart.
 @CommandHandler
fun handle(command: RemoveItemCommand) {
    if (!this.cartItems.contains(command.itemId)) {
        throw CommandException(
            "Item ${command.itemId} not in the Cart"
        )
    }
    AggregateLifecycle.apply(
        ItemRemovedEvent(
            command.aggregateId,
            command.itemId
) )
}
At this point, it makes sense to define an additional “Given / When / Then”-scenario for the special case that the system tries to remove an item that is not in the cart (anymore). Maybe it was already removed and this attempt is just a retry? In the end, we should not be able to remove an item twice. Of course we could define that this action might be idempotent and just ignore the command execution. The subject-matter-experts on this topic decided otherwise and clearly stated, that this is an error case.
345

UNDERSTANDING EVENTSOURCING
 Fig. 23.3 - Specifying “Remove Item” using Given / When / Then
I mentioned this before, but the next thing is so crucial to understand, that I will repeat it once more, so please pay close attention for a second. This will almost certainly be an “Aha” moment for you.
You hopefully noticed that we did not remove the cart item from the list in the command handler. The command handler is not responsible for changing the state and the attributes of the aggregate. The only job of the command handler is to enforce invariants, validate business rules and apply the correct events in case all rules apply. The state of the aggregate only changes in the event-sourcing handler later as a result of an event. Please read the previous sentence again and make sure you thorougly understand this. This is where most developers struggle in the beginning.
The sole purpose of the command handler is to make a decision 346

IMPLEMENTING REMOVE-ITEM AND CLEAR-CART
whether a command can be processed by validating the existing state of the system.
The change of state happens only afterwards in the event- sourcing handler based on all previously made decisions (events). The only purpose of the event-sourcing handler is to evolve the state of the aggregate.
 @EventSourcingHandler
fun on(event: ItemRemovedEvent) {
    this.cartItems.remove(event.itemId)
}
An aggregate’s internals only changes because of events that occurred, never because of an action. In the event-sourcing handler, we can be sure that the aggregate is in a valid state and that the “cartItems” list contains the item to be removed because we validated it while processing the command.
It’s crucial to understand this. There is no need to validate the state of the aggregate while applying events in event-sourcing handler. It’s guaranteed that the aggregate is always in a valid state; the only place where validation is crucial is on the write- side in the command handler. This is the only way information enters the system.
347

UNDERSTANDING EVENTSOURCING
Implementing - Clear Cart
To implement the “Clear Cart” slice, we follow the same steps again.
Fig. 23.4 - Clear Cart Slice
First, let ́s define the command and event for this slice.
  data class CartClearedEvent(
    var aggregateId: UUID
) : Event
data class ClearCartCommand(
348

IMPLEMENTING REMOVE-ITEM AND CLEAR-CART
     @TargetAggregateIdentifier
    override var aggregateId: UUID
) : Command
We’ll also implement a simple test case before we proceed with the implementation.
 @Test
fun ClearCartAggregateTest() {
    // GIVEN
    val events = mutableListOf()
    events.add(
        RandomData.newInstance<CartCreatedEvent> {
            aggregateId = AGGREGATE_ID
        }
)
    events.add(
        RandomData.newInstance<ItemAddedEvent> {
            aggregateId = AGGREGATE_ID
        }
)
    // WHEN
    val command = ClearCartCommand(
        aggregateId = AGGREGATE_ID
    )
    // THEN
    val expectedEvents = mutableListOf()
    expectedEvents.add(
349

UNDERSTANDING EVENTSOURCING
 }
    CartClearedEvent(command.aggregateId)
)
fixture
    .given(events)
    .when(command)
    .expectSuccessfulHandlerExecution()
    .expectEvents(*expectedEvents.toTypedArray())
The crucial step again is the implementation of the business logic in the aggregate with a command handler on the write- side and an event-sourcing handler for the read-side.
You should notice that there are no business rules defined for clearing a cart, so it’s perfectly fine to clear an empty cart for now and directly apply the event.
We modify the state of the cart session only in the event- sourcing handler and only based on events already stored in the system. In this case, for every “Cart Cleared”-event, we clear the cart items list in the aggregate.
 // Clear Cart
@CommandHandler
fun handle(command: ClearCartCommand) {
    AggregateLifecycle.apply(
        CartClearedEvent(command.aggregateId)
)
350

IMPLEMENTING REMOVE-ITEM AND CLEAR-CART
 }
@EventSourcingHandler
fun on(event: CartClearedEvent) {
    this.cartItems.clear()
}
Conclusion
Looking at the progress made, we achieved a lot already. Don ́t get too confused by all the lines, i ́m showing them here by intention. This is how you define all the dependencies in the system.
Fig. 23.5 - finished slices
We’ve learned some crucial things about event-sourced applica- 351
 
UNDERSTANDING EVENTSOURCING
tions while implementing these two simple state-change slices. There is a clear distinction between command handling and event processing. A command does not directly change the attributes in an aggregate. The command handler only validates if a command can be executed and persists events, which trigger state changes in the end. The change in the aggregate is a direct result of the persisted events in the system.
This is crucial to understand and is one of the fundamental building blocks you need to master. This is one of the aspects where event sourcing requires a subtle mental shift to really be understood, because we are all taught for years and some of us for decades that a change operation also alters the state of our entities, aggregates or value objects.
In my opinion, this chapter is very important. I suggest you revisit it in a few days and simply re-read it.
From now on, we will not go through every single slice imple- mentation and every single line of code, but only if there is something new to learn. You will see a lot of repetition while building event-sourced systems. This is by design. The patterns we learned repeat again and again. Building event-sourced system is a lot like learning the patterns once, and applying them when appropriate. You will find the source code for every slice in the repository and the corresponding branches.
In the next chapter, we will closely examine how we could integrate with external systems using a middleware like Apache Kafka. Integration is such an important topic and can make or break an architecture. I’m very much looking forward to it as
352

IMPLEMENTING REMOVE-ITEM AND CLEAR-CART
this is another major hurdle you will master.
353

24
Example Integration with Apache Kafka and Translations
Axon has a Kafka-Extension42 that allows to integrate with Apache Kafka. In this chapter and the following examples, we will rely on the Spring-Kafka Integration. You can learn about the Axon Integration in the linked documentation.
In this chapter, we’ll focus on managing inventories. As we discussed during the Event Modeling Sessions with the business team and subject-matter-experts, inventories can change at any point in time. You will see an implementation example of translating external events to internal events using the Translation Pattern we discussed in chapter 16.
42 https://docs.axoniq.io/reference-guide/extensions/kafka 354
 
EXAMPLE INTEGRATION WITH APACHE KAFKA AND TRANSLATIONS
 Fig. 24.1 - Modeling integrations using translations
External events in the Event Model
Whenever we receive data from an external source, we model it as an external event in the Event Model. These external events are explicitly modeled in yellow, indicating that external data is entering the system during this process step. The Event Model doesn’t specify how the data enters the system—this is entirely dependent on the implementation and typically not crucial for the process.
The notation for external events here is not an “official” notation in Event Modeling.
Now that we’re implementing it, we need to clarify how we communicate with external systems. For our project, we’ll
355

UNDERSTANDING EVENTSOURCING
assume that the inventory system communicates via Apache Kafka.
Essentially, Apache Kafka is an event-streaming-platform that facilitates communication between systems. We had a quick Apache Kafka Primer in chapter 7, so if you skipped this chapter, you might want to review it quickly before jumping into the implementation details in this chapter. In many of the projects I’m involved, Apache Kafka allows systems to communicate without being directly coupled, serving as a message broker in between.
There are many books written on Apache Kafka, and it only serves as an implementation example in this book, as it ́s in use in many projects. I want to make it clear though, that any message broker could have been used as an example in this chapter. The relevant point is the information flow from the inventory system to the cart system.
Fig. 24.2 - Message Broker between services
 356

EXAMPLE INTEGRATION WITH APACHE KAFKA AND TRANSLATIONS
Command Handler Implementation
In this section, we’ll implement the Kafka consumer, which will take records from Apache Kafka and translate them to the internal “InventoryChanged”-event. The consumer essentially functions as a translator.
The implementation for this chapter can be found on the branch 06/inventory-translation. On this branch, the entire infras- tructure is already prepared. When you start the application locally, the Kafka cluster will start automatically—no additional configuration is required.
This infrastructure is also used in the integration-tests on this branch. The tests also start the database and Apache Kafka to verify the integration.
First, as always let’s start with commands and events:
 data class InventoryChangedEvent(
    var productId: UUID,
    var inventory: Int
) : Event
data class ChangeInventoryCommand(
    @TargetAggregateIdentifier
    var productId: UUID,
    var inventory: Int
357

UNDERSTANDING EVENTSOURCING
 )
The inventory system tracks inventories by product. So for each product (and thus product-id), the system provides the current inventory. This makes the product-id a natural kind of key for inventories also in our system. Since we are not yet sure, what “Inventory” actually means for our system, we simply start by introducing an new aggregate “Inventory” for now, fully aware this might change when we understand more.
Since the product-id seems to be a natural identifier for the aggregate, we also define it as the identifier on the “Change Inventory” command by using the @TargetAggregateIdentifi er annotation. I often indicate “identifying”-attributes in the model with a “*“.
Fig. 24.3 - Change inventory command
For translations, there is typically very few logic involved, 358
 
EXAMPLE INTEGRATION WITH APACHE KAFKA AND TRANSLATIONS
so there is no need for validation in the command handler. We could have also decided to skip the aggregate-definition altogether in this case and completely rely on events.
 @Aggregate
class InventoryAggregate {
    @AggregateIdentifier
    lateinit var aggregateId: UUID
    @CreationPolicy(CREATE_IF_MISSING)
    @CommandHandler
    fun handle(command: ChangeInventoryCommand) {
        AggregateLifecycle.apply(
            InventoryChangedEvent(
                command.productId,
                command.inventory
            )
) }
    @EventSourcingHandler
    fun on(inventoryChangedEvent:
    InventoryChangedEvent) {
        this.aggregateId =
        inventoryChangedEvent.productId
    }
}
As you can see, there is nothing really new in this implemen- tation. Any Translation of events results in a simple “State- Change” slice basically using a command and corresponding command-handler. We implement the command-handler,
359

UNDERSTANDING EVENTSOURCING
which directly applies the event (no validation required), and the corresponding event-sourcing handler to set the aggregate-id.
But how do we invoke the “Change Inventory” command from the “ExternalInventoryChanged”-event?
Fig. 24.4 - translation slice structure
Spring Kafka Configuration
Axon provides a Kafka integration that can handle the transla- tion between Kafka and Axon and vice versa. However, since we are using the Spring-Framework, I typically rely on the Spring- internal integration.
Axon-Kafka Integration especially works well, when you
 360

EXAMPLE INTEGRATION WITH APACHE KAFKA AND TRANSLATIONS
want to generally translate all events from the eventstore to kafka, using it as a bridge between systems.
In our implementation, we assume records are sent as plain JSON. Of course, they could also be sent using any other pro- tocol from plain text to more advanced protocols like Avro43, Protobuf44 or Apache Thrift45.
We will not go into detail on the Spring / Kafka Integration. I encourage you to take a closer look at the possibilities in the documentation46, as it is a very mature stack to build enterprise ready information-systems.
We need to instruct Spring and Kafka on how to deserialize Kafka records to be able to translate them to internal domain events. For this, we need to provide some simple configurations so Spring knows how to handle incoming records.
Kafka Records always come as Key / Value-Pairs, where the Key is always some kind of identifier (not necessarily unique) and the value the actual payload. Do not confuse a Kafka-Record- Key with something like an Aggregate-Identifier. They can be the same, but at the core, they are different concepts. The key in Kafka is primarily responsible to distribute the workload between different consumers. It is a technical, not necessarily a
43 https://avro.apache.org/
44 https://protobuf.dev/
45 https://thrift.apache.org/
46 https://spring.io/projects/spring-kafka
 361

domain concept.
UNDERSTANDING EVENTSOURCING
 spring:
  kafka:
    consumer:
      // (1)
      key-deserializer:
      org.apache...StringDeserializer
      // (2)
      value-deserializer:
      org.springframework...JsonDeserializer
With this configuration, we configure the Spring-Framework to deserialize keys as strings and to deserialize values as JSON.
Implementing the Translation
Translations have the single responsibility to convert external- to internal data. They always follow the same steps: consume the external record or API call, translate the data into a com- mand, and submit the command to the system.
First, we define the external event as a data structure:
 package
de.eventsourcingbook.cart.changeinventory.internal
data class ExternalInventoryChangedEvent(
  var productId: UUID,
362

EXAMPLE INTEGRATION WITH APACHE KAFKA AND TRANSLATIONS
   var inventory: Int
)
The package location is important. Since the external event resides in the internal package of the “changeinventory” slice, it is generally inaccessible to other slices. We are hiding away the fact, that this is an integration workflow from other modules. This clear encapsulation decouples slices from each other. This was discussed in more detail in chapter 10 already.
Besides the slice providing the translation, no other slice has to be aware of the fact that inventory-information comes via Kafka. If the inventory-system changes the communication-type, we would only have to replace the existing “Inventory Change” slice and replace it with a new implementation. Translations basically shield other slices from external changes, serving as a natural protection layer. In Software Architecture this is often referred to as Anti-Corruption Layer47.
To implement a Kafka-Consumer that consumes records, we im- plement the consumer directly within the “Change-Inventory” slice package. We only need to annotate a method in the consumer-class with the annotation @KafkaListener. Our Kafka listener subscribes to the inventories topic and receives any record published to this topic:
47 https://learn.microsoft.com/en-us/azure/architecture/patterns/anti-corr uption-layer
 363

UNDERSTANDING EVENTSOURCING
 @Component
class InventoryChangedKafkaConsumer(
private val commandGateway: CommandGateway ){
    @KafkaListener(topics = ["inventories"])
    fun handle(
        externalInventoryChangedEvent:
ExternalInventoryChangedEvent ){
        // Handle translation logic
    }
}
We covered the fundamentals of Apache Kafka in chapter 7. If you want to dive even deeper, now would be your chance to quickly jump to the Kafka-Documentation48. Don ́t let this confuse you, if you are not yet familiar with Kafka. The implementation details are absolutely not essential for this chapter. Since no logic is involved in translations, we can simply issue the command directly from the Kafka listener and fill it with the data from the external event:
 commandGateway.send(
    ChangeInventoryCommand(
        externalInventoryChangedEvent.productId,
        externalInventoryChangedEvent.inventory
    )
)
 48 https://kafka.apache.org/documentation/ 364

EXAMPLE INTEGRATION WITH APACHE KAFKA AND TRANSLATIONS
That’s all we need to do for the translation. Since the “ChangeIn- ventory” command will result in an “InventoryChanged”-event, we can now work with the internal event and are no longer concerned with Kafka or the external data in our business logic.
Fig. 24.5 - shielding core from infrastructure
Conclusion
In this chapter, we explored a straightforward way to integrate our system with external systems through a simple translation pattern. This approach effectively isolates our core domain from infrastructure concerns, allowing us to focus solely on handling internal domain events.
This method is powerful because it keeps domain functions fo- cused on core business logic while also bridging the technology gap that commonly arises during external system integrations.
 365

UNDERSTANDING EVENTSOURCING
By treating external information as “external events,” we focus on the data itself rather than the underlying technology.
In the next chapter, we will use newly generated “Inventory Changed” events to build a database projection that displays inventory details in the frontend.
366

25
Implementing a database projection for inventories
You find the implementation of this chapter on the branch “07/inventories-database-projection”
In this chapter, we will build our first database projection for the inventories. Basically continuing the implementation from chapter 24. You might be wondering: why did I choose a database projection instead of using a Live Report as we did in chapter 22?
There is an important difference here. The inventories are not stored in one stream with a single aggregate-id but rather in one stream per product-inventory. This means we have to deal with multiple streams here ( hundreds of products potentially ).
To build a view of all inventories, we would have to construct the Live-Report from more than all these streams. While this is generally possible, it is often easier in such cases to gradually
367

UNDERSTANDING EVENTSOURCING
build a database projection that gets constantly updated.
Another reason for this choice is that the streams per product could become quite large, depending on how frequently the inventory changes. Each time the inventory changes, a new event is added to the inventory stream. In chapter 34, you will learn about Snapshotting, which technically addresses this issue.
In the end, it is certainly a questionable design, whether it makes sense to constantly stream the inventories like it is designed in this architecture. But from the perspective of the cart-system though, we can ́t change how the inventory system works.
The idea behind a database projection is straightforward.
We define dedicated projection tables for each use case. These tables should be highly optimized for their specific use case and designed to allow easy and fast querying. We do explicitly not normalize the schema, but flatten it as much as possible typically. We try to prevent joins or complex index structures wherever possible.
Every time a new event is stored in the Event Store, we update the subscribed projection tables with the new data, ensuring that they always contain the latest information.
One event can, of course, be projected into many tables. Con- versely, one table can be populated by multiple events.
368

IMPLEMENTING A DATABASE PROJECTION FOR INVENTORIES
 Fig. 25.1 - one event, multiple projections
So let’s look at our use case: What would the ideal table look like?
What we want is the ability to query the inventory for a spe- cific product. Using plain SQL, as we’ve done for the past 20 years, this would be a straightforward query that any software developer could write.
That is exactly the goal we want to achieve. Make information retrieval as simple as possible.
 select inventory from product_inventory where
product_id = '4321'
369

UNDERSTANDING EVENTSOURCING
Implementing the database layer
We will be using JPA (Java Persistence API) and Hibernate to model our persistence layer. Hibernate is what is commonly known as “Object-relational-Mapper” and implements the JPA- Specification. JPA allows us to work directly with classes and objects and map their data to tables, rows, and columns. Many developers dislike these frameworks for their additional (acci- dental) complexity. And it is true, Object-relational Mappers can grow incredibly in complexity when the relational schema grows over time. This is unlikely to happen in our system, as the relational tables will stay simple, small and focused. Tailor- made and optimized for each use case.
Let’s define a simple JPA entity with just two columns for now.
 @Entity
class InventoriesReadModelEntity {
    @Column(name = "productId")
    var productId: UUID? = null;
    @Column(name = "inventory")
    var inventory: Int? = null;
}
We will also leverage the simplicity of Spring Data JPA49 by defining a straightforward JPA repository.
49 https://spring.io/projects/spring-data-jpa 370
 
IMPLEMENTING A DATABASE PROJECTION FOR INVENTORIES
 interface InventoriesReadModelRepository
    : JpaRepository<InventoriesReadModelEntity, UUID>
Defining this interface is all it takes; Spring Data JPA will automatically generate typical query methods like findAll and findById for direct use. With this setup, we can already persist the required information. Next, let’s create the scripts to set up the new table.
 CREATE TABLE inventories_read_model_entity (
    inventory INTEGER,
    product_id VARCHAR(36) NOT NULL,
    PRIMARY KEY (product_id)
);
The application uses Flyway50 to recreate the database on every startup. Flyway is a database migration tool, that allows to apply database scripts automatically in automated deployments. We simply place this script into “src/main/resources/db/migrati on” and it will be automatically applied, when this version is deployed.
Implementing the Database Projection
In chapter 20, the introduction chapter to the Axon Framework, we briefly touched on the different types of processors available. In this chapter, we will use a Tracking-Event-Processor to
50 https://www.red-gate.com/products/flyway/community/ 371
 
UNDERSTANDING EVENTSOURCING
handle the database projection.
Tracking Event Processors keep track of their position in the event stream even after a restart. This makes sure that new Events are applied to the database projection only once.
 @Component
class InventoriesReadModelProjector(
var repository: InventoriesReadModelRepository ){
    @EventHandler
    fun on(event: InventoryChangedEvent) {
        val entity = this.repository
            .findById(event.productId)
            .orElse(InventoriesReadModelEntity())
        entity.apply {
            inventory = event.inventory
            productId = event.productId
        }.also {
            this.repository.save(it)
} }
}
In the code snippet above, you can see the projector handling the “InventoryChanged”-event. The implementation is sur- prisingly simple. We load the entity by its product-id or create a new one if it doesn’t already exist. We then apply the data from the event to the entity and persist it.
This technique ensures that the database always reflects the 372

IMPLEMENTING A DATABASE PROJECTION FOR INVENTORIES
latest data from the persisted events. You need to be aware that the database update by default is eventually consistent in this case. It does not happen immediately, but asynchronously in the background. By using a Tracking Event Processor, Axon guarantees that the database gets updated, just not necessarily immediately.
Error Handling
Since the projection of the database happens with a small time delay and is eventually consistent, we cannot ignore the possibility of failures.
The default behavior of the Tracking Event Processor is to log the error and continue. In this case, we would lose the inventory information from this event and temporarily work with stale inventory data.
It is important to understand that the failure types we are dealing with here are not business errors, but technical errors. It has been mentioned in other chapters already, we generally trust the events in the event store. It is guaranteed that the system is in a valid state if we apply the events in order. But what happens if the database is not available exactly in the moment, when the Projector tries to update the table? Or if we deal with a short network latency and the operation times out? The default behavior in Axon is to ignore this error and continue.
Whether this is acceptable depends on the business require- 373

UNDERSTANDING EVENTSOURCING
ments. Is it acceptable to temporarily display incorrect inven- tory information? Is it acceptable to allow ordering an item that is out of stock and address the issue later? This is not a technical question and should be discussed with the subject- matter-experts.
Let’s quickly review the options available for handling such errors. You can configure the behavior both globally and per use case51 :
- Ignore the error and just log it: This is the simplest error- handling strategy and also the default if you don’t configure anything. The error is logged, and the process continues as if nothing happened. This might be sufficient depending on the use case.
- Stop the event processing and handle the problem: By configuring a Propagating ErrorHandler, the exception is prop- agated to the Event Processor instance, which then enters into error mode and retries applying the event infinitely with an exponential backoff. This means the first retry happens after 1 second, the next after 3 seconds, and so on, until it reaches a maximum interval of one minute. After that, the processor will infinitely continue to retry applying the event once per minute. You will find these entries in your log statements.
In this case, the event processing comes to a halt until this
51 https://docs.axoniq.io/reference-guide/v/4.0/configuring-infrastructure- components/event-processing/event-processors#exceptions-raised-by- event-handler-methods
 374

IMPLEMENTING A DATABASE PROJECTION FOR INVENTORIES
problem is fixed.
 Releasing claim on token and preparing for retry in
16s.
Releasing claim on token and preparing for retry in
32s
Releasing claim on token and preparing for retry in
60s
Use a Dead Letter Queue
Sometimes it’s not feasible to stop processing altogether. A Dead Letter Queue (DLQ) 52 allows you to skip the failing record by moving it to a temporary event stream - the dead letter queue. Processing all other events will continue, but all events with the same aggregate-id as the failing event will also be put into the Dead Letter Queue to maintain the order per aggregate.
 52 https://docs.axoniq.io/axon-framework-reference/4.10/events/event-pro cessors/
375

UNDERSTANDING EVENTSOURCING
 Fig. 25.2 - maintain ordering in DLQ
In Event Sourcing, we rely heavily on the correct ordering of events, so Axon cannot simply continue as if nothing happened after an error. Just placing the event into the Dead Letter Queue will stop event processing for this aggregate-id altogether until the error is resolved. However, the processor will be unlocked and can continue processing other records, so you have unblocked the overall processing but still need to address the error.
In our example, if an error occurs while updating the inventory for a specific product, other inventories can still be processed correctly, but the erroneous inventory will require manual in- tervention. The erroneous product will not receive any updates until the problem is resolved.
Implementing the Inventories-Read-Model
After discussing with the business experts, it’s clear that we can safely ignore errors for inventories since all orders will be
376

IMPLEMENTING A DATABASE PROJECTION FOR INVENTORIES
reviewed and stopped later in the process if needed. This is great and allows us to keep the process simple. The primary goal is to ensure that customers can submit their orders.
Although logging errors is the default behavior, I typically explicitly configure the desired behavior to make it very ex- plicit. We will define a dedicated processing-group for inven- tory handling and specify the error-handling strategy for this processing-group. Processing-groups allow to apply configu- rations to groups of event-processors.
 @Autowired
fun configurationEventHandling(config:
EventProcessingConfigurer) {
     // (1)
     config.registerListenerInvocationErrorHandler(
     "inventories" ) {
} }
LoggingErrorHandler()
In (1), we register the “LoggingErrorHandler” for the “invento- ries”-processing-group. This Error-Handler will log errors without halting the processing. But how did we define the
“inventories”-processing-group?
We do this directly on the projector-implementation. Let ́s go through the implementation step by step.
 // (1)
@ProcessingGroup("inventories")
377

UNDERSTANDING EVENTSOURCING
 @Component
class InventoriesReadModelProjector(
// (2)
var repository: InventoriesReadModelRepository ){
    // (3)
    @EventHandler
    fun on(event: InventoryChangedEvent) {
} }
// (4)
val entity = this.repository
    .findById(event.productId)
    .orElse(InventoriesReadModelEntity())
// (5)
entity.apply {
    inventory = event.inventory
    productId = event.productId
}.also {
    this.repository.save(it)
}
In (1), we assign a processing group. If not explicitly assigned, the package will be used by default.
The projector gets the JPA repository injected to communicate with the database (2).
We define the event handler method annotated with @Even- tHandler (3) and use a parameter with the event as the payload-
378

IMPLEMENTING A DATABASE PROJECTION FOR INVENTORIES
type. You can, of course, define several event handler methods per class.
We then load or create a new entity instance for the product inventory (4) and update the entity with the data from the event (5). This simple implementation ensures that our database table always contains the latest inventory information.
This whole implementation is of course very Axon-specific. Even if you don ́t work with Axon, don ́t let this confuse you. It is just an example, the general process is the same in any technology.
“Something”, in our case the Projector, reacts to an event and updates one or more database tables using the new information.
Fig. 25.3- updating database projections
 379

UNDERSTANDING EVENTSOURCING
Querying Inventories
To build our Read Model and deliver inventory information to display it in the frontend, we need to provide a way to query the information. While we could use the repository directly, doing so would require every module or system using the Read Model to be aware of its implementation details. This was also discussed in chapter 22.
We intentionally place the implementation into the “internal” package of the module to prevent any other module from being able to even access the repository at runtime.
To keep our slice independent, we provide a Query Handler that abstracts the querying of inventories.
The Query Handler implementation is, once again, quite simple.
 de.eventsourcingbook.cart.inventories.internal
 @Component
class InventoriesReadModelQueryHandler(
    private val repository:
InventoriesReadModelRepository ){
    // (1)
    @QueryHandler
    fun handleQuery(query:
    InventoriesReadModelQuery): InventoriesReadModel?
    {
380

IMPLEMENTING A DATABASE PROJECTION FOR INVENTORIES
 } }
// (2)
if
(!repository.existsById(query.aggregateId)) {
return null }
// (3)
return InventoriesReadModel(
    repository.findById(query.aggregateId).get()
)
The Query-Handler is a simple Spring Component implement- ing a “handleQuery”-function that receives the query object and returns a populated instance of our read model.
The read model class is essentially a wrapper around the database entity. You could of course map the attributes from the entity and treat the read model as a simple data transfer object, if you do not want to expose the entity directly. Since the entity is tailor-made for this use case, it does not make much of a difference in my opinion.
The query object defines the parameters for the query, in this case the product-id, since we will always query inventories by
 data class InventoriesReadModel(val data:
InventoriesReadModelEntity)
381

UNDERSTANDING EVENTSOURCING
product-id.
But how does querying the data work? As we learned in chapter 22, we use the QueryGateway to submit queries and wait for the result. Axon will lookup the query handler, which is able to handle this query and the desired return type.
 data class InventoriesReadModelQuery(val productId:
UUID)
 var productInventory = queryGateway.query(
    InventoriesReadModelQuery(aggregateId),
    InventoriesReadModel::class.java
)
Conclusion
That concludes our discussion on database projections. We now have a reliable method to query inventory information and deliver it to clients. Instead of querying events directly, we use optimized database tables, often without needing joins.
The choice between live reports and database projections de- pends on the specific use case. If the database projection no longer meets our needs in the future, we can update the implementation without impacting clients, as the query in- terface generally remains stable. Database projections are my preferred solution for querying across aggregate boundaries, as they simplify data gathering from multiple aggregates into a structured table format.
382

IMPLEMENTING A DATABASE PROJECTION FOR INVENTORIES
In the next chapter, we’ll delve into automations and how to implement them—a significant challenge developers face when working with event-sourced systems.
383

26
Implementing Automations
You will find the implementation for the “Change Price” slice and the necessary translation on the branch
“08/price-change-translation”
In this chapter, we will dive deeper into the implementation of automations and background processes. Not all process steps involve user interactions; many, if not most, parts of a system run silently in the background.
I will show you how an implementation might look like for a processor. There are many ways to implement automatic processes, starting from a schedule-based implementation that runs every few seconds, polls data from a read model, and triggers some business logic, to a process that gets explicitly triggered by an event, essentially behaving like an event handler.
This is one of the longest chapters in the book. We will gradually implement all parts of the automation, uncover a few problems
384

IMPLEMENTING AUTOMATIONS
with eventual consistency and improve the design step by step to have a working solution in the end.
Fig. 26.1 - Price Change Automation Flow
The whole process consists of 5 steps we need to implement. Starting with an external price change in (1) which starts a translation in (2), we will need to implement a projection in (3) for the automation in (4) to archive an affected item for a specific cart-session by a price change in (5).
Let ́s start with the translation step. In this chapter, we will deal with price changes from an external system. We need to translate the notification from the pricing system to internal events that we can work with.
 385

UNDERSTANDING EVENTSOURCING
Implementing Translations for price changes
Now you see one of the most powerful benefits of using Event Modeling and Event Sourcing together: patterns. When you use these approaches long enough, you’ll realize that everything repeats, all the time.
Implementing the Translation Pattern is basically a straight- forward copy of the previous slice, and using strategic copy & paste will become one of your strongest tools over time. Code generation, as I do it, is essentially just automated copy & paste. The concept remains the same. So don’t underestimate its power—it’s one of your greatest assets.
We have already defined the structure of the external Price Changed event. Prices always change for a specific product and product-ID.
386

IMPLEMENTING AUTOMATIONS
 Fig. 26.2 - external Price Changed Event
The external event will be located directly in the slice package (“de.eventsourcingbook.cart.changeprice.internal”) and cannot be accessed directly by any other module.
 data class ExternalPriceChangedEvent(
    var productId: UUID,
    var price: BigDecimal,
    val oldPrice: BigDecimal
)
Next, we define our internal event as usual. In this case, the internal event looks exactly like the external event. This will often be the case for translations, but not necessarily. It’s perfectly fine to skip some attributes or apply transformations while implementing the translation pattern.
387

UNDERSTANDING EVENTSOURCING
 // internal event
data class PriceChangedEvent(
    var productId: UUID,
    var newPrice: Double,
    var oldPrice: Double,
) : Event
Last but not least, we define the command “Change Price,” which will be used to trigger the price change translation in the system.
 data class ChangePriceCommand(
    @TargetAggregateIdentifier
    var productId: UUID,
    var newPrice: Double,
    var oldPrice: Double,
)
Putting this all together results in this typical setup for transla- tions.
388

IMPLEMENTING AUTOMATIONS
 Fig. 26.3 - Price Change Automation
Price Aggregate
To handle prices, we define a new aggregate called “Pricing”. The aggregate follows the same implementation principles we’ve already discussed in the last few chapters, so we won’t go into detail here and will just show the important parts of the code.
Essentially, the Pricing-aggregate accepts the “Change Price” command, and if successfully executed the “PriceChanged” - event, using the “product-id” as the aggregate-id.
389

UNDERSTANDING EVENTSOURCING
 @Aggregate
class PricingAggregate {
    @AggregateIdentifier
    lateinit var aggregateId: UUID
    @CreationPolicy(CREATE_IF_MISSING)
    @CommandHandler
    fun handle(command: ChangePriceCommand) {
        AggregateLifecycle.apply(
            PriceChangedEvent(
) )
}
command.productId,
command.newPrice,
command.oldPrice
    @EventSourcingHandler
    fun on(event: PriceChangedEvent) {
        this.aggregateId = event.productId
    }
}
Who will trigger the “Change Price” command?
In our implementation, we again assume communication with the pricing system happens via Apache Kafka53. The “ExternalP riceChanged”-event will be delivered as a Kafka record in the topic “price_changes.”
This will also allow us to easily test the scenario with the setup outlined in Fig. 26.4
53 https://kafka.apache.org/
 390

IMPLEMENTING AUTOMATIONS
 Fig. 26.4 - Test Case Setup
From the system perspective - it could as well be a simple API Call, a Webhook or a manual price configuration. From the process-perspective, it ́s basically all the same operation.
Let’s define a simple Kafka listener that gets notified whenever a new Kafka record is available. We are following the same steps outlined in chapter 24.
The Kafka listener will trigger the “Change Price” command for every external price change event. To achieve this, we subscribe to the “price_changes” topic, a shared topic also used by the pricing system to produce notifications of price changes.
 @Component
class ChangePriceKafkaConsumer(
391

UNDERSTANDING EVENTSOURCING
 var commandGateway: CommandGateway ){
    @KafkaListener(topics = ["price_changes"])
    fun handle(event: ExternalPriceChangedEvent) {
        commandGateway.send<ChangePriceCommand>(
            ChangePriceCommand(
} }
) )
event.productId,
event.price.toDouble(),
event.oldPrice.toDouble()
This is a powerful setup, as it allows us to also easily test the entire flow. We can define a simple test case that fires the “ExternalPriceChanged”-event into a running Kafka instance, and all we need to verify is that, after a short delay, the “Price Changed”-event is available for the given “product-id” in the
Event Store.
The next code-snippet is rather long, but it shows the complete setup. The test case uses Testcontainers54 to start a real Kafka instance, enabling us to test the entire workflow in Fig. 26.4 in a near-production like scenario.
54 https://testcontainers.com/
392
 
IMPLEMENTING AUTOMATIONS
 class ChangepriceProcessorTest :
BaseIntegrationTest() {
    // (1)
    @Autowired
    private lateinit var kafkaTemplate:
        KafkaTemplate<String,
        ExternalPriceChangedEvent>
    @Autowired
    private lateinit var streamAssertions:
    StreamAssertions
    @Test
    fun ChangepriceProcessorTest() {
        val aggregateId = UUID.randomUUID()
        val oldPrice = BigDecimal.valueOf(25.99)
        val newPrice = BigDecimal.valueOf(26.99)
        // (2)
        kafkaTemplate.send(
            "price_changes",
            ExternalPriceChangedEvent(
                aggregateId,
                oldPrice,
                newPrice
) ).get()
        awaitUntilAssserted {
            // (3)
            streamAssertions.assertEvent(
                aggregateId.toString()
){
it is PriceChangedEvent && it.productId == aggregateId
}
393

UNDERSTANDING EVENTSOURCING
 } }
}
In (1), we use a simple KafkaTemplate, which allows us to send records to Kafka. This is configured automatically by Spring and can be injected directly.
In (2), we send an “ExternalPriceChangedEvent” to Kafka, ex- actly as it would occur if the pricing service sends a notification.
In (3), we wait for the internal PriceChangeEvent to be stored in the Event Store. This test case will time out after 15 seconds if the event is not stored.
Implementing the “Carts with Products” Read Model
You will find the implementation of the Read Model on the branch “09/carts-with-products”
Now that we’ve translated the external event to an internal event, we can implement the automation that will react to every price change, find the cart sessions containing the product, and actively remove the cart item that contains the product.
First, we need to implement the Read Model for the automation. Let’s ensure we understand which events will affect the data
394

IMPLEMENTING AUTOMATIONS
projection of the read model:
• CartCreated
• ItemAdded
• ItemRemoved • CartCleared
• ItemArchived
Fig. 26.5 - carts with products read model
For the implementation, we need to decide how we will project this data.
Ultimately, our goal is simple: if we have the “product-id”of a product whose price has changed, and we want to be able to query the list of cart ids affected by the price change.
We decide to start with a simple database-projection and use 395
 
UNDERSTANDING EVENTSOURCING
a flat table to store each combination of cart Id and product Id. This table will hold tons of records in production, so we ́ll need to ensure, we have an index on “cart-id” and “product-id” defined.
Fig. 26.6 - simple table layout
Creating the table-schema for the projection is quite simple.
  CREATE TABLE cadeprts_with_products_read_model_entity
(
    cart_id uuid NOT NULL,
    product_id uuid NOT NULL,
    PRIMARY KEY (cart_id, product_id)
);
It is not that important what data stucture we choose to imple- ment this Read Model. We simply choose the one that works best for now. Designing architectures is all about trade-offs and decisions. Using Event Sourcing makes it easy to take a decision as you can almost always revert it later and make another one.
The simple relational table will serve our needs for now. But 396

IMPLEMENTING AUTOMATIONS
in the future, this might change, as requirements constantly change. Maybe we also need to take into account what type of user-session we have and at what time it was created. And maybe using a relational database will not even be the best fit to solve this problem anymore, who knows?
Using Event Sourcing, we aren ́t bound to the decisions we took in the past. If we realize, the Read Model needs to be refactored, we can simply delete it and replace it with a different implementation that is a better fit without impacting other parts of the system.
As we did in the last chapter, we start by defining a simple JPA Entity to persist our data. In this case we define the Entity using a composite primary key consisting of the cart-id and and product-id.
JPA requires us to define an additional Key-Class “CartSession WithProductId” to define the composite keys.
 @NoArg
data class CartSessionWithProductId(
    @Id
    @Column(name = "cart_id") val aggregateId: UUID,
@Id
    @Column(name = "productId") val productId: UUID
)
@IdClass(CartSessionWithProductId::class)
@Entity
397

UNDERSTANDING EVENTSOURCING
 class CartsWithProductsReadModelEntity {
    @Column(name = "cart_id") @Id
    lateinit var aggregateId: UUID
    @Column(name = "productId") @Id
    lateinit var productId: UUID
}
We also define our JPA repository with two additional custom query methods. It is enough to just name these methods correctly. At runtime, Spring Data JPA will parse the method name and build the correct query for us. We do not have to provide the implementation, which is very convenient.
findByProductId - delivers all cart sessions containing. a specific product-id. In SQL this would be something like: “select * from carts_with_products_read_model_entity where productId = <productId>”
deleteAllByCartId - removes a cart session from the projection. This could happen if a user removes the product from the cart session or clears a cart instance for example.
 interface CartsWithProductsReadModelRepository :
    JpaRepository<CartsWithProductsReadModelEntity,
    CartItemId> {
    fun findByProductId(productId: UUID):
        List<CartsWithProductsReadModelEntity>
    fun deleteAllByCartId(cartId: UUID):
        List<CartsWithProductsReadModelEntity>
398

IMPLEMENTING AUTOMATIONS
 }
So, how do we fill this table?
We need a projector that knows how to extract the necessary information from the events and fill the projection.
 @Component
class CartsWithProductsReadModelProjector(
    var repository:
CartsWithProductsReadModelRepository ){
[...] }
We will now analyze the behavior for each event type.
Item Added Event
Whenever an item is added to a cart session, we will store a new entry in the table. if the cart-id / product-id combination is not already in the table. Depending on the scale of our eCommerce system (e.g., Amazon), this could be an inappropriate imple- mentation, as we need to maintain this table holding potentially millions and millions of entries. However, as mentioned earlier, we can review and change the implementation at any point in time. It’s often a good idea to start with the simplest case.
 @EventHandler
fun on(event: ItemAddedEvent) {
399

UNDERSTANDING EVENTSOURCING
 ) }
repository.save(
    CartsWithProductsReadModelEntity().apply {
    this.aggregateId = event.aggregateId
    this.productId = event.productId
}
Item Removed Event
We simply delete the item if we receive an “ItemRemoved”- event. This ensures that the table always contains the current state of all cart items. This also assumes that the cart contains each product-id only once.
 @EventHandler
fun on(event: ItemRemovedEvent) {
    this.repository.deleteById(
        CartItemId(event.aggregateId, event.productId)
) }
Cart Cleared Event
If a cart session is cleared, we simply remove all entries for that cart-id.
400

IMPLEMENTING AUTOMATIONS
 @EventHandler
fun on(event: CartClearedEvent) {
    // Throws exception if not available (adjust
    logic)
    this.repository.deleteAllByAggregateId(
    event.aggregateId )
}
Item Archived Event
And last but not least, we must not forget to react to the “Item Archived” event as well. For the cart, the behavior is the same as for the “Item Removed” event.
 @EventHandler
fun on(event: ItemArchivedEvent) {
    // Throws exception if not available (adjust
    logic)
    this.repository.deleteById(
        CartItemId(event.aggregateId, event.productId)
    )
}
Testing the Read Model
I’m continually astonished by how easily everything becomes testable with this approach. We can write a test for the entire
401

UNDERSTANDING EVENTSOURCING
functionality in just a few lines of code.
Our “Given / Then” definitions can be directly translated into executable specifications.
GIVEN a cart was created, and GIVEN an Item was added THEN we expect the Read Model to contain an entry with
the product of the “ItemAdded”-event.
Fig. 26.7 - Given / Then for Automations
Here is what the corresponding test case looks like. I print the whole test case here, so you can easily follow.
 402

IMPLEMENTING AUTOMATIONS
 @Test
fun Cart with products Read-Model Test() {
    val aggregateId = UUID.randomUUID()
    val productId = UUID.randomUUID()
    val itemId = UUID.randomUUID()
    // (1)
    val addItemCommand =
    RandomData.newInstance<AddItemCommand> {
        this.aggregateId = aggregateId
        this.productId = productId
        this.itemId = itemId
    }
    // send command
    commandGateway.sendAndWait<Any>(addItemCommand)
    awaitUntilAssserted {
        // (2) - query read model
        val readModel = queryGateway.query(
} }
    CartsWithProductsReadModelQuery(productId),
    CartsWithProductsReadModel::class.java
)
// (3) - assert data
assertThat(readModel.get().data)
    .first()
    .matches {
        it.aggregateId == aggregateId &&
        it.productId == productId
    }
In (1), we set the system into the desired state by issuing the “Add Item” command.
403

UNDERSTANDING EVENTSOURCING
In (2), we query the read model.
In (3), we perform assertions on the read model.
The amazing thing is that this test case is completely implementation-independent. If we later decide to switch from the relational table to Redis, the test should continue to work without any adjustments.
Implementing the Automation
You will find the initial implementation on the branch “10/archive-items”.
On this branch there is a failing Test-Case. Before you move on, make sure to check it out and try to understand the reason why it fails. We discuss the issue at the end of the chapter
Now that we’ve implemented the translation and the read model, we can start with the implementation of the automation.
We will begin with a very simple implementation and refine it in the next chapter.
404

IMPLEMENTING AUTOMATIONS
 Fig. 26.8 - Archive Item Automation
According to the Event Model, the automation should be respon- sible for sending the “Archive Item” command for cart sessions affected by a price change. Thus, this automation may send multiple commands from a single event.
It is perfectly fine for a Processor to schedule more than one command.
The processor will react to the internal “PriceChanged”-event, query all cart sessions using the “Carts with Products” Read Model, and trigger the “Archive Item” command for each affected cart session.
 @EventHandler
fun on(event: PriceChangedEvent) {
    // (1)
    queryGateway.query(
405

} }
) }
UNDERSTANDING EVENTSOURCING
     CartsWithProductsReadModelQuery(event.productId),
    CartsWithProductsReadModel::class.java
).thenAccept {
    // (2) For each cart instance containing the
    product
    it.data.forEach { cart ->
        commandGateway.send<ArchiveItemCommand>(
            ArchiveItemCommand(
    aggregateId = cart.aggregateId,
    productId = cart.productId
)
In (1), we query all cart sessions containing the affected product- id. In (2), we send a command to archive the item for each cart session.
We can define a clear test case for the automation using a Given / Then scenario. Test cases for processors are typically modeled using a “GIVEN / THEN” approach, skipping the “WHEN” part similar to Read Model tests. Note that there is no need to define the WHEN block for processors, as their actions happen automatically in the background and are triggered by the events in the GIVEN block.
GIVEN we have a Cart Instance, and GIVEN an item was added to the Cart Instance and GIVEN the price of the product in the Cart changes.
406

IMPLEMENTING AUTOMATIONS
THEN we expect the Cart-Item to be archived.
With this approach, we can once more effectively communicate the desired system behavior using just a few simple sticky notes.
Fig. 26.9 - Given / Then for “Item Archived”
With this, you completely describe the flow of a price change without referring to technology or implementation details. Every businessperson will understand this immediately.
Now, let’s address the actual business logic for archiving a cart item.
 407

UNDERSTANDING EVENTSOURCING
Implementing the business logic
To implement the business logic for archiving an item, we need to implement the functionality in the cart aggregate to handle
“Archive Item” commands.
The main responsibility of the aggregate for this use case is to determine which cart item (if any) needs to be archived when a product price changes.
The command provides the affected “product-id”, while the event contains the affected cart item Id. The provided
“aggregate-id” identifies a cart-session.
Fig. 26.10 - item archived (state change)
In the aggregate, we need to maintain a mapping from products to cart items to enable the lookup from product-id to cart-item-
 408

id.
IMPLEMENTING AUTOMATIONS
 val cartItems = mutableMapOf<CartItemId, ProductId>()
@EventSourcingHandler
fun on(event: ItemAddedEvent) {
[...]
    this.cartItems[event.itemId] = event.productId
}
@EventSourcingHandler
fun on(event: ItemRemovedEvent) {
[...]
    this.cartItems.remove(event.itemId)
}
With this mapping in place, it’s straightforward to implement the logic to handle the archiving process.
 @CommandHandler
fun handle(command: ArchiveItemCommand) {
    // Ignore if items were removed
    // (1)
    cartItems.entries.find { it.value ==
    command.productId }?.let {
        // (2)
        AggregateLifecycle.apply(
            ItemArchivedEvent(
                command.aggregateId,
                it.key
) )
} }
409

UNDERSTANDING EVENTSOURCING
 // (3)
@EventSourcingHandler
fun on(event: ItemArchivedEvent) {
    this.cartItems.remove(event.itemId)
}
In (1), we find the cart item in the list that contains the affected product. We intentionally ignore the case if the item has been removed in the meantime. Since the “Carts with Products” Read Model is eventually consistent, it’s possible that a customer may have removed the product from their cart session before the Read Model could update.
If we find the item, we apply the ItemArchivedEvent in (2), which will trigger the event-sourcing handler in (3)
Finally, and also in (3), we apply the archived event and remove the item from the cart session altogether. Alternatively, we could move the item into an archived item queue to handle it in the next process steps.
Issues with this Implementation
There are a few issues with the implementation in this chapter, that we need to be aware of.
410

IMPLEMENTING AUTOMATIONS
Eventual Consistency
The Read Model we are using here is eventually consistent. This means there is a small time window between when a product is added to the cart and the update of the Read Model(s) subscribing to the “Item Added”-event. If the price change occurs in this time window, the processor might miss a cart session because the Read Model has not yet been updated.
This issue should be discussed with the subject-matter-experts during one of the Event Modeling sessions to determine if this is acceptable. If not, we need to take action to prevent it. We have several options to addressing this problem. We will implement one of them later in the chapter.
It’s critical that this issue is discussed in detail and made transparent to everyone involved, as it could lead to hard-to- find bugs that are nearly impossible to reproduce.
Event Replays
The current implementation relies on the @EventHandler an- notation. We need to be aware that an event triggering an action might be triggered again during an event replay. This is often undesirable, as a replay could result in all cart sessions being updated again, even if no real price change occurred.
This is not a problem in general if the changes are idempotent, meaning we can apply them any number of times without changing the end result. This is not the case for all operations.
411

UNDERSTANDING EVENTSOURCING
One solution to this is the TODO-List-Pattern discussed in chapter 35. The discussion about replays will also be covered in chapter 28 talking about how to handle breaking changes.
To prevent an action from being triggered again by an Event- Handler during a replay, you can annotate the event handler with @DisallowReplay, which will generally ignore it during replays.
Error Handling
Currently, our processor implementation relies on the default error-handling mechanism, which ignores erroneous event handlers. This is not acceptable in this case; we need to handle errors correctly.
The whole logic is currently implemented as a single step, making it challenging to provide proper error handling per cart session. Consider the following scenarios:
- Partial Failures: If the update fails for Session 1 but succeeds for Session 2, should all updates be rolled back, or just the failed ones?
- Rollback vs. Retry: Is a rollback necessary, or can we simply retry and process the sessions that haven’t been adjusted yet? Since the ItemArchivedEvent continuously updates the read model, only sessions that still need adjustment will remain open.
These considerations must be addressed to ensure robust error 412

IMPLEMENTING AUTOMATIONS
handling and system reliability.
Handling Eventual Consistency
The eventual consistent Read Model is a “problem,” as there might be cases where the model is not yet updated and will cause data inconsistencies. Let’s discuss what possibilities we have to handle eventual consistency securely.
Accept the fact and live with it
Sometimes developers think about a problem for hours and days to come up with a good solution, only to find out later that the business side does not really care about that problem. “Not a big deal.”
If a problem is not a problem, we should not try to fix it with technology just because we can. The best solution is the cheapest, quickest thing that could possibly work and that does not become our biggest problem in 12 months.
So, if there is a chance to just live with the problem and document it properly, this can be a feasible solution, at least in the beginning.
Make it immediately consistent
You will find an implementation of this approach in the branch “10/archive-item-with-subscribing-read-model.”
413

UNDERSTANDING EVENTSOURCING
Another option to deal with eventual consistency is to remove it entirely. Just make sure everything happens in the same transaction, and eventual consistency simply goes away.
You need to be aware of the (unwanted) side-effects, and then choose the best approach.
1. The system is no longer independently scalable as we couple the write-side to the read-side.
2. Errors in projections can suddenly abort business trans- actions. If the update of the projection-table results in an error, this error might rollback the business transaction on the write-side as well.
3. Addingmoreprojectionscansignificantlyimpactthesys- tems performance, as more and more projections need to be updated in the same transaction.
We should embrace eventual consistency as a natural part of most processes. But still, the option exists with Axon to remove it. This is configurable per processing group.
 axon:
  eventhandling:
    processors:
      archive-item:
        mode: subscribing
By defining a processing group “archive-item” and making it “subscribing”, all event handlers assigned to this group will only receive events from the time they subscribed to a stream. In this case, Events are generally processed in the same thread
414

IMPLEMENTING AUTOMATIONS
and transaction. We discussed the different threading models in Axon in chapter 20.
Now we just assign the “archive-item” processing group to the database projector, and the whole process of storing the “Archive Item”-event and updating the database projection will
happen immediately within one transaction.
This means, if the “Archive Item”-event is stored, it is guaran- teed that the projection-table is updated basically at the same time.
 @ProcessingGroup("archive-item")
@Component
class CartsWithProductsReadModelProjector(
    var repository:
CartsWithProductsReadModelRepository ){
... }
Transform the Read Model to a (partial) Live Model
There is another approach, and we will discuss it in detail also in chapter 32.
Basically, instead of relying on an eventually consistent database projection, we can read the events directly from the
415

UNDERSTANDING EVENTSOURCING
stream as they happen and build a short-lived in-memory projection of it alongside the database projection.
In-Memory projections are generally fast and ultra-performant. But they only last until the next restart of the system. We can use a nice little trick to combine our eventually-consistent database projection with an in-memory projection, that keeps the last events in memory.
We use the in-memory projection to fill the “eventual- consistency-gap” we have with the database-projection. Basically our in-memory-projection acts as a cache for the latest events.
Fig. 26.11 - partial Live-Model
For this, we need to adjust the CartsWithProductsReadModelQu eryHandler.
 416

IMPLEMENTING AUTOMATIONS
 // (1)
@ProcessingGroup("cart-with-products")
@Component
class CartsWithProductsReadModelQueryHandler(
    private val repository:
CartsWithProductsReadModelRepository ){
    // (2) - holds our in-memory projection
    val limitedDeque = ConcurrentLinkedDeque<..>()
    @QueryHandler
    fun handleQuery(query:
    CartsWithProductsReadModelQuery):
        CartsWithProductsReadModel? {
        val data =
        repository.findByProductId(query.productId)
        val set = data.toMutableSet()
        // (6)
        set.addAll(limitedDeque)
        return
        CartsWithProductsReadModel(set.toList())
}
    // (3)
    @EventHandler
    fun on(event: ItemAddedEvent) {
        // (4)
        while (limitedDeque.size > 20) {
            limitedDeque.pollFirst()
        }
        // (5)
        limitedDeque.push(
            CartsWithProductsReadModelEntity().apply {
                this.productId = event.productId
                this.aggregateId = event.aggregateId
}
417

UNDERSTANDING EVENTSOURCING
 ) }
}
In (1), we assign a specific processing group. Again, the default would have been the package name. I like to use the dedicated @ProcessingGroup annotation here to indicate that there is some special behavior attached.
In (2), we are using a simple ConcurrentLinkedDeque that can easily add items at the end and remove items at the beginning. This Deque generally holds the latest 20 events ( number can vary ) as an in-memory projection.
In (3), we define an event handler to react to ItemAdded events. So whenever an item is added to a cart session, we keep the important information for it in memory for a short period of time.
In (4), we ensure that the data structure does not grow indef- initely, and we keep it at a size of 20 items. Depending on the scale of the system, this number might vary.
In (5), we add the items to the deque structure.
In (6), we dynamically add the data to the query, so we have all data from the eventual consistent model, and in addition, the last 20 items that might not have been applied to the read model.
The final step is now to make this processing group immediately 418

IMPLEMENTING AUTOMATIONS
consistent, as changing an in-memory data structure is very cheap compared to updating a database.
 axon:
  eventhandling:
    processors:
      cart-with-products:
        mode: subscribing
Keeping the latest 20 or 100 or 1000 Events in memory redun- dantly allows us to quickly access the information and still keep it persistent, as the database projection is still populated in the background.
This is just one implementation pattern and I learned this one from Greg Young a few years ago. Chapter 32 discusses this pattern in detail.
Conclusion
This chapter covered a lot of ground, and we’ve made substantial progress in implementing a functional system. We successfully deployed our first automation, which now runs in the back- ground to manage part of the business process automatically. Although the initial implementation faced significant issues, we gradually refined it throughout the chapter.
By employing a small but effective trick, we were able to make the eventually consistent Read Model behave like an immediate
419

UNDERSTANDING EVENTSOURCING
consistent one, without sacrificing performance significantly. In the next chapter, we’ll dive into implementing “Cart Submis- sion” and the publication of data to other systems.
420

27
Submitting the Cart
You will find the implementation for “submit cart” on the Branch “11/submitted-cart”
In this chapter, we will finally implement the cart submission, basically starting all follow-up processes like order fulfillment, checkout, and payment. We will only focus on the process steps from the perspective of the cart system.
This chapter is very technical, because I really wanted to show how an integration with an external system can be implemented.
Cart Submission is modeled in 3 slices: “submit cart” (State Change), “submitted carts” (State View) and “publish cart” (Automation).
421

UNDERSTANDING EVENTSOURCING
 Fig. 27.1 - Submitting the cart
Let’s start with the State Change for “submit cart.”.
Submit Cart
Taking a closer look at the command and the event, we will now see the first time we are using a complex object type within an event. I typically model it as “Custom” type and provide examples about the structure. In this case “orderedProducts” is a list of ordered products containing the “productId” and
“price”.
Some developers do prefer a JSON-like structure to provide the examples. I typically just list the attributes. That ́s just a personal preference.
422

SUBMITTING THE CART
 Fig. 27.2 - Modeling complex objects
All the information needed to construct the “Cart Submitted” event, based on the ordered products, is already present in the system from earlier events like “Item Added.” This enables us to keep the command minimal and focused, requiring only the ID of the submitted cart.
 data class SubmitCartCommand(
    @TargetAggregateIdentifier
    override var aggregateId: UUID
) : Command
The “CartSubmitted”-event will contain all information about the submitted cart. Here again you can ask, if this is truly necessary. Does the fact, that the cart was submitted need to
423

UNDERSTANDING EVENTSOURCING
contain the submitted products? I cannot tell, but for now we say yes, it ́s beneficial to have them available. This might change in the future.
 data class OrderedProduct(val productId: UUID, val
price: Double)
data class CartSubmittedEvent(
    var aggregateId: UUID,
    var orderedProducts: List<OrderedProduct>,
    var totalPrice: Double
) : Event
We will now extend the Cart Aggregate with the logic to submit a cart. For this, besides the products in the cart, we also need to track the product prices. We choose the easiest implementation that could possibly work and just maintain a mapping from product-id to price in the aggregate.
This mapping is automatically updated whenever the Cart session changes.
 var productPrice = mutableMapOf<ProductId, Double>()
 @EventSourcingHandler
fun on(event: ItemAddedEvent) {
// ...
    this.productPrice[event.productId] = event.price
}
Since we now have all the information available, implementing the submission logic is straightforward.
424

SUBMITTING THE CART
 @CommandHandler
fun handle(command: SubmitCartCommand) {
    // (1)
    if (cartItems.isEmpty()) {
        throw CommandException("cannot submit empty
cart") }
    // (2)
    if (submitted) {
        throw CommandException("cannot submit a cart
twice") }
    AggregateLifecycle.apply(
        // (3)
        CartSubmittedEvent(
            command.aggregateId,
            cartItems.map { OrderedProduct(it.value,
            productPrice[it.value]!!) },
            cartItems.map { productPrice[it.value]!!
            }.sumOf { it }
) )
}
In (1), we make sure that an empty cart cannot be submitted. “Empty carts cannot be submitted” is an invariant in the system and should not be possible from the UI, so throwing an exception
in this case is the correct behavior.
In (2), we ensure that a cart session cannot be submitted more than once by tracking its submission status within the Aggregate. Once a “CartSubmitted” event has been processed, we set a
“submitted” flag to true, indicating that this cart instance has 425

already been submitted.
UNDERSTANDING EVENTSOURCING
 var submitted = false
@EventSourcingHandler
fun on(event: CartSubmittedEvent) {
    this.submitted = true
}
In (3), we build the list of ordered products as a simple mapping from the current cart items list.
Make sure to also review the test cases for the implementation, as they exactly verify all business requirements.
Publishing the Cart
You will find the implementation for “publish cart” on the Branch “12/publish-cart”
Since we have implemented the cart submission, we can now think about how to publish the cart to external systems. Until now, we have consumed information from other systems like Inventory and Pricing. This is the first time we are providing information to other systems (without knowing which systems will consume the information in the end).
426

SUBMITTING THE CART
 Fig. 27.3 - Modeling external system integration
For this, let’s assume that we will again be using Apache Kafka to communicate with the external systems. So, “External Cart Published” is, in fact, a Kafka record that needs to be published.
We can imagine that a whole set of systems is just waiting for cart sessions to be submitted to further process the information.
427

UNDERSTANDING EVENTSOURCING
 Fig. 27.4 - Fan-out submitted carts to other systems
To implement the publishing, we basically have to provide a processor that will publish the “External Cart Submitted”-event to Kafka and record the fact that a cart session was published. This often results in the infamous “Dual Write” problem, where information needs to be written to two different data stores (in this case - our Event Store and Kafka). Let ́s see how we can handle this, especially in case of errors.
It’s challenging to make transactional changes across system boundaries without using distributed transactions (which typi- cally introduce even more problems).
What if publishing the Kafka record fails, but storing the “Exter- nal Cart Published” event succeeds? Or worse, what if publishing the “External Cart Published” event fails, but publishing to Kafka succeeds?
428

SUBMITTING THE CART
The “Dual Write” problem can lead to hard-to-solve data inconsistencies in the system if not properly implemented.
Transactional Outbox
One way to work around the issue is to rely on classical relational databases and ACID transactions. What if we make sure that everything happens within the same transaction?
The classical pattern in this case is the Transactional Outbox Pattern, which encapsulates the write operation in an database transaction and thus ensures data consistency on the write side. In our case, instead of directly writing to Kafka, we prevent the problem by writing the event to be published to an intermediate table in our PostgreSQL database within the same transaction.
We then have a second process that monitors the table for new entries to further process them. This can be done using a database trigger, a cron job periodically checking the table for entries, native logical replication in PostgreSQL55 or something like Kafka Connect using a JDBC Source Sink Connector56.
 55 https://www.postgresql.org/docs/current/logical-replication.html 56 https://docs.confluent.io/kafka-connectors/jdbc/current/index.html
429

UNDERSTANDING EVENTSOURCING
 Fig. 27.5 - Transactional outbox
The whole implementation of the “Cart Publisher” can be reduced to a simple database projection for the Transactional Outbox table.
The format of the Transactional Outbox is not defined and can be adjusted on a per use case basis.
Fig. 27.6 - Transactional outbox table layout
The Outbox table format can be straightforward, tailored to the specific use case and the system responding to new entries. Typically, it requires either a monotonically increasing number
 430

SUBMITTING THE CART
or a valid timestamp, allowing the processing system to identify new entries.
A process checking for new entries only needs to remember the last processed ID and can then handle all records with IDs greater than this value, for example.
Transactional Inbox
Similar to the Transactional Outbox, the Transactional Inbox pattern can be used to handle incoming records. Instead of processing records directly from Kafka, they are initially stored in an Inbox table for later handling. This approach enables deduplication of incoming records and decouples the system from Kafka, providing greater flexibility in processing.
Kafka Transactions
Another more specific option to solve the Dual-Write Problem is to rely on Kafka transactions, which allow you to publish the cart (and roll back) as though it were a local transaction. Using Spring and its transaction management allows you to bind the Kafka transaction to our database transaction. If one of them fails, both transactions will be rolled back.
Beware that this is still two dedicated transactions with the possibility of one being committed and the other rolled back. So your application has to take care of this, typically by ensuring that all transactions are idempotent and can be replayed.
431

UNDERSTANDING EVENTSOURCING
 Fig. 27.7 - synchronizing transactions
Kafka transactions can be very convenient for certain scenarios, but the Transactional Outbox Pattern often provides a more robust solution for handling distributed system consistency in the long term. Also it ́s easier to understand and debug for most developers, so in general its my first choice typically.
Nevertheless, we decide to go with the Kafka-transaction- approach for now. We can always revisit this decision later and make adjustments to the implementation.
Implementing the Cart Publication
Let’s first define the external data format. This will be transmit- ted in plain JSON, but it could again also be Avro57, for example.
57 https://avro.apache.org/
 432

SUBMITTING THE CART
 data class OrderedProduct(
    val productId: UUID,
    val price: Double,
)
data class ExternalPublishedCart(
    val aggregateId: UUID,
    val totalPrice: Double,
    val orderedProducts: List<OrderedProduct>,
)
So the external data format basically mimics the internal format; nevertheless, instead of just publishing the internal “Cart Submitted”-event, I typically try to be very explicit about the data format and define a specific “External Published Cart”- event separately.
The processor reacts to “Cart-Submitted” and should instruct the system to publish the submitted cart so that other systems can react to this user action.
 @Component
// (1)
@ProcessingGroup("publish_cart")
class PublishCartAutomationProcessor : Processor {
    var logger = KotlinLogging.logger {}
    @Autowired
    lateinit var commandGateway: CommandGateway
    // (2)
    @EventHandler
433

UNDERSTANDING EVENTSOURCING
 } }
fun on(event: CartSubmittedEvent) {
    commandGateway.sendAndWait<Any>(
        PublishCartCommand(
            event.aggregateId,
            event.orderedProducts,
            event.totalPrice
) )
In (1), we ensure this processor has its own processing group. This allows us to make dedicated configurations per processor. If no processing group is provided, it will fall back to the package name by default.
(2) processes the “CartSubmitted” event and issues a “Publish- Cart” command to the system.
While processing the command, two actions need to be pro- cessed:
• Storingthe“CartPublished”event
• Sendingthe“ExternalCartPublished”eventtoKafka
Both actions should occur together. If one of the actions fails, the other should be rolled back (dual write).
Whenever we need external dependencies injected like the Kafka- Template, I typically implement the logic to interact with other
434

SUBMITTING THE CART
systems/dependencies in an external command handler, rather than directly in the aggregate. That ́s just a personal preference.
 @Component
class PublishCartCommandHandler(
    val kafkaTemplate: KafkaTemplate<String, in
    ExternalPublishedCart>,
    val repository: Repository<CartAggregate>
){
@CommandHandler
    fun handle(command: PublishCartCommand) {
        //
} }
The implementation of the handle method now takes care of instructing the cart aggregate and sending the record to Kafka.
 // (4)
@Transactional
@CommandHandler
fun handle(command: PublishCartCommand) {
    // (1)
    repository.load(command.aggregateId)?.execute {
    aggregate ->
        // (2)
        kafkaTemplate.send(
            "published_carts",
            ExternalPublishedCart(
                command.aggregateId,
                command.totalPrice,
                command.orderedProducts.map { item ->
                    OrderedProduct(item.productId,
435

UNDERSTANDING EVENTSOURCING
 ) )
// (3)
        aggregate.publish()
    }
}
    item.price)
}
We will use the Aggregate Repository to load the Aggregate. This is taken care of automatically when the Command Handler is implemented in the Aggregate directly.
Within the execute function, the Aggregate is correctly initial- ized, and we can work with the AggregateLifecycle as usual.
As soon as the Aggregate is completely initialized, we can issue both actions in (2) and (3).
Now the critical part: since we want both actions to be part of a transaction, we mark the whole function in the Command Handler as @Transactional. This ensures a transaction is started when we enter the function, and the transaction is committed when we exit it.
We need to enable transaction-mode for Kafka by setting the “transaction-id-prefix”-property.
 spring:
  kafka:
    producer:
      transaction-id-prefix: "cart-tx-"
436

SUBMITTING THE CART
Now every call to the KafkaTemplate will automatically be part of a transaction. Since there will already be an open transaction because of our @Transactional annotation, Spring will automatically synchronize both transactions and treat them as one.
In the cart aggregate, we now implement the basic business logic to mark the cart as published, completely free from any side effects and infrastructure.
 fun publish() {
    if (!this.submitted) {
        throw CommandException("cannot publish
        unsubmitted cart")
    }
    // (1)
    if (this.published) {
        throw CommandException("cannot publish cart
twice") }
    apply(CartPublishedEvent(this.aggregateId))
}
// (2)
@EventSourcingHandler
fun on(event: CartPublishedEvent) {
    this.published = true
}
In (1), we make sure that a cart cannot be published twice. The flag is set in the event-sourcing handler in (2).
437

UNDERSTANDING EVENTSOURCING
Error Handling
How should the system behave in case of an error? What is the business behavior when a cart cannot be published?
This is not something a developer should think about in isola- tion; it’s a topic best discussed in one of the Event Modeling sessions with the business experts.
Dead Letter Queue
You will find an implementation using a Dead Letter Queue on the branch “12/publish-cart-dlq”
In case of an error, event processing typically stops (depending on your exception handling strategy). In the worst case, your whole system can come to a halt, and someone will need to unblock it. For this, we need to define a clear process - ideally before this problem occurs.
One way is using a Dead Letter Queue (DLQ), that we already discussed in chapter 25. A DLQ moves erroneous events to a safe place, where we can replay any sequence at a later point in time. If an event for the Aggregate-Id 123 fails, Axon will ensure that all subsequent events are moved to the DLQ and can be replayed later in order.
DLQs in Axon basically provide a structured Retry-Mechanism. You can theoretically indefinitely postpone a replay of an event sequence while the rest of the application continues to work. Replaying a DLQ sequence will invoke the same event processors
438

SUBMITTING THE CART
again, hoping the problem was only temporary and has been resolved in the meantime.
DLQs are limited in size and highly configurable, but they are not a holy grail for system recovery. You are essentially just buying time to fix any problems while parking the problematic records in a safe place that maintains the order of your events.
The business-side decided against DLQs for this case, as they want a possibility to clearly help the customer immediately. So we decided to go with a different solution.
Explicit Modeling
Instead of relying on the error case and Dead-Letter-Queues, we can explicitly model the error process together with the business. Business people do not really understand the concept of exceptions or dead-letter-queues. In business, there are no exceptions, only processes. So, if an error occurs, the error process should kick in, and being explicit about this process is beneficial to everyone involved.
439

UNDERSTANDING EVENTSOURCING
 Fig. 27.8 - explicitly modeling failure
Instead of failing the publication process with an error, we could provide a “Cart Publication Failed” event in case of an error and offer a custom UI for business people to manually resolve the error, and possibly even call the customer if necessary.
For this, we can provide a simple @ExceptionHandler in our Command Handler that gets called whenever an error occurs.
 @ExceptionHandler
fun onException(command: PublishCartCommand) {
    // load the aggregate
    repository.load(command.aggregateId)?.execute {
        // fail the publication
        it.failPublication()
    }
}
440

SUBMITTING THE CART
We need to be aware that this also changes some of the existing test cases where initially an error was assumed. Now, this will be represented as a “PublicationFailed” event. In Fig. 27.9 for example, we see the “Given / When / Then” defined as:
GIVEN a cart session was created and an item was added WHEN the cart gets published with an error THEN we expect the “Cart Publication Failed” event to be stored
Fig. 27.9 - explicitly modeling failure
This error case was already modeled in chapter 18.
 441

UNDERSTANDING EVENTSOURCING
 Fig. 27.10 - simple solution - just show an error
Conclusion
In this chapter, we explored the process of cart submission and publication in depth, learning some techniques to handle errors. We also talked about transaction management in distributed systems using a real-world example. By integrating Kafka trans- actions with our existing database transaction, we developed a robust implementation for the modeled process.
This setup also enables a detailed error process, allowing cus- tomer service to efficiently recover from failed publications.
It’s crucial to understand that the decisions made in this chapter are not set in stone, thanks to the slice-based approach. Each decision affects only one slice of the system, meaning that changes—such as reverting our reliance on Kafka transactions— would only require adjustments within that specific slice, with- out impacting the rest of the system.
442

SUBMITTING THE CART
This approach minimizes risk and allows us to make confident decisions. In the next and final chapter of Part III, we will delve into handling breaking changes and strategies for recovery. I look forward to seeing you there!
443

28
Handling breaking changes
You will find the implementation of this chapter on the branch “13/breaking-changes”
In this chapter, we learn how to handle breaking changes and how the replaying of events is one of the greatest strengths of eventsourced systems. Breaking changes are inevitable, no matter how hard we try to prevent them. At some point, they will be necessary. We should make a plan for them from the beginning.
Breaking Changes
What are breaking changes? In short, every change that is incompatible with an earlier agreed-upon contract with another team or service. Breaking changes can be incoming, meaning another team changed something we cannot handle, or outgo- ing, meaning we had to change something and need to inform the other teams. In any case, every breaking change requires
444

HANDLING BREAKING CHANGES
either communication or an automatic process to handle it. Examples of breaking changes:
• Adding new required fields (without providing defaults) • Removingafieldthataclientdependson
• Renamingfields
Breaking changes slow down development significantly if not handled properly. Let’s assume a team wants to remove a deprecated field. First, they need to find out who is using this field. Then, they need to make an announcement that the field will be deprecated and subsequently removed in a few weeks (or months, or years). Typically, clients need enough time to plan for the necessary adjustments.
The more clients involved, the more communication is required. Breaking changes are typically slow and painful.
We can limit the effects of breaking changes by using contracts and formats that allow for evolving a contract in a planned way. Using Avro, for example, we can define defaults for new or removed fields, which will allow clients to adapt to many changes without impact.
Avro is my personal favorite when designing schemas for communication with external systems.
445

UNDERSTANDING EVENTSOURCING
A new requirement - Fraud Detection
In one of the Event Modeling sessions, a new topic was brought in by the business experts. Whenever a cart item is added to a cart session, a device fingerprint should be calculated to enable quick analysis and detection of fraud attempts. The new field is mandatory. If the device fingerprint cannot be calculated or is not available, a well-known default fingerprint should be used that is recognized by the fraud detection system.
In a real scenario, we would most probably not put something like the deviceFingerPrint into the domain event, but store it in the meta data of each event.
Adding a mandatory field is a breaking change, as we need to handle it properly in the system. This also means we can’t just add the field to the event, as any component processing an older version of the event would simply break because the mandatory field does not exist.
This is where event versioning comes in handy, as it allows us to evolve events in a structured way. In Axon, we define a new version of an event by using the @Revision annotation.
 @Revision("2")
data class ItemAddedEvent(
    var aggregateId: UUID,
    var description: String,
    var image: String,
    var price: Double,
446

HANDLING BREAKING CHANGES
     var itemId: UUID,
    var productId: UUID,
    // since v2
    var deviceFingerPrint: String,
) : Event
I typically keep the old event in the code, within a “versioned” package, and mark it as deprecated. This will come in handy when we write the tests for this breaking change. You ́ll learn more about that later in the chapter.
 package de.eventsourcingbook.cart.events.versioned
@Deprecated("1")
data class ItemAddedEvent(
    var aggregateId: UUID,
    var description: String,
    var image: String,
    var price: Double,
    var itemId: UUID,
    var productId: UUID,
) : Event
If you remember, “Cart-Items”-Read Model is implemented as a “Live Model,” meaning it directly consumes the event stream to build the projection. For this, we simply need to adjust the mapping to include the additional fingerprint.
 fun applyEvent(events: List<Event>):
CartItemsReadModel {
    events.forEach {
        when (it) {
            is ItemAddedEvent -> {
447

UNDERSTANDING EVENTSOURCING
 } }
}
    // Add cart item to list
    this.data.add(
        CartItem(
            // (1)
            fingerPrint =
            it.deviceFingerPrint
        )
)
// ... }
In (1), we access the new field “deviceFingerPrint” from the Event and map it to a fingerprint-field in the Read Model. However, if we receive an old version of the “ItemAdded” event, this will break because the fingerprint field is non-nullable. This could happen, for example, during an event replay, where we reprocess all events for a certain aggregate.
Of course, we could handle all event types like this:
 events.forEach {
        when (it) {
            is ItemAddedEvent -> {}
            is ItemAddedEventV2 -> {}
            is ItemAddedEventV3 -> {}
It ́s certainly not wrong to do it like this and keep the logic very explicit. I personally do not like it as it increases the size of the code-base without adding any functionality, but as often, this is only a personal opinion and is not meant as a recommendation.
448

HANDLING BREAKING CHANGES
To prevent having to deal with multiple versions of events, I typically work with upcasters.
Using Upcasters
An upcaster knows how to transform one version of a data structure to another version. In our case, for example, it knows how to transform Event version “1.0” to Event version “2.0”, therefore the code then only has to deal with “2.0”-type events.
One type of upcaster in Axon is the SingleEventUpcaster, which knows how to handle one specific event type.
 public abstract class SingleEntryUpcaster<T>
implements Upcaster<T> {
    // (1)
    protected abstract boolean canUpcast(
       T intermediateRepresentation
    );
    // (2)
    protected abstract T doUpcast(T
    intermediateRepresentation);
}
For every version change, a dedicated upcaster needs to be imple- mented. Upcasters do not work with event types directly, as the outdated event types might sometimes not even be in the code anymore. Instead, they rely on “intermediate representations,” which are essentially the raw data formats like JSON or XML (or any other format you choose for serialization).
To implement an upcaster, we simply extend one of the base 449

classes like SingleEventUpcaster.
UNDERSTANDING EVENTSOURCING
 // (1)
@Order(0)
// (2)
@Component
class ItemAddedEventUpcasterV1V2 :
SingleEventUpcaster() {
    override fun canUpcast(
        intermediateRepresentation:
        IntermediateEventRepresentation?
    ): Boolean {
        // ..
}
    override fun doUpcast(
        intermediateRepresentation:
        IntermediateEventRepresentation
    ): IntermediateEventRepresentation {
        // ...
} }
Upcasters can be defined as regular Spring beans, as shown in (2), and will automatically be registered for Axon event processing. Axon cannot determine the right order for upcasters if there is more than one available. To guide Axon, upcasters are typically annotated with the @Order annotation.
Upcasters always need to be applied in the correct order. Assum- ing we also had a “version 3” of the event and wanted to process a “version 1”, the upcasters will have to upcast the whole chain in order.
450

HANDLING BREAKING CHANGES
 Fig. 28.1 - Upcaster-Chain
First, we need to implement the canUpcast function. This function basically checks whether upcasting is possible and required for each event.
 override fun canUpcast(
     intermediateRepresentation:
     IntermediateEventRepresentation?
 ): Boolean {
     // (1)
     return
     intermediateRepresentation?.type?.equals(SOURCE_TYPE)
     ?: false
}
In this case, we explicitly define the type of event we want to upcast by specifying the SOURCE_TYPE directly in the upcaster.
  val SOURCE_TYPE: SimpleSerializedType =
    SimpleSerializedType(
)
// (1)
ItemAddedEvent::class.java.getTypeName(),
null
451

UNDERSTANDING EVENTSOURCING
In (1), we reference the old version of the event, that is the reason why I typically keep them in code. Now, the upcaster knows that whenever this event comes, it has to migrate the data using the doUpcast function. The doUpcast function actually works with raw JSON.
 intermediateRepresentation.upcastPayload(
    // (1)
    SimpleSerializedType(TARGET_TYPE.name, "2.0"),
    // (2)
    com.fasterxml.jackson.databind.JsonNode::class.java,
    { item ->
        // (3)
        (item as ObjectNode).put(
            ItemAddedEvent::deviceFingerPrint.name,
            DeviceFingerPrintCalculator
)
item }
)
.DEFAULT_FINGERPRINT
In (1), we define the target type, which is Version 2 of the ItemAdded event.
 val TARGET_TYPE: SimpleSerializedType =
    SimpleSerializedType(
         ItemAddedEvent::class.java.getTypeName(),
"2.0" )
In (2), we define the payload type we expect to receive. In this case, since we are working with JSON, accepting a JsonNode is the right choice. In (3), we add the missing field directly in JSON
452

HANDLING BREAKING CHANGES
to bring the old version to the new version. This is essentially a manual data migration.
When running the tests, everything magically seems to work. But how can we test if the data migration is also effective with an older event version? We can write a small test case for the migration. Although I have never seen this done in this way, it makes perfect sense to me.
We can reuse our Read Model test and add a small variation to it. The trick is to apply an old version to the aggregate and ensure that the Read Model still works. This is pretty simple but requires a bit of understanding of how Axon actually works.
 @Test
fun add item upcasts v1 to v2() {
    // (1)
    val unit = DefaultUnitOfWork.startAndGet(null)
    // (2)
    unit.execute {
        // (3)
        val aggregate = repository.newInstance {
        CartAggregate() }
        aggregate.execute {
            [...]
            // (4)
            AggregateLifecycle.apply(
                RandomData.newInstance<ItemAddedEvent>
                {
                    this.aggregateId = aggregateId
                    this.productId = productId
                    this.itemId = itemId
}
453

UNDERSTANDING EVENTSOURCING
 ) }
}
    awaitUntilAssserted {
        // (5)
        assertThat(
            readModel
                .get()
                .data
                .first()
                .fingerPrint
        ).isEqualTo(DEFAULT_FINGERPRINT)
    }
}
Axon does not allow to apply older event versions out of the box. To be able to test it, we need to trick Axon a little bit. We need to manually start a “Unit of Work” in Axon (1). It’s essentially what happens under the hood when you fire a command or process an event.
Within our “Unit of Work” in (2), we will manually apply events to our newly created CartAggregate (3).
In (4), we apply the old event, which is still available in the “versioned” package. All event handlers only listen to the “2.0”
version of the event, so this old event will not get processed.
The assertion in (5) can only be true if the upcaster works and migrates the old version to the new version in the background, so that all event handlers get triggered again. This is a true test
454

HANDLING BREAKING CHANGES
of the expected behavior.
That’s it. Since the Read Model for this slice is a Live Model, there is nothing more to do. The system can handle both types of events seamlessly.
And the best thing is, since the tests are implementation agnos- tic, they will continue to work without adjustments.
Fig. 28.2 - Testing upcasters using Read Models
Replaying Projections
You will find the implementation of the “Cart Items”- ReadModel using a database projection on the branch
“13/breaking-changes-replay”
Replaying event streams is both a major advantage and a chal- lenge when building event-sourced systems. As requirements
 455

UNDERSTANDING EVENTSOURCING
are always evolving, many IT systems struggle to adapt. Event Sourcing, however, provides the flexibility to modify projections and stored data to accommodate these changes.
For example, if a new field is needed in one of our database projections, we simply update the projector to include this field. Then, by replaying all events, the projector can gradually rebuild the projection by processing each event one at a time.
Axon provides a Replay API that enables replaying events by resetting a projector to the beginning of the event stream or any specific point within the stream stored in the Event Store. Once reset, the projector will start reprocessing events from that point up to the most recent event in the stream.
In practice, I usually implement a straightforward solution that allows for easily triggering this reset when needed.
 @Component
class ResetStream(
// (1)
val config: EventProcessingConfiguration ){
    fun reset(processingGroup: String) {
        // (2)
        this.config.eventProcessor<..>
        (processingGroup).ifPresent {
            // (3)
            it.shutdownAsync().thenRun {
                // (4)
                it.resetTokens()
                it.start()
}
456

HANDLING BREAKING CHANGES
 } }
}
The Replay API can be accessed through the EventProcessingCo nfiguration, which can be directly injected into your application (1). This API operates on a specific processing group, identified to locate the TrackingEventProcessor. To initiate a reset, the processor must first be shut down (3) and then its tokens reset, which forces the processor to start processing from the earliest available event.
When using a database projection, it’s often helpful to clear the database before performing a full reset and replay. This allows the projection to be rebuilt from scratch. You can achieve this by defining a @ResetHandler function on the processor, which is triggered just before the reset begins, ensuring the database is clean and ready for the replay process.
 @ResetHandler
fun onReset(resetContext: ResetContext<*>?) {
    this.repository.deleteAll()
}
If your application runs on several instances, you need to ensure that all instances of the Event Processor are shut down. Otherwise, shutting down one instance will just force the other instance to take over the segments, and the reset will not work.
After the reset, all @EventHandler functions will be called again, 457

UNDERSTANDING EVENTSOURCING
and the projection will be repopulated with the available data.
Special attention needs to be given to event handlers with side effects. If an event handler triggers a command, this command will be triggered again during a replay. Therefore, it’s best to mark these event handlers with @DisallowReplay. My best practice is that no event handler with side effects should update any projection data and vice versa. Annotating modifying event handlers with @DisallowReplay is absolutely fine.
Fig. 28.3 - Side Effects
Conclusion
With this chapter, we conclude Part III of the book, wrapping up the implementation phase. It’s been quite a journey, and I hope you’ve followed along to gain a solid understanding of how to design and implement a typical event-sourced application,
 458

HANDLING BREAKING CHANGES
similar to what you might encounter in real-world projects.
After a brief introduction to the Axon Framework and project setup, we began by implementing simple state changes and state view slices within a Vertical Slice Architecture. We explored various types of Read Models, from live models (reading directly from the event stream) to database projections. We even implemented a hybrid approach, using a clever workaround to manage eventual consistency—all backed by comprehensive tests.
You also learned how to implement automations and properly test them using “Given/When/Then” scenarios.
Speaking of tests, we emphasized how to thoroughly test the different building blocks of the system. Every slice in the system is accompanied by Given/When/Then test scenarios, which are also implemented in code. We tested Upcasters, Projections, and Processors, ensuring our system’s behavior met the expected outcomes.
Throughout the entire implementation process, we never had to start the system itself. Instead, we fully relied on the tests to guide our development, giving us confidence in our work.
We learned how to handle breaking changes in events using Upcasters. Additionally, after making some significant changes, triggering a replay for specific processing groups can efficiently update Read Model projections on the fly.
We took a deep dive into system integration, focusing on Kafka 459

UNDERSTANDING EVENTSOURCING
and addressing the dual-write problem using the Transactional Outbox pattern.
This is the chapter I wish I had when I first started with Event Sourcing—it would have saved me years of trial and error!
Now that we’ve made it through Part III, I’m excited about Part IV, where we’ll explore typical patterns in event-sourced applications and learn how to implement them.
Take a deep breath, relax, and get ready for the final part of the book.
460

IV Implementation Patterns

29
What this part is about
This is the last but certainly not the least important part of the book. When you start to work with Event Sourcing, sooner than later you will recognize something. Using Event Sourcing is full of patterns. They are everywhere. You learn a pattern, and how to solve a specific problem and then you can reapply this pattern everywhere.
We already discussed many of the important pattern in Part III while implementing the System. Part IV is a bit more structured and provides you a small pattern catalog with the most important patterns, I use all the time in my projects.
We will leave our Cart domain for the patterns and use something very simple that does not require context to understand it. We will be using a simple “TODO App”. The basic requirements are listed in Fig. 29.1
463

UNDERSTANDING EVENTSOURCING
 Fig. 29.1
All Pattern in the Catalog follows a simple schema:
Name of the Pattern
Use Case Description
How to model it?
Where to apply it?
How to implement it?
My guess is, that this is the chapter you will come back to review a certain pattern most often and that ́s exactly what I hope for.
I will extend the Catalog whenever I find a new pattern, so make sure to review the latest editions of the book from time to time.
464

30
Pattern: Database Projected Read Model
Use Case Description
We already used the database projected Read Model. It basically projects the data from events in the Event Store to a suitable format for querying via SQL. Typically the data is not normal- ized (at least not completely) and provides a tailor made data structure for each use case.
This also means each Read Model projected to a database typically requires a new table.
How to model it?
465

UNDERSTANDING EVENTSOURCING
 Fig. 30.1 - Modeling a complete slice
The “Database Projected Read Model” is a simple Read Model (green sticky) from the Modeling-Perspective. The database- part is just an implementation detail. You can provide additional implementation hints in the same column in the model as shown in Fig. 30.2, so that every developer knows either what to do or how this is implemented. Fig. 30.2 just gives a hint that this Read Model is implemented as database projection.
I ́m hesistant to put in too much implementation details into the model, as implementations change often while processes stay the same. This is definitely something to discuss, and I prefer to put the implementation details into the documentation directly in Code or at least outside of the Event Model.
466

PATTERN: DATABASE PROJECTED READ MODEL
 Fig. 30.2 - Implementation hints
Where to apply it?
Database-Projections allow to improve performance for com- plex Queries, as we basically precalculate the data necessary as much as possible.
Database-Projections also allow to combine data from different streams into a consistent dataview. Oftentimes it not really feasible to query data from more than a handful of Streams in realtime. The database-projection allows to do this in the background.
We need to be aware that these database projections need to be maintained and kept in sync, if the data model changes. Especially when we add information to events, it might be
467

UNDERSTANDING EVENTSOURCING
necessary to adjust the projection-table and add new columns ( or remove old ones ).
In this case, Event-Replays can be used to repopulate the table. If more than one stream is used as a source for the projection table, you need to be aware that the order of events typically is only guaranteed within one stream, not over several streams.
How to implement it?
Branch: pattern/database-projected-readmodel
We ́ve already seen a handful of implementation examples in Part III of the book.
Step 1: Implement Persistence
Persistence can be implement in any kind of database and any persistence technology. I typically use JPA and PostgreSQL if I have the choice, but of course something like MongoDB or DynamoDB would also work.
 @Entity
class TodosReadModelEntity {
    @Id
    @JdbcTypeCode(Types.VARCHAR)
    @Column(name = "aggregateId")
    var aggregateId: UUID? = null
    @Column(name = "todo")
    var todo: String? = null
468

PATTERN: DATABASE PROJECTED READ MODEL
 }
Step 2: Implement the Projector
 @Component
class TodosReadModelProjector(
var repository: TodosReadModelRepository ){
    @EventHandler
    fun on(event: TodoAddedEvent) {
        val entity =
        this.repository.findById(event.aggregateId)
            .orElse(TodosReadModelEntity())
        entity.apply {
            aggregateId = event.aggregateId
            todo = event.todo
            // Save entity
        }.also { this.repository.save(it) }
    }
}
The Projector is called for every event and defines methods annotated with @Eventhandler. It persists data from the events into the persistence store.
Step 3: Provide a Query
 data class TodosReadModelQuery(
    val aggregateId: UUID,
469

UNDERSTANDING EVENTSOURCING
 )
Queries are just typesafe ways to query data. The Query-Class defines the Parameters used for querying.
Step 4: Provide a Query Handler
 @Component
class TodosReadModelQueryHandler(
private val repository: TodosReadModelRepository ){
    @QueryHandler
    fun handleQuery(query: TodosReadModelQuery):
    TodosReadModel? {
        if
        (!repository.existsById(query.aggregateId)) {
return null }
        return TodosReadModel(
            repository.findById(query.aggregateId).get()
) }
}
Step 5: Submit Query using the QueryGateway
 var readModel = queryGateway.query(
                 TodosReadModelQuery(aggregateId),
470

PATTERN: DATABASE PROJECTED READ MODEL
 TodosReadModel::class.java)
Queries can be tested as well. In the code-snippet below you will find a complete test-case example.
 class ReadmodelTest : BaseIntegrationTest() {
    @Autowired
    private lateinit var commandGateway:
    CommandGateway
    @Autowired
    private lateinit var queryGateway: QueryGateway
    @Test
    fun `Read Model Test`() {
} }
val aggregateId = UUID.randomUUID()
var addTodoCommand =
    RandomData.newInstance<AddTodoCommand> {
        this.aggregateId = aggregateId
    }
var addTodoCommandResult =
   commandGateway.sendAndWait<Any>(
        addTodoCommand
   )
awaitUntilAssserted {
    // issue query
    var readModel = queryGateway.query(
         TodosReadModelQuery(aggregateId),
         TodosReadModel::class.java)
    assertThat(readModel.get()).isNotNull
}
471

31
Pattern: Live Model
Use Case Description
The Live Model is a Read Model that does not rely on a different persistence technology but directly uses the Event Store and Streams to build projections. Using Live Models, there is no eventual consistency. Data is always consistent with the latest state in the Event Store.
How to model it?
It is modeled exactly as the “Database Projected Read Model” in chapter 30
472

Where to apply it?
PATTERN: LIVE MODEL
 Fig. 31.1
The Live Model works well in case you only rely on one event stream with limited size. What “limited” means in this case is highly dependent on your system. You can process hundreds and thousand of events within milliseconds. Since events have an index on the aggregate-id, retrieval of all events for a single aggregate or stream is typically very efficient.
In general, this approach requires a bit more processing at runtime, as you constantly query events to calculate the latest projection. On the other side you do not have to deal with eventual consistency at all.
I typically don ́t use it when we work with different event streams, as the order of events is not guaranteed across streams.
473

UNDERSTANDING EVENTSOURCING
How to implement it?
Branch: pattern/live-model
Step 1: Implement the Read Model Projection
 class TodosReadModel : ReadModel {
    var aggregateId: UUID? = null
    var todos: List<String> = mutableListOf()
    fun applyEvents(events: List<Event>):
    TodosReadModel {
        events.forEach { event ->
            when (event) {
} }
return this }
}
is TodoAddedEvent -> {
    aggregateId = event.aggregateId
    todos.add(event.todo)
}
is TodoRemovedEvent -> {
    todos.remove(event.todo)
}
I typically use a method “applyEvents” that get a List of Events from the Event Store.
Building the projection is a simple left-fold over the events to build the projection and set the attributes in the Read Model appropriately using a pattern-match on the event-type. The
474

PATTERN: LIVE MODEL
fields in the Read Model can be mutable, as the Read Model instance will only be used by one thread and then discarded. The Read Model is build from scratch every time it is requested.
Step 2: Implement the Query Handler
The Query-Handler can query events directly from the Eventstream. The benefit, we do not have to deal with concurrency here, as we return a new ReadModel-Instance for every request.
 @Component
class TodosReadModelQueryHandler(
    val eventStore: EventStore
) : QueryHandler<TodosReadModelQuery, TodosReadModel>
{
    @QueryHandler
    override fun handleQuery(
        query: TodosReadModelQuery
    ): TodosReadModel {
        val events = eventStore.readEvents(
            query.aggregateId.toString()
        ).asStream()
            .filter { it.payload is Event }
            .map { it.payload as Event }
            .toList()
        return TodosReadModel().applyEvents(events)
    }
}
475

32
Pattern: The (partially) synchronous Projection
Use Case Description
Most of the time, projections of our data are handled asyn- chronously in the background for a good reason. It allows us to decouple the read from the write side and also to scale different parts of the system independently. Sometimes this is undesired. Sometimes you need a projection to be immediately consistent, maybe not in total, but at least in parts.
How to model it?
It is modeled exactly as the “Database Projected Read Model” in chapter 30. You can give an additional hint on the arrow that this is synchronous.
476

Where to apply it?
PATTERN: THE (PARTIALLY) SYNCHRONOUS PROJECTION
 Fig. 32.1
We had one use case where we had this eventually consistent Read Model that was used by a processor. Because of the eventually consistent nature, in certain situations, it could happen that entries get lost if the processor was running before the model got updated.
Oftentimes, this is because of bad modeling, but sometimes it’s just a problem that needs to be solved.
The partially synchronous projection is applicable if the Read 477

UNDERSTANDING EVENTSOURCING
Model needs to be immediately consistent and we can ́t or don ́t want to make the whole Read Model immedately consistent. Making the whole Read Model immediately consistent would mean you would need to run all the event processing and database projections in the same thread as the user who is waiting for the response.
But what is feasible oftentimes is to keep a small portion of the event stream in memory and use this to make the model look like it was immediately consistent. Technically it ́s small hack, but a very efficient one.
How to implement it?
Implement a second event handler directly in the query handler to fill an in-memory query cache using a LinkedBlockingQueue. In this example it contains a maximum of 20 elements to work around eventual consistency. The number is is highly dependent on the Use Case and could as well be 1000.
 val queue =
LinkedBlockingQueue<TodosReadModelEntity>(20)
@EventHandler
fun handle(event: TodoAddedEvent) {
    queue.add(
        TodosReadModelEntity().apply {
            this.todo = event.todo
            this.aggregateId = event.aggregateId
        },
)
478

PATTERN: THE (PARTIALLY) SYNCHRONOUS PROJECTION
 }
Make only the in-memory handler a subscribing Eventhandler (runs in the same Thread and is immediately consistent).
 axon:
  eventhandling:
    processors:
      partial-consistent:
        mode: subscribing
Now you can run a test case with no Wait-Time in between the Command-Handling and the Read Model. Fire the command and immediately query the data. The test case is green.
 @Test
fun `Read Model Test`() {
    val aggregateId = UUID.randomUUID()
    val addTodoCommand =
    RandomData.newInstance<AddTodoCommand> {
        this.aggregateId = aggregateId
    }
    val addTodoCommandResult =
        commandGateway.sendAndWait<Any>(addTodoCommand)
    // No wait in between, we query the model
    immediately.
    // Without immediate consistency, it will fail.
    val readModel = queryGateway.query(
        TodosReadModelQuery(aggregateId),
        TodosReadModel::class.java
    )
479

UNDERSTANDING EVENTSOURCING
     assertThat(readModel.get()).isNotNull
}
You need to make sure if there is a replay, that your ResetHandlers reset the database and the in-memory data structure at the same time. Even though this workaround feels like a little hack; sometimes it is okay to make these tiny little workarounds to make life easier. I am pretty sure I learned this trick from Greg Young, but I cannot find the reference to it anymore.
480

33
Pattern: The Logic Read Model
Use Case Description
Often the question comes up if it is okay, to have logic in Read Models and as always there is a wide range of opinions. This pattern description is my take and you need to decide for your team, if you apply it or not.
Branch: pattern/logical-read-model
Sometimes there are calculations and aggregations and you can ́t seem to find a good place for them. Let ́s say for example, we want to display the number of open TODOs in our TODO Application. Where would you put this calculation?
You can make the calculation whenever a TODO is added into the system and store it in the Event.
481

UNDERSTANDING EVENTSOURCING
You could implement an Automation, that reacts to “Todo Added” events and calculates the total in the background. Tech- nically fine, but feels a bit overengineered. Let ́s first find out how to model it.
How to model it?
It modeled exactly as the “Database Projected Read Model” with additional properties that are calculated.
Fig. 33.1
 482

PATTERN: THE LOGIC READ MODEL
Where to apply it?
For me it is perfectly fine to put Logic directly into the Read Model. There is one important rule I follow. It can only access state already available in the system. Calculations in Read Models must not introduce any side-effects in the System.
You can ́t call external services or access external datastores that are not direct derivatives from the events stored in the system. Simply said, I only use logic if it ́s based on events.
How to implement it?
Extend your projection using calculated attributes like “todoCount”. These attributes are sometimes persisted,
sometimes calculated at runtime.
 data class TodosReadModel(
    val data: List<TodoData>,
    val aggregateId: UUID,
    val todoCount: Int,
)
Implement the required logic directly in the Query-Handler if calculated at runtime.
 @QueryHandler
fun handleQuery(
    query: TodosReadModelQuery
): TodosReadModel? {
    val todos =
    repository.findByAggregateId(query.aggregateId)
483

UNDERSTANDING EVENTSOURCING
     return TodosReadModel(
        todos.map { TodoData(it.todo) },
        query.aggregateId,
        // do the necessary calculations
        calculateNumberOfTodos(todos)
) }
Make sure to not introduce unintended side-effects.
484

34
Pattern: Snapshots
Branch: pattern/snapshot
Use Case Description
Snapshots are a technical tool used to reduce the number of events that need to be processed in a stream. Creating a snapshot essentially involves building a data projection from the current state of a stream and then persisting it for later use. Technically it very much acts like a cache.
By storing the state of a stream as a snapshot, we eliminate the need to process any events that occurred before the snapshot was created. Instead, we can simply load the latest snapshot and process only the events that have occurred since then.
485

UNDERSTANDING EVENTSOURCING
How to model it?
Snapshots are a pure technical tool and are neither modeled nor mentioned in an Event Model typically.
Where to apply it?
Snapshots can be used when you want to limit the number of events you need to process in a stream. In this sense, snapshots serve a similar purpose to caching: we cache the projected state of the system instead of calculating it at runtime.
“You have a problem, and you cache it. Now you have two problems.” (Every Senior Software Developer ever)
Often, it’s better to limit the length of a stream naturally by understanding the business processes.
For example, although the number of transactions for a bank account over the years can be significant, most banks do not rely on something like snapshots to my best knowledge Why? Banks naturally limit the number of events through a process called “closing the books.” By closing the books, they conclude the account transactions after a day, month, or year and store the current balance as a starting point for a new stream.
Instead of collecting all events for an account over a 30-year pe- riod, we could use 30 dedicated streams, each with significantly fewer events to process.
The same principle applies to the stock market. There are 486

PATTERN: SNAPSHOTS
millions of transactions per day, but after each trading day, all accounts are settled. This means we “close the book” for the account and start the new trading day with the current balance in a new stream.
However, sometimes there is no business concept that can be used to close the stream, and we just need a technical way to improve performance. That’s where snapshots come in. But for me, snapshots are the exception, not the rule. Many developers try to understand and use snapshots from day one, assuming they are an essential part of Event Sourcing. I disagree. Snapshots are a technical workaround that can be used, but they are definitely one of the least important aspects of building an event-sourced system.
As always, this is my personal opinion.
How to implement it?
Axon provides first-class support for Snapshots.
For each Aggregate, we can decide what triggers a Snapshot by
providing a SnapshotTriggerDefinition.
 @Bean
fun todoAggregateSnapshotTriggerDefinition(
    snapshotter: Snapshotter?
): SnapshotTriggerDefinition =
    EventCountSnapshotTriggerDefinition(
        snapshotter,
487

UNDERSTANDING EVENTSOURCING
     // (1) - take a snapshot every 5 events
5 )
Axon supports two types of TriggerDefinitions out of the Box:
• EventCountSnapshotTriggerDefinition-triggeredwhen- ever the number of events since the last Snapshots exceeds the configured threshold
• AggregateLoadTimeSnapshotTriggerDefinition - trig- gered whenever the load time of an Aggregate exceeds the configured time limit
In our example, we want to take a Snapshot every 5 events. In a real world scenario, this number would be configurable and most probably much higher.
To activate the trigger definition, we just need to reference it from the Aggregate directly.
 @Aggregate(snapshotTriggerDefinition =
"todoAggregateSnapshotTriggerDefinition")
class TodoListAggregate {
//... }
On the branch, we provided a simple Scheduled-Job implemen- tation that fires a Command for the aggregate every second, basically simulating a busy aggregate receiving lots of interac- tion.
488

PATTERN: SNAPSHOTS
 @Component
class CommandScheduler {
    @Autowired
    private lateinit var commandGateway:
    CommandGateway
    @Scheduled(fixedDelay = 1000)
    fun addTodos() {
        commandGateway.sendAndWait<Any>(
            AddTodoCommand(AGGREGATE_Id, "todo")
) }
    val AGGREGATE_Id = UUID.randomUUID()
}
If you start the application and you connect to the locally running PostgreSQL database,you will see how the Snapshots gets updated constantly after every 5 events. The Axon- Documentation58 goes into greater detail about Snapshots and how they work.
 58 https://docs.axoniq.io/reference-guide/axon-framework/tuning/event-sn apshots
489

35
Pattern: “Processor-TODO-List” - Pattern
Process: a series of actions or operations conducing to an end59
Implementing Automations is the most complex part in an event-sourced system. Most implementation questions in the community Discord60 circle around the implementation of automations and how to coordinate them.
A very simple and accessible way to think about tasks and automations is by looking at them as Processor-To-Do-Lists that can be simply checked off.
59 https://www.merriam-webster.com/dictionary/process 60 https://eventmodeling.org/resources/#discord
 490

Use Case Description
PATTERN: “PROCESSOR-TODO-LIST” - PATTERN
 Fig. 35.1
Any process can be implemented as a series of process steps that are conducted in order.
Let’s say our simple Todo-System has the requirement to expire todo-list items that are older than 24 hours automatically if they weren’t resolved.
This will certainly be something we will implement as an automation running in the background using a processor.
Just as users create todo-lists for their tasks that need to be done during the day, we can imagine our processor also maintaining its own technical todo-list of tasks that need to be done next. It’s the same principle.
For each process step, you can ask yourself:
How does the Processor get a new task to execute?
491

UNDERSTANDING EVENTSOURCING
What needs to happen for this task to get checked off the Processor-Todo-List?
In our case, the processor gets a new task on the Processor- Todo-List when an item was added more than 24 hours ago and has not been resolved since. The task of the processor is then to expire this item and make sure the system knows about it.
How does a Task get checked off the Processor-Todo-List?
As soon as the item either gets resolved (“Todo Resolved” event for this item exists) or the item was already expired previously (“Todo Expired” Event for this item exists), the Processor can check off this item and move on to the next task.
Processor-Todo-Lists are calculated based on the state of the System and typically provided by a Read Model.
How to model it?
To be able to expire items after 24 hours, we either need the creation-date or the expiry-date in the system. We calculate the expiry-date at the time when we create the todo and store it directly in the Event.
492

PATTERN: “PROCESSOR-TODO-LIST” - PATTERN
 Fig. 35.2
Our Read Model is responsible for calculating the Processor- Todo-List. We name the Read Model “Todos to expire.” It delivers a list of todo-items that need to be expired by the processor.
493

UNDERSTANDING EVENTSOURCING
 Fig. 35.3
For each item on the Processor-Todo-List, the Processor will issue a “Expire Todo” command to the system and instruct it to expire the item. The “Todo Expired” event feeds back into the Read Model and checks it off the Processor-Todo-List to not expire it twice.
For this “Back-Channel” I typically use a dotted line to indicate that this is not part of the “Flow” but just updating the data of the Read Model.
494

PATTERN: “PROCESSOR-TODO-LIST” - PATTERN
 Fig. 35.4
The whole structure typically looks like in the following picture.
495

Where to apply it?
UNDERSTANDING EVENTSOURCING
 Fig. 35.5
Any automation can use a Processor-Todo-List to coordinate tasks. A processor could periodically fetch data provided by the Read Model and process all open Tasks.
How to implement it?
Before we can implement the processor, we need to build the Read Model for the Processor-Todo-List.
We use a simple database projection, as we did several times in Part III of the book already.
496

PATTERN: “PROCESSOR-TODO-LIST” - PATTERN
 @Component
class TodosReadModelProjector(
var repository: TodosReadModelRepository, ){
    @EventHandler
    fun on(event: TodoAddedEvent) {
        val entity = TodosReadModelEntity().apply {
            aggregateId = event.aggregateId
            todo = event.todo
            expirationDate = event.expirationDate
}
        this.repository.save(entity)
    }
}
The database projection has an additional field, “expired,” which indicates to the Processor whether the item has already been processed.
 @Entity
class TodosReadModelEntity {
//...
    @Column(name = "expiration_date")
    lateinit var expirationDate: LocalDateTime
    @Column(name = "expired")
    var expired: Boolean = false
}
The Processor itself could be implemented as a periodic Job (“Polling Processor”), for example, which continuously checks if there are any items (“Processor-Todos”) that need to be
497

UNDERSTANDING EVENTSOURCING
processed. Expressed in SQL, it would be a simple query like this:
 SELECT *
FROM todo_data
WHERE expiration_date < NOW() and expired = false
Whenever there is an item, the processor instructs the system to expire it by issuing the “ExpireTodo” Command.
 // (1)
@Scheduled(fixedDelay = 1000L)
fun handleExpirations() {
    // (2)
    queryGateway.query(
        TodoItemsToExpireQuery(),
        TodosReadModel::class.java
    ).thenAccept { item ->
        item.data.forEach {
            // (3)
            commandGateway.send<Any>(
                ExpireTodoCommand(it.aggregateId,
                it.todo)
) }
} }
In (1) we use Scheduling-Support in Spring. This allows us to define tasks that periodically run in the background in a dedi- cated thread. This could also be a cron-Job running periodically
498

PATTERN: “PROCESSOR-TODO-LIST” - PATTERN
or a job scheduled by a workflow engine like Temporal61. In (2) we simply fetch all active Processor-Tasks by querying expired items from a database projection. The processor then goes through all items, expiring them in (3). The system issues a “TodoExpired” event to record the fact that an item was expired.
 @CommandHandler
fun handle(command: ExpireTodoCommand) {
    AggregateLifecycle.apply(
        TodoExpiredEvent(command.aggregateId,
        command.todo)
) }
The presence of the “TodoExpiredEvent” indicates to the proces- sor that this task should be removed from the Processor-Todo- List. It has already been handled. For this, we set the “expired” flag in (1) for the database projection, which will automatically remove it from the query results when the processor runs the next time. The Processor-Todo-Item has been successfully checked off.
 @EventHandler
fun on(event: TodoExpiredEvent) {
    val entity = repository.findByAggregateIdAndTodo(
        event.aggregateId,
        event.todo
    )
    entity?.let {
        // (1)
        it.expired = true
 61 https://temporal.io/
 499

UNDERSTANDING EVENTSOURCING
         this.repository.save(it)
    }
}
Eventual Consistency and Todo-Lists
There is a race condition in the implementation. The update of the Read Model after an item is expired is eventually consistent and happens only after a time delay. If the processor starts another check after one second, the item might already be checked off or not, which could lead to items being expired twice.
If this is not feasible, the back channel should be designed to be immediately consistent by running it in the same thread, for example.
500

PATTERN: “PROCESSOR-TODO-LIST” - PATTERN
 Fig. 35.6
How does this relate to the Saga-Pattern?
Any distributed process can be broken down into a list of local processes that are executed in order. You’ll find a detailed discussion about Sagas in Part I of the Book. Personally, I find it much simpler to reason about processes by looking at their atomic building blocks instead of trying to understand the whole process completely.
Consider an order processing system involving three services: Order, Payment, and Inventory.
501

UNDERSTANDING EVENTSOURCING
Step 1: The order service creates an order and publishes an “OrderCreated” event.
Step 2: The payment service listens for the “OrderCreated” event, processes the payment, and publishes a “PaymentPro- cessed” event.
Step 3: The inventory service listens for the “PaymentPro- cessed” event, reserves the inventory, and publishes an “In- ventoryReserved” event.
Compensation: If the Inventory service fails to reserve the item, it publishes an “InventoryReservationFailed” event. The Payment service then executes a compensating transaction to refund the payment, and the Order service marks the order as canceled.
This process allows the system to maintain data consistency across distributed services without using distributed trans- actions, which are often complex and not supported across different databases and systems.
502

PATTERN: “PROCESSOR-TODO-LIST” - PATTERN
 Fig. 35.7
The payment Service translates “Inventory Reservation Failed” to a “Payment Refund Registered” event. The “Payment Refund Registered” event opens a Processor-Todo for the Payment- Refund-Processor, which now has a task on the list.
The Payment-Refund-Processor issues a “Refund Payment” command, which results in a “Payment Refunded” event. The “Payment Refunded” event closes the task. If the refund does not work, the task will not get closed and will be retried on the next processor schedule until something happens that closes this task (maybe moving it to a Dead-Letter-Queue and processing
it manually by a Service Agent). 503

UNDERSTANDING EVENTSOURCING
For this process, the payment service does not need to have any specific knowledge about any other service. It just needs to know that inventory reservations can fail and can decide how to handle this.
The “Payment Refunded” event could be published, and any service interested in the fact that a specific payment has been refunded could act on it.
What’s the big difference from Sagas? We do not define a specific Process-Orchestrator or Saga-Process-Definition but simply act on the facts in the system. That’s it.
504

36
Pattern: The Reservation Pattern
Use Case Description
In distributed systems, there is often a need to synchronize workflows and actions with business- and technical constraints. Imagine you design a system to sell premium first-row concert tickets. The number if tickets is strictly limited, so you cannot go over your inventory. In a distributed system, you typically cannot rely on ACID-Transactions to ensure data consistency. Transactions typically do not span over system boundaries. The reservation pattern allows you to linearize workflows and ensure, that process-steps are executed in order.
How to model it?
The Reservation-Pattern always consists of two steps.
505

Reservation
Execution
UNDERSTANDING EVENTSOURCING
 Fig. 36.1
506

Where to apply it?
PATTERN: THE RESERVATION PATTERN
 Fig. 36.2
The Reservation-Pattern helps to synchronize concurrent ac- cess to a limited resource across aggregate boundaries. This could be a limited number of tickets available or a user who can only ever have 5 Todo Items active in the TODO List.
All these requirements focus on managing access to a limited resource within a highly distributed environment, where rely- ing on ACID transactions is either impossible or significantly limited.
507

UNDERSTANDING EVENTSOURCING
How to implement it?
Although it is modeled as Event, Read Model and Processor - the whole cycle of reservation and execution can be done within one single web-request.
Using a Database to synchronize access
On the first sight, a simple way to ensure consistency is a database to synchronize access. The system accesses the database to check constraints and verify, that an E-Mailadress is not yet taken.
Fig 36.3
We can enforce constraints by defining unique constraints in 508
 
PATTERN: THE RESERVATION PATTERN
the database. Using a database always comes with additional cost and complexity, but is certainly easier to understand if you have a traditional software development background.
Using aggregates to ensure consistency
For most use cases where the Reservation-Pattern applies, you can completely omit using a separate database by using specific aggregates to synchronize the access to limited resources.
Fig. 36.4
Using this approach sounds complicated but is in the end only 50 Lines of Code.
  @Aggregate
class ReserveEmailAggregate {
    @AggregateIdentifier
    lateinit var email: String
    var reserved: Boolean = false
}
509

UNDERSTANDING EVENTSOURCING
The trick we take advantage of here, there can only ever be one aggregate for a given ID at any point in time. So if we define the E-Mail address as the aggregate-id, it ensures, that an E-Mail can only be taken once.
Since the ReserveEmailAggregate protects the E-Mail address, we will also use it to directly register the Account.
 @CreationPolicy(CREATE_IF_MISSING)
@CommandHandler
fun handle(command: ReserveEMailCommand) {
    // (1)
    if (reserved) {
        throw CommandException(
            "email already registered"
) }
    // (2)
    AggregateLifecycle.createNew(
AccountAggregate::class.java ){
        AccountAggregate(
            RegisterAccountCommand(
) }
    command.aggregateId,
    command.email
)
    // (3)
    AggregateLifecycle.apply(
        EMailReservedEvent(
            command.aggregateId,
            command.email
) )
}
510

PATTERN: THE RESERVATION PATTERN
In (1) we check if the E-Mail is already reserved, exactly as we would do using a database. If this check succeeds, we create and store the actual Aggregate. We use the AggregateLifeCycle to create a new Instance of the AccountAggregate. If this succeeds, we can finally confirm that the E-Mail is taken by storing the E-Mail-Reserved Event in (3).
By implementing the Event Sourcing handler, we block the E- Mail address from being taken again (see again the check in the previous Code-Snippet).
 @EventSourcingHandler
fun handle(event: EMailReservedEvent) {
     reserved = true
     this.email = event.email
}
This approach is very simple and effective and should be in the toolbelt of every developer.
511

V
The missing chapters

37
Why the missing chapter?
My goal for this book was to cover the essentials of getting started with Event Sourcing: planning systems using Event Modeling, understanding eventual consistency, structuring events, building information in projections, and appreciating the importance of flexibility.
Not everything made it into the book, though. At nearly 500 pages, I had to make tough decisions about what to include. Some important questions were left unanswered, and since its release, I’ve been reflecting on how to address the topics I couldn’t fit in.
These are questions that come up daily in the Event Modeling and Event Sourcing community—questions I’ve grappled with myself, like handling sensitive data under GDPR. While some of these topics will be discussed in the podcast and newsletter, I’ve realized it’s important to consolidate this knowledge in one place.
515

UNDERSTANDING EVENTSOURCING
The reason I wrote this book was simple: I wanted to create the book I desperately needed five years ago. A book that could have saved me countless hours searching forums and chats for scattered answers.
But the topics I couldn’t cover mean - you still have to do exactly that. Building enterprise systems involves tackling these very challenges, and readers may still find themselves searching for answers, piecing together knowledge from various sources. While “Understanding EventSourcing” can save you significant time—in my case, several years—it doesn’t answer every question.
After much thought, I’ve found a solution to this gap: “the missing chapters.” These additional chapters will be an ongoing extension of the book, covering a wide range of topics that didn’t make it into the original version. I don’t yet know how many there will be, but I’m optimistic that, over time, this collection and the book itself will feel complete.
516

38
Handling Metadata
One topic we didn’t cover in the book is metadata. While it’s crucial to understand for many applications, I decided to skip it initially and address it later in the “missing chapters.”
Merriam-Webster defines metadata as “data that provides information about other data.”62 Essentially, metadata adds context to data, making it easier to understand, organize, and retrieve. It enables systems and users to quickly identify details like the purpose, source, structure, and relationships of data, all of which improve comprehension and usability.
In Event Sourcing, the unit of persistence is the event itself. We attach metadata to each event to give it context. This allows us to answer essential questions, such as:
Which command resulted in this event? Which user (if known) triggered the action?
62 https://www.merriam-webster.com/dictionary/metadata 517
 
UNDERSTANDING EVENTSOURCING
What happened before and after this event?
Common examples of metadata in Event Sourcing include: - Event ID
- Event version
- Correlation ID
- Causation ID
- User ID that triggered the event - Session ID
It’s important to distinguish between payload and metadata. The event payload holds the business-relevant information, while the metadata contains context-specific details that sup- port the event and give more context.
One powerful use of metadata is the inclusion of correlation and causation IDs, which enable system traceability by creating a clear data trail for each event.
The causation ID identifies the current step of a process, often corresponding to something like the request ID of an HTTP re- quest. As the process continues, the correlation ID is propagated across all subsequent steps, linking them together and making it easy to trace all events associated with the same process.
In Fig. 38.1, you can see three distinct process steps, each with a unique causation ID. Despite this, all steps share the same correlation ID “2”, which ties them together and ensures the process remains traceable.
518

HANDLING METADATA
 Fig. 38.1 - causation- and correlation-ID
If a problem arises with a command, correlation and causation IDs enable us to see exactly what the user did before the problem and, if necessary, replay all actions to restore the system to the exact state at the time the command was issued.
Combining well-configured metadata with a solid logging strat- egy allows us to use just one identifier to instantly locate all relevant context.
In Axon, all messages contain an identifier, payload, and meta- data, making it easier to manage and trace the flow of events.
 public interface Message<T> extends Serializable {
    String getIdentifier();
    MetaData getMetaData();
519

UNDERSTANDING EVENTSOURCING
     T getPayload();
... }
Metadata in general is just a list of key / value pairs modeled as a Map.
Context Information is attached to a message as early as possi- ble, for Commands typically by the sender.
To handle Metadata, Axon provides the concept of Correlation Data Providers.63
Correlation Data Providers are pluggable components that automatically supply metadata to messages. They allow context- specific information, like correlation and causation IDs, to be seamlessly attached to each message, ensuring that events carry the necessary information for traceability and analysis.
 public class MetaData implements Map<String, Object>
 public interface CorrelationDataProvider {
    Map<String, ?> correlationDataFor(Message<?>
message); }
 63 https://docs.axoniq.io/axon-framework-reference/4.10/messaging-conce pts/message-correlation/
520

HANDLING METADATA
Axon provides two default implementations of a Correlation Data Provider.
Message Origin Provider
When a command is processed and results in an event, the *Message Origin Provider* attaches the command identifier to the metadata of all resulting events. It provides two key
identifiers:
Correlation ID: This is the identifier of the origin message—in this case, the command identifier—which ensures events can be linked back to the initiating command.
Trace ID: This is a persistent identifier that passes from message to message, enabling tracking across an entire flow.
Simple Correlation Data Provider
The Simple Correlation Data Provider allows for additional head- ers to be configured and passed from message to message. Essentially, it acts as a configurable “copy” mechanism for metadata headers, enabling flexible data propagation.
This is crucial, as the flexible copy mechanism allows consistent propagation of information throughout the entire flow, from message to message.
521

UNDERSTANDING EVENTSOURCING
Implementing a Custom Correlation Data Provider
Often, business requirements call for additional metadata. Im- plementing a custom Correlation Data Provider is a straightfor- ward way to meet these needs.
For example, if a user initiates an HTTP POST request, you might store a session cookie for the user’s current cart on the client side. A custom correlation data provider could then capture this session identifier, attaching it to commands and events, so the user’s session can be traced throughout the system.
Fig. 38.2 - Metadata provides context
In addition to the correlation and trace IDs already discussed, each event should contain the cart-session ID, if available.
 522

HANDLING METADATA
We assume that the cart session is stored as a cookie on the client side. With every request, this cookie is sent to our system, allowing us to track the user’s cart session. The cart session is initialized when the first item is added to the cart (as discussed in Part II of the book).
One straightforward way to provide this functionality is by implementing a CartSessionIdProvider.
 interface CartSessionIdProvider {
     fun provideCartSession(): String?
}
The imlementation coud rely on a ThreadLocal. ThreadLocals are a simple concept that essentially allow us to set a global variable within the context of a single thread’s execution.
Be careful combining ThreadLocals with the new virtual Threads available in the JVM.
 @Component
class ThreadLocalCartSessionIdProvider :
CartSessionIdProvider {
    // (1)
    private val threadLocal = ThreadLocal<String>()
    override fun provideCartSession(): String? =
    threadLocal.get()
// (2)
523

UNDERSTANDING EVENTSOURCING
     fun setSessionId(sessionId: String) {
        this.threadLocal.set(sessionId)
}
    // (3)
    fun reset() {
        threadLocal.remove()
    }
}
In (1) we initialize the ThreadLocal. In (2) and (3) we set the value for each execution. This could happen for each web request cycle.
Now implementing the Correlation Data Provider is simple. For each command execution we only need to provide the Cart- Session-ID as an entry in the Metadata-Map ( in (1) )
 @Component
 class CartSessionCorrelationDataProvider(
     val cartSessionIdProvider: CartSessionIdProvider,
 ) : CorrelationDataProvider {
     override fun correlationDataFor(
         message: Message<*>?
     ): MutableMap<String, *> {
         if (message is CommandMessage) {
             // (1)
             return mutableMapOf(
                 CartSessionIdProvider.SESSION_ID to
                 cartSessionIdProvider.provideCartSession()
             )
 524

HANDLING METADATA
 }
        return mutableMapOf<String, String>()
    }
}
We don’t need to take any further steps, as Axon automatically applies all metadata provided during command execution to all resulting events.
On the branch “missing-chapters-1/correlation”, you’ll find an executable test case that lets you experiment with metadata. This approach works exceptionally well and provides a consis- tent way to manage metadata.
Fig. 38.3 - Sample Metadata
Working with Metadata
Now that we’ve added the metadata, how do we make use of it? The most crucial part is already done—we’ve persisted it along
 525

UNDERSTANDING EVENTSOURCING
with the event.
To access this information later, the simplest approach is to use Axon’s @MetadataValue annotation. This allows you to directly inject metadata values into your event handling components whenever you need them.
 @Component
class SampleEventHandler {
    @EventHandler
    fun on(
        event: ItemAddedEvent,
        @MetaDataValue("cartSessionId") cartSession:
        String?,
) {...} }
It’s also possible to create a custom ParameterResolver64 using a custom annotation, though this is rarely necessary. For more on this, you can refer to the relevant chapter in the Axon documentation.
A great use case for processing metadata is to populate the MDC (Mapped Diagnostic Context) for your logging system, ensuring that every log message includes all relevant (non-sensitive) metadata. This adds valuable context to your logs, making it easier to trace and debug application flows.
Logging plays a less critical role in event-sourced systems compared to traditional CRUD-based systems, as the data trail
64 https://docs.axoniq.io/message-handler-customization-guide/developme nt/parameter-resolvers/
 526

HANDLING METADATA
is inherently persisted in the Event Store. However, logging remains crucial for quickly analyzing issues, especially when collaboration with other teams and departments is required.
Conclusion
Metadata is essential for ensuring traceability and providing deeper insights into your system’s data trail. Don’t underesti- mate its power. Event Sourcing is about preserving all data, and that includes metadata—so make sure it’s handled carefully.
It’s easy to fall into the trap of thinking, “We’ll deal with metadata later.” However, this approach can lead to problems down the road. By applying the techniques shown in this “miss- ing chapter,” you can establish a robust metadata-handling strategy that simply works.
Everything covered in this chapter can be explored in the branch “missing-chapters-1/correlation.”
527

39
Handling Security
You find the Code for this chapter on the branch
missing-chapters-2/security
Almost every system must address security in one way or another. How do you model and handle cross-cutting concerns like security and authorization? If I had to guess, aside from questions about implementing automations, this is one of the most frequently asked questions in the community.
Should security be part of your model? Should you explicitly model logins, roles, and required permissions?
As with many things, it depends.
Is security in your system purely a technical requirement, or is permission management a critical aspect of the system’s func- tionality? Do business stakeholders regularly discuss security- related topics, or do they simply assume the system to be secure
 528

HANDLING SECURITY
without concerning themselves with the details?
Let’s consider a concrete example. In our e-commerce system, we need to provide a back office where clerks can handle orders, manage packaging, and send out prepared shipments.
Fig. 39.1 - Backoffice Clerk View
Sometimes, it’s necessary to cancel an order, for example, if a certain coffee brand is out of stock.
Additionally, there’s functionality to block customers. However, not all clerks can perform this action, as it requires admin privileges. Blocking a customer might be necessary due to on- going payment issues or too many unpaid orders. Nevertheless, blocking is a serious action and should only be done after careful consideration.
 529

UNDERSTANDING EVENTSOURCING
 Fig. 39.2 - Backoffice Admin View
Before we start implementing the system, let’s model the required steps. We’ll stay on the same board but provide a separate model for this process.
As always, we follow the same cycle described in Part II of the book. First, we identify the story of the system—what happens in the back office?
Fig. 39.3 - Backoffice Storyline
 530

HANDLING SECURITY
As soon as an order is submitted, the next available clerk picks it, assigns it to himself and begins working on it. Once a clerk has been assigned, the ordered products are packaged and shipped. In rare cases, an order might be canceled, or the customer might be blocked.
In addition to the storyline, we need to address two distinct actor flows: one for clerks and another one for admins. Instead of merging these flows into a single screen, I clearly separate them. Whenever possible, I model flows from the perspective of a single actor. For example, if I want to illustrate something an admin does, I focus exclusively on the admin’s flow.
The blue arrows above the Event Model represent “chapters” and “sub-chapters,” as described in Chapter 18. They help clearly visualize which flow is currently active.
Fig. 39.4 - Actor Flows
 531

UNDERSTANDING EVENTSOURCING
Using actor lanes, we clearly show which actor is responsible for a specific screen or action in the system. However, it’s important to distinguish between business roles and technical roles.
Actors in the Event Model represent business roles, which can have multiple technical roles assigned to it. Conversely, a technical role can be associated with many different business roles.
Consider the example of a customer in a supermarket. Cus- tomers can move freely throughout the store, but some areas, such as the liquor section, may require the customer to be an adult. In this case, the actor is the customer, but different technical roles (e.g., “adult”) are assigned depending on the context.
Business Role:
Refers to the high-level functional role that defines the primary responsibilities or context of the user (an “Admin” manages the system, a “Customer” interacts with products).
Technical Role:
Represents fine-grained technical classifications within a busi- ness role that influence access control, permissions, or behavior within the system (e.g., Adult customers have broader access than Minor customers).
This terminology aligns well with access control and RBAC (Role- Based Access Control) frameworks, where you define:
532

HANDLING SECURITY
• businessrolesatthemacrolevel.
• technicalrolestoenforcepermissionsorrestrictionswithin
those broader roles
For the rest of this chapter, I will use the term “Role” to refer specifically to the technical roles that enable an actor to perform certain actions in the system.
Dealing with technical roles
Business experts often don’t concern themselves with technical roles. They understand the functionality, such as blocking a customer, but they don’t care whether the technical role required is “admin” or “clerk”. Business roles are modeled using actor lanes, and I typically don’t include technical roles in the Event Model at all—a decision that initially surprises many developers.
Let’s take some examples from the back-office flow. I inten- tionally omit attributes on commands, events and read models not relevant to this chapter in the following examples.
To assign a clerk, the “Assign Clerk” command requires the clerk ID. The clerk ID is determined from the currently logged- in user. The red arrow in the model highlights a problem: we currently don’t know how to model this, as the system doesn’t inherently know where the clerk ID comes from.
533

UNDERSTANDING EVENTSOURCING
The worst thing we can do is simply assume the ID will be available. Assumptions can be wrong, and building a system on incorrect assumptions is something we should prevent at all times. Being very explicit is one of the biggest benefits of Event Modeling.
Fig. 39.5 - Missing Authentication data
Most likely, the clerk has logged in at some point. Does this mean we need a “User Logged-In” event?
Surprisingly, the answer is often no—at least not in this flow— unless the fact that a user logged in at a specific time changes the system’s state. What does that mean? Is any business process dependent on the “User Logged-In” event? Does it update a read model? Does any part of the system need to track how often a
 534

HANDLING SECURITY
user logs in? In other words, is logging in part of the system’s functionality, or is it just a necessary technical precondition?
For most systems, it’s the latter. What typically matters is whether the user is currently logged in and has the necessary permissions to perform an action.
So how do we represent login in the Event Model? If I want to highlight the occurrence of a login, I usually use the screens for the login page along with a read model that provides the
“Logged-In User.”
Fig. 39.6 - Showing the login flow
This read model differs from others in that it isn’t connected to any events or event streams. Its primary purpose is simply to signal that the logged-in user is available from this point
 535

UNDERSTANDING EVENTSOURCING
onward. This approach aligns with the core principle of Event Modeling, which emphasizes making the flow of information clear and explicit at all times.
Fig. 39.7 - using Read Models to provide “authentication” data
It’s a good idea to use examples to show how the login name is used in subsequent slices, as illustrated in Figure 39.7. From the examples you can indicate, that the username from the Login is used as the clerkId in the command.
Authentication and authorization are often handled by third- party systems like Keycloak, an OpenID Connect provider that offers these features out of the box.
Regardless of whether this functionality comes from an external system or is custom-built, we can still model the user/login flow separately to facilitate high-level discussions with business ex-
 536

HANDLING SECURITY
perts. Since this is purely a technical flow meant for discussion, and not part of the system’s Event Model, I typically keep it in a separate model.
Fig. 39.8 - modeling technical flows
Modeling Authorization
Security checks are applied at the slice level, determining whether the user can execute a specific command or query certain information. Now that we know who is currently logged in, how do we define the permissions required for a particular slice? For example, how do we specify that the technical role
“admin” is necessary to block a customer?
 537

UNDERSTANDING EVENTSOURCING
 Fig. 39.9 - Modeling technical roles is not necessary
Short answer: Typically I don’t. That’s an implementation detail. The required role can change without affecting the overall flow.
This can be a tough adjustment for many developers. Adding “implementation hints” to the model may seem useful during development, but the more implementation details you include,
the harder it becomes to focus on the essential information.
Implementing Security
How authentication and authorization are implemented can vary significantly, from handcrafted solutions and simple database persistence using a username and (hopefully properly hashed) password, to state-of-the-art approaches leveraging OAuth2 and OpenID Connect (OIDC).
538

HANDLING SECURITY
For a working implementation using OIDC and Keycloak, check out the branch “missing-chapters-2/security.” While an entire book could be written on security and its implementation, this chapter focuses on the essential components.
Interestingly, there’s an open pull request for Axon that pro- vides a default implementation for what I’ll demonstrate here. It’s well worth exploring.65
Fig. 39.10 - Key Cloak as Authentication / Authorization Provider
In this example, we fully delegate security to Keycloak, an external Authentication and Access Management solution.
The system redirects users to a login page hosted by a locally running Keycloak instance. After successful authentication, the user is redirected back, and the login data is exchanged for an access token. Keycloak allows us to create new users and assign
65 https://github.com/AxonFramework/AxonFramework/pull/3057/ 539
   
UNDERSTANDING EVENTSOURCING
roles through its Admin Console. When you start the applica- tion on the branch, it will automatically spin up a fully pre- configured Keycloak instance, accessible at “localhost:8081” (credentials: admin / admin).
The implementation concept is straightforward: once a user is authenticated, we extract their assigned roles and include them as metadata in each command. When a command is executed, the roles in the metadata are compared against the roles required by the command handler. If they don’t match, the command execution is rejected.
This assumes that we authorize the user at the Command- and Query-Level. Additionally, it may be beneficial to authorize at the API level to prevent unauthorized calls from entering the system. This is an architectural decision.
This approach keeps our domain free from infrastructural concerns, leveraging what we learned about metadata in the previous chapter.
First, we need to ensure the appropriate permissions are as- signed to each command handler. For example, to secure the clerk assignment to an order, we use the @Secured annotation on the command handler.
 @Secured("ROLE_ADMIN")
@CommandHandler
fun handle(command: AssignClerkCommand) {
    AggregateLifecycle.apply(
540

HANDLING SECURITY
         ClerkAssignedEvent(
            clerkId = command.clerkId,
            orderId = command.orderId,
), )
}
In this chapter, we’ll introduce a new Axon concept: the HandlerEnhancerDefinition66 .
The code is somewhat technical, so I’ll focus on the key compo- nents.
The function “hasRequiredRoles” in the class “SecuredMet hodMessageHandlerDefinition” extracts metadata from the message and compares it against the roles required by the command handler. These roles are defined using the @Secured annotation on each method.
 fun hasRequiredRoles(message: Message<*>): Boolean {
    message.metaData.get("authorities")?.let {
        return it.map { it.authority
        }.containsAll(requiredRoles)
    }
    return false
}
If the user does not have the proper roles assigned, an AccessDen iedException is thrown.
66 https://docs.axoniq.io/message-handler-customization-guide/developme nt/handler-enhancers/
 541

UNDERSTANDING EVENTSOURCING
 if (!hasRequiredRoles(message)) {
    throw AccessDeniedException(...)
}
return super.handle(message, target)
This clear separation of domain logic and security concerns enables us to write simple tests to verify all security aspects. We don’t need to mock any services or objects; we only need to provide the correct metadata on the message.
In the following code snippet, we assign a list of SimpleGranted Authorities (which represent the assigned roles) to a command message and verify successful handler execution. Of course, we should also include a negative test, executing the same test case without roles and expecting the AccessDeniedException.
 // provide prefilled metadata
val metaData = mapOf(
    "authorities" to listOf(
        SimpleGrantedAuthority("ROLE_CLERK")
) )
fixture
    .given(events)
    .`when`(
        command,
metaData )
    .expectSuccessfulHandlerExecution()
    .expectEvents(
        *expectedEvents.toTypedArray()
542

HANDLING SECURITY
 )
Conclusion
Security is a complex topic, too vast to cover comprehensively in a single chapter.
The goal of this chapter wasn’t to teach you how to implement security and authorization in detail. There are far too many nuanced approaches and technologies to be addressed. Instead, the focus was on demonstrating how to model security. Mapping the flow of information is far more critical than the specifics of implementation.
Technologies are interchangeable. The external Keycloak sys- tem could be replaced tomorrow with another off-the-shelf product or even an in-house database solution. This wouldn’t affect the information flow of our system.
Finding the right balance between modeling information flow and including implementation details took me a long time. It’s easy to add too many implementation “hints” to the Event Model, which can significantly harm its readability and main- tainability.
The Event Model isn’t about implementation. So, where should implementation hints go?
543

UNDERSTANDING EVENTSOURCING
You could include them in a separate document describing secu- rity roles and permissions. Alternatively, you might annotate your slices, as we briefly discussed in Chapter 30. In my opinion, it’s perfectly fine to include some implementation details on the board—just ensure they don’t influence the business process or compromise readability.
Fig. 39.11 - Implementation hints
As always, what you read here reflects my point of view and current understanding. I hope it gives you some ideas on how to approach this, and I encourage you to discuss with your business experts and development team to determine the best way for you to model it.
 544

40
GDPR - Handling sensitive Data
In short, GDPR (General Data Protection Regulation) clearly defines the rules regarding how a company is allowed to process personally identifiable information (PII) of its customers.
What is personally identifiable information?
For many developers, it’s often surprising what qualifies as personally identifiable information. Some examples are quite obvious, like names, email addresses, physical addresses, and bank details. However, there are also less obvious examples, such as IP addresses or date of birth, which can still be used to identify a person.
It’s important to note that this chapter is not about the legal aspects of GDPR. I am not a legal expert, nor do I claim to be one. The truth is, handling personal data responsibly has been a critical topic long before GDPR came into existence. GDPR simply brought this issue into sharper focus. This chapter will
545

UNDERSTANDING EVENTSOURCING
concentrate on the technical implications of GDPR and how we can address its challenges when working with Event Sourcing.
GDPR is one of the first topics that comes up when discussing Event Sourcing with customers and developers. The concerns are significant: How can you delete data when the past is immutable? Is it even possible? In this chapter, we’ll explore ways to handle these challenges and examine potential imple- mentations that align GDPR compliance with the principles of Event Sourcing.
Why GDPR matters
The GDPR establishes two key rights: the right to access data and the right to be forgotten. These rights are powerful tools that put customers in control of their personal information and requires explicit consent as a prerequisite for data processing.
The right to access allows customers to know exactly what information an organization holds about them, why it is being used, and who it is shared with. This represents a significant step forward in regulated data transparency.
The right to be forgotten takes this empowerment further, enabling customers to request the deletion of their data when it is no longer necessary or has been mishandled. However, it ́s important to understand that there are legitimate reasons to process and retain personal information, such as fulfilling a business purpose or honoring a clear contractual agreement between the parties involved.
546

GDPR - HANDLING SENSITIVE DATA
These rights are more than just compliance obligations—they are about giving customers the confidence and freedom to manage their digital footprint.
While implementing these rights poses challenges for informa- tion systems, addressing them effectively is critical for building systems that respect and uphold customer rights.
What is the challenge?
Event Sourcing highlights a fundamental principle: the past is immutable and should never be altered. This directly contrasts with GDPR, which strictly requires the ability to modify the past by deleting or correcting already stored personal information.
However, it’s often overlooked that this isn’t a challenge exclu- sive to Event Sourced systems. Traditional systems also contain “immutable” data. It’s not just the “live” data that’s impacted— personal information such as names, email addresses, phone
numbers, social security numbers, bank details, and health- related data can also exist in logs, backups, Excel sheets, and even unencrypted files if a system isn’t properly designed. All of this falls under the scope of GDPR.
So, how do we design information systems that comply with GDPR while safely managing customer data? Unfortunately, there’s no simple or universal solution to this problem.
There are, of course, technical approaches that can make han- dling such data easier, and we will explore them later in this
547

UNDERSTANDING EVENTSOURCING
chapter. But for me, one principle stands above all others.
Data Minimalism
The simplest data to handle is the data that doesn’t exist. While it’s tempting to store as much information as possible in a system—after all, someone might find a use for it later—this approach is not GDPR-compliant.
A guiding principle I always follow is data minimalism: store only what is essential and absolutely necessary. Event Modeling is an excellent tool to support this principle, as it provides transparency into what data is available in the system and where it is used. By leveraging the information completeness check discussed in Chapter 3, we can precisely identify where personal information is used within the system.
Data minimalism also means avoiding redundant storage of information unless there is a strong justification for it.
Let’s consider a concrete example from our domain. One of the most critical events in any e-Commerce system is the “Order Submitted” event, which triggers a series of business processes such as fulfillment, shipping, and payment.
Most of these processes require some sort of personal data. Sometimes, we delegate the handling of this data to external payment providers; other times, we manage it ourselves, such as when shipping products to a customer’s address.
548

GDPR - HANDLING SENSITIVE DATA
During the shopping journey, there are natural events that mark key steps, such as “Cart Submitted,” which leads the customer to the checkout. At the checkout stage, the customer enters the necessary personal information, which might be recorded in a “Checkout Completed” event. The customer is then redirected to a “Final Review” page before ultimately submitting their order through the “Order Submitted” event.
Fig. 40.1 - External “Fat” Event containing personal information
It can be tempting to consolidate the personal information of the whole process into the “Order Submitted” event at the end. Storing this information redundantly can complicate efforts to trace where specific data is used.
If a customer requests their data to be deleted, does this mean the “Order Submitted” event containing that data should be altered or removed? For most businesses, this isn’t practical, as critical processes often depend on the submitted order. Removing personal information should not compromise the integrity of the event or obscure the fact that the order was placed.
 549

UNDERSTANDING EVENTSOURCING
Consider a scenario where a customer requests the deletion of their address data—especially if a digital product was ordered. In such cases, the address data isn’t necessary for fulfilling the order and arguably should not have been collected in the first place.
Adhering to principles like data minimalism and designing events to store only the information essential for each step can create systems that are both efficient and GDPR-compliant. For instance, using fine-grained events such as “Cart Submitted,” “Checkout Completed,” and “Order Submitted” can help limit personal data storage to the “Checkout Completed” event.
In such a design, the “Order Submitted” event might only need to store an order ID and maybe an additional session ID as a reference, indicating the order was finalized without containing unnecessary personal data.
Fig. 40.2 - Keeping Events small
The more personal information is scattered across the system, the harder it becomes to track its usage. In some cases, it can even become nearly impossible, as the data ends up being spread
 550

everywhere.
GDPR - HANDLING SENSITIVE DATA
 Fig. 40.3 - Scattered personal data
Adam Dymitruk famously said in one of our recent podcast episodes67 that we shouldn’t focus on Event Design. Instead, we should simply write down events as they happen, without overthinking or striving for a perfect design. This approach often results in events that are more closely aligned with ac- tual business processes, making it easier to reason about how personal data is used throughout the system.
The technical side of GDPR
Unfortunately, we cannot simply choose not to process personal information. Many businesses—such as insurance companies, banks, and even medical practices—heavily depend on process- ing personal data to support their core operations. As such, we must find technical solutions to handle personal information in a compliant and responsible manner.
67 https://creators.spotify.com/pod/show/eventmodeling/episodes/Episode- 5-e2rodoo/a-ablksd1
 551

UNDERSTANDING EVENTSOURCING
In the subsequent paragraphs, I will present two common approaches for dealing with personal information in Event Sourced systems. While there may be other methods, these are the ones most commonly used.
Crypto Shredding
The concept of crypto shredding is straightforward: encrypt all personal data stored in events using a dedicated key for each user or customer before persisting the event. When an event is loaded, the data is automatically decrypted.
If a customer requests their data to be deleted, you simply delete the encryption key, rendering the data unreadable. This ap- proach allows you to retain the original events without altering the history in the event store.
However, the downside of this method is that it requires a robust cryptographic infrastructure to securely manage potentially thousands of encryption keys. This process must be fully automated, as manual handling is impractical in most scenarios.
For those interested, you can find a possible implementation of crypto shredding using Axon on the branch: missing-chapters- 3/gdpr. While we will briefly touch on the implementation details in the following paragraphs, the primary focus here is to understand the concept and its implications.
552

GDPR - HANDLING SENSITIVE DATA
 Fig. 40.4 - Crypto Shredding
For each set of personal data—such as all the data related to a specific customer—we generate a unique encryption key. This key is dedicated solely to that customer. Before an event is persisted, we encrypt all fields containing personal data during the serialization process. To achieve this, we need to inform the system about which data should be encrypted, using a custom annotation like @EncryptedField, for example.
 data class OrderPreparedEvent(
    ...
    @EncryptedField
    var name: String,
    @EncryptedField
    var surname: String,
    @EncryptionKeyIdentifier
    var customerId: UUID,
) : Event
553

UNDERSTANDING EVENTSOURCING
To ensure we use the same key for all of a customer’s data, we also annotate the identifier used to look up or create the secret encryption key. In the OrderPreparedEvent, for example, we annotate the customerId with @EncryptionKeyIdentifier.
Whenever we load or create an event with the customerId annotated with @EncryptionKeyIdentifier, the same encryption key will be used to encrypt or decrypt the event. If the key is not yet available, the system will automatically create a new one and store it in a database table.
Of course it is also possible to use a managed cloud service like AWS Key Management Service (KMS) or HashiCorp Vault to han- dle the keys and often it should be the preferred implementation.
If the customer requests data deletion, we simply delete the encryption key and, if needed, replace the encrypted values with default placeholders like “deleted.”
You will find a Test-Case CryptoShreddingIntegrationTest on the branch that showcases exactly how Crypto Shredding is meant to work.
 val prepareOrder =
RandomData.newInstance<PrepareOrderCommand> {
    this.orderId = aggregateId
    this.name = "Martin"
}
// wait until the assertion holds true or time out
awaitUntilAssserted {
    streamAssertions.assertEvent(aggregateId.toString())
554

GDPR - HANDLING SENSITIVE DATA
     { event ->
        // (1) assert "Martin" is the decrypted value
        event is OrderPreparedEvent && event.name ==
        "Martin"
} }
// (2) delete the key
keyRepository.deleteById(aggregateId)
awaitUntilAssserted {
    streamAssertions.assertEvent(aggregateId) { event
    ->
} }
// (3) assert the value is now a default
// value to mark deleted data
event is OrderPreparedEvent && event.name ==
DELETED_DEFAULT
Forgettable Payload
If crypto shredding is not an option, “Forgettable Payload” is another way to handle personal data. I first encountered the term “Forgettable Payload” in a blog post by Mathias Verraes68.
What if we strictly separate personal and non-personal data from the outset? What if we store personal information in database tables, for instance, and reference it by an ID in our
68 https://verraes.net/2019/05/eventsourcing-patterns-forgettable-payload s/
 555

UNDERSTANDING EVENTSOURCING
events?
In the example below, rather than directly storing personal information in the Event Payload, we store a reference using the event ID and the personal data identifier. This means that for each event, there is at most one corresponding entry in the
“personal_data” table.
Fig. 40.5 - Forgettable Payloads
Whenever we need to access personal information, we can simply query it by the reference ID when an event is loaded. This approach makes data removal much simpler, but it does come with a cost, as we must maintain an additional data store.
 556

GDPR - HANDLING SENSITIVE DATA
If a customer requests data deletion, we can delete all personal information associated with the customer and replace it with meaningful default values for our system.
Handling Projections
So far, we have focused primarily on the write side of our system. All incoming data is either properly encrypted or externalized to another data store. But what about the read side? Data stored in our events could have been used to generate numerous persisted database projections.
Deleting an encryption key or clearing data in an external data store will not automatically update these projections. As discussed in Chapter 28, replays are necessary for projections, and this process is exactly what’s required here. Since the data is no longer available in the events, a replay will discard all the previously projected information.
But how do you identify which projections need to be replayed?
This is where the Event Model becomes invaluable. It provides a clear view of where specific pieces of information, such as a customer’s surname, are used throughout the system. If you’re interested in how professional Event Modeling tools can enhance this process, check out the example provided in the linked video69.
69 https://tinyurl.com/cd73kxvw
557
  
UNDERSTANDING EVENTSOURCING
Additionally, it might be a good practice to publish an event indicating that personal data has been purged—such as a “Per- sonal Data Purged” event. However, ensure that this event does not unintentionally include personal data, such as a personal identifier.
Conclusion
Handling sensitive data correctly is a crucial aspect of designing information systems. It is both our duty and obligation to handle it properly and in compliance with the law.
In this chapter, you’ve learned a few approaches for designing GDPR-compliant information systems using technical solutions like Crypto-Shredding and Forgettable Payloads, as well as the more natural approach of designing data streams correctly from the outset.
Which approach is best depends heavily on the context and the project setup. However, the most important takeaway is not to ignore the fact that personal data requires special treatment. Ignoring it and hoping that no one will ask is a recipe for disaster and directly contradicts the fundamental principle of Event Sourcing—treating information properly.
558

41
Handling the UI
You will find the code for this chapter on the branches “ missing-chapters-4/ui-with-polling” and “ missing-chapters-4/ui-with-sse”
Every developer building their first event-sourced system will encounter this challenge: A button click triggers an action in the system that changes state, but the UI continues to display outdated data. Only after a few manual refreshes does the fully updated data appear. What is the problem?
When the client queries the updated information, it can never know if the projections are still updating in the background and thus stale data might be returned back to the client.
  559

UNDERSTANDING EVENTSOURCING
 Fig. 41.1 - Data Flow in Eventsourcing
What most developers don’t realize is that this behavior is a feature, not a problem when approached correctly. We’ve discussed eventual consistency several times throughout this book, and this situation is a perfect example .
In Chapter 4, we explored the concept of CQRS, which involves separating the Write Side (handling commands) from the Read Side (querying projections). When the system state changes, it typically impacts one or more projections used by clients. Typically, these projections are not updated immediately after a command is processed but asynchronously in the background.
Traditional user interface design often conflicts with this sep- aration. When a user clicks a button, the typical user interface expects the result of this operation to be included directly in the
560

HANDLING THE UI
response.
In a classic CQRS-based approach, a write operation typically returns only a status code, indicating whether the operation succeeded or failed—that’s it. To retrieve the updated data, the client needs to query it in a separate step.
This introduces a small challenge: the client doesn’t know when the data has been updated, making it unclear when to issue the query. If the query is sent too early, stale data will be returned. If it’s sent too late, users may experience frustration due to unresponsive interfaces.
Since solving the “eventual consistency problem” can be chal- lenging for those new to the concept, the instinctive reaction is often to abandon eventual consistency and revert back to immediate consistency. This would allow developers to stick with the old paradigm, returning updated data directly in the response after each action.
In chapter 4 we already discussed the problems with this ap- proach. The core idea of CQRS is to enable system scalabil- ity by allowing changes to propagate asynchronously in the background. Doing this synchronously becomes increasingly challenging as a single change can trigger updates to numerous projections. If everything were required to occur within the same transaction, a failure in updating one projection could result in rolling back the entire transaction.
But that is not the only issue. Oftentimes it ́s not clear which part of the User Interface is affected by a change in the system.
561

UNDERSTANDING EVENTSOURCING
Always returning the full data to display the cart page is unnec- essarily complex if only the inventory changed for one single product.
Accessing data from the client
User interfaces are rarely these clean, focused designs that present only one piece of critical information at a time. More often than not, they are complex entities, frequently blending different business concepts and multiple domains into a single screen as displayed in Fig. 41.2.
Fig. 41.2
This complexity almost always leads to the need for multiple Read Models to construct a single screen ( Fig. 41.3 ). Whenever something in the system changes, the screen must be updated— either by refreshing specific parts or reloading the entire screen.
 562

HANDLING THE UI
 Fig. 41.3
Each Read Model could be a dedicated database projection, updated asynchronously in the background. So how can you know when all projections have finally caught up? The simple answer is: you can’t.
Backend for Frontend
Instead of allowing the Client to access several Read Models directly via API, the introduction of a “Backend for Frontend”70
70 https://philcalcado.com/2015/09/18/the_back_end_for_front_end_patt ern_bff.html
 563

is often discussed.
UNDERSTANDING EVENTSOURCING
 Fig. 41.4
A “Backend for Frontend” (BFF) usually serves as a lightweight aggregation layer on top of one or more APIs consumed by a client. It is designed to tailor data specifically for a particular type of client. While this approach offers benefits such as unified data and fewer API calls, if done wrong it can create tight coupling between vertical slices and, at times, evolve its own lifecycle.
564

HANDLING THE UI
I have seen BFFs turn into their own business entities with their own logic and even dedicated teams maintaining them.
In Chapter 10, we explored the concept of Vertical Slicing, which helps break the system into small, independent units of work. Introducing a BFF can create a subtle layer of coupling between these slices. A change in the UI may necessitate reworking both the BFF and one or more Read Models, which can undermine the original intent of the sliced architecture. For that reason, we will not explore the BFF approach further in this chapter.
Structuring the UI
A vertical slice is essentially a functional block that spans from the UI, through the API, Read Model or Command Handler, domain logic, and finally to (event) persistence ( Fig. 41.5 ).
Fig. 41.5
 565

UNDERSTANDING EVENTSOURCING
When done correctly, vertical slices should also be mirrored in the module or folder structure of the frontend. If the frontend is written in the same language as the backend, I place the frontend components directly within the corresponding slice packages.
Technologies like Apache Wicket71 or Vaadin72 are excellent examples that align well with this approach.
However, in most projects I’m involved with, the frontend is developed using JavaScript or TypeScript, so the Frontend lives in a different project typically. For this chapter, let’s assume the small frontend in Fig. 41.6 built with React73 and Next.js74
This user interface provides a simple view on the system while additionally allowing to change inventory and product pricing to test how the system reacts to these changes.
Fig. 41.6
71 https://wicket.apache.org/ 72 https://vaadin.com/
73 https://react.dev
74 https://nextjs.org
  566

HANDLING THE UI
If you start the application on the branch for this chapter (missing-chapters-4/ui-with-polling), the UI will automat- ically be started with the system and connected to the running backend. It will be accessible on “localhost:8080/index.html”.
First, let’s begin with the structure of the UI project. It mirrors the exact structure of our slices.
Fig. 41.7 - UI Project structure
For each command, we have a small API wrapper function that calls the corresponding API-endpoint. Since the slice is named “changeinventory”, we put this wrapper in the “changeinven-
tory” folder.
 567

UNDERSTANDING EVENTSOURCING
 export const changeInventory = async (
    productId: string,
    inventory:number) => {
    await fetch(`/changeinventory/...`, {
}) }
method: 'POST',
headers: {
    'Content-Type': 'application/json'
}
In the same way, each View-Component is placed in the cor- responding slice folder. To display the inventories for ex- ample, there is a folder “inventories” which contains the UI- component.
Fig. 41.8
But how do we connect View-Components to their correspond- ing Backend-API? In the following paragraphs, I will present two possible approaches. Each works slightly differently and has its own use cases, which we will discuss.
Fenced Client Side Polling / Server Side Polling
We can simply let the components poll the system state pe- riodically. Although client-side polling is a solid and simple
 568

HANDLING THE UI
approach, I get these rolling eyes whenever I mention it. The solution is considered old-school nowadays.
The first attempt to client side polling might be to simply query the endpoints every few seconds. This is neither sustainable nor scalable as the number of clients increases above a certain threshold. It will put unnecessary load on the system, as each client is continually asking if something has changed, which, most of the time, isn’t the case.
So, how can we optimize this?
Instead of asking every few seconds, “Has something changed?”, it would be far more efficient if the client only queried when it knew something had happened. Ideally, the client would know exactly which version of a projection it needs and keeps asking only until it receives this specific version. For example, if the client knows it needs version 172 of a projection but currently has version 168, it would only need to ask a few times before receiving version 172. After that, it could stop and
“sleep” until something new happens.
How can we version a projection? One approach is to use the aggregate sequence. In the Axon Framework, every time a command is executed, the aggregate sequence number is incremented for the affected aggregate. By persisting this sequence number alongside a projection, it can be used to determine whether a projection has caught up with the latest changes.
A different but similar approach is to use something like a 569

UNDERSTANDING EVENTSOURCING
Client Correlation Id passed from the client that gets persisted alongside the projection. This approach is described in the linked article by Frank Scheffler75.
To make client side polling work, we need to ensure the client receives the updated aggregate sequence after each command execution. This can be easily accomplished by customizing the return value of the command handler. For our use case, we define a new class CommandResult, which contains the aggregate ID and sequence number.
 data class CommandResult(
    val identifier: UUID,
    val aggregateSequence: Long,
)
// Command execution
@CommandHandler
@CreationPolicy(CREATE_IF_MISSING)
fun handle(
    command: AddItemCommand
): CommandResult {
    return CommandResult(
        command.aggregateId,
        // (1) get the current aggregate version
        AggregateLifecycle.getVersion()
) }
Using AggregateLifeCycle.getVersion() (1) gives us the current aggregate sequence, which we can then pass to the client as
75 https://medium.com/digitalfrontiers/axon-101-handling-client-side-cor relation-ids-3f11db66e0a6
 570

HANDLING THE UI
plain JSON after the command execution. The version can be persisted on the client side and subsequently used to query the latest information.
Some developers might immediately argue that this approach violates the principles of CQRS, as a command handler should not return any information. I disagree. While it’s true that a command handler should not read any persisted information, returning context information—such as the aggregate version— after a command execution is not only acceptable but, in my opinion, a best practice.
Now that the client is aware of the expected version, we just need to track when the projection has caught up. For this, I either define a dedicated version table for each projection or persist the sequence number along the projection table itself (Fig. 41.9).
If there are multiple projections from the same Aggregate, they typically maintain dedicated version tables.
 {
   "identifier":"691538cf-...",
   "aggregateSequence":7
}
571

UNDERSTANDING EVENTSOURCING
 Fig. 41.9
Now, the projector only needs to update the projection version alongside the projection.
The following Code-Snippet shows the Event Handler reacting to the “ItemRemoved”-Event updating the database projection for cart items.
 @EventHandler
fun on(
    event: ItemRemovedEvent,
    // (1) inject sequence number by Axon
    @SequenceNumber sequenceNumber: Long,
){
// (2) update projection this.repository.deleteById(
        CartItemsReadModelKey(
            event.aggregateId,
            event.itemId
) )
    // (3) update projection version
       with new sequenceNumber
572

HANDLING THE UI
 ) }
versionRepository.save(
    CartItemsProjectionVersion(
    event.aggregateId,
    sequenceNumber
)
To access the aggregate-sequence in the projector, we can use the @SequenceNumber annotation in (1), which is an Axon- Feature and automatically injects the current aggregate version in the Event Handler.
In (2), we update the projection as usual by deleting the item from the projection, and now in addition, we update the projec- tion version in the table in (3).
The steps involved are visualized in Fig. 41.10. First, the client executes a command (1) and retrieves the updated aggregate sequence—in this case, version 7. The client then issues a query (3). Since the projections are still updating in the background, the client might initially receive the old version, 6. The key difference is that the client now knows that it is out of sync because the required version differs from the current version. The client simply retries after a short period of time, checking if version 7 is now available. Once the projector has updated the projection version to 7 (5), the client’s request will succeed (6) and the polling can be stopped.
573

UNDERSTANDING EVENTSOURCING
 Fig. 41.10 - Fenced Polling Data Flow
This approach provides a lot of flexibility. We can now decide whether we want the polling logic on the client- or server- side. For instance, if the client includes an optional parameter,
“expectedVersion”, the polling could be handled completeley on the server, where it waits until the projection has caught up. The only job left for the client is to track the currently expected version.
Server-Side Polling
574

HANDLING THE UI
To efficiently implement server-side polling, we typically use a reactive library like Reactor76 to avoid blocking any container threads.
You can find an implementation of this approach in the branch “missing-chapters-4/ui-with-polling,” where we wait on the server side until the projections have caught up, using a small utility function that waits until the projection version matches
the requested version.
Using this approach, the client remains completely unaware of the polling mechanism, as the logic is managed on the server side. The following code snippet provides an example of how server-side polling can be implemented using Project Reactor. Its purpose is simply to demonstrate the concept, so a full understanding of the code is not essential. I won’t delve into the details of using Project Reactor, as that topic alone could easily fill an entire book.
 fun <T> doOnVersionMatch(
    versionMatch: () -> Boolean,
    query: () -> T & Any
): Mono<T> =
    Mono.defer {
        // (1)
        if (versionMatch()) {
            // matched version
            Mono.just(query())
        } else {
[..] }
 76 https://projectreactor.io/
575

UNDERSTANDING EVENTSOURCING
 }
// (2)
.retryWhen(
    Retry.fixedDelay(
        100,
        Duration.ofMillis(100)
    )
)
// (3)
.timeout(Duration.ofSeconds(20))
.onErrorMap(
TimeoutException::class.java ){
    IllegalStateException("Timeout", it)
}
In (1), we check if the projection version matches, if not it automatically retries every 100 milliseconds (2). The server- side polling times out after 20 seconds (a randomly chosen value in (3) for this example).
Polling works well when updates result from direct user interac- tions. However, it falls short and requires additional logic when updates are triggered by background processes without direct user interaction. In such cases, the client simply doesn’t know that it’s expected to fetch a new version of the data.
This is a perfect use case for using client notifications, which we ́ll discuss later in the chapter. Another workaround to address this is falling back to client-side process polling for the current version every few seconds. Only if the version changes, the client starts to fetch new data. This approach results in
576

HANDLING THE UI
rather cheap API-Calls for the version checks.
Fig. 41.11 showcases a client currently holding version 5 of a projection. The Client is constantly asking if the version has changed. Most calls simply result in a HTTP Status Code 304 (” Not Modified ” ). At some point the version changes to 6, which triggers a fetch call from the client and then restarts the whole cycle.
Fig. 41.11 - Version Polling
You can find an implementation of this approach in the inven-
 577

UNDERSTANDING EVENTSOURCING
tories component77. The inventory component polls the latest inventory version every five seconds and updates the inventory data to display if the version has changed. Since inventories are changed in the background, we can ́t rely on a direct client interaction.
In the next paragraph, we will explore a different approach, where the client is actively notified about changes.
Client Notifications using Server-Sent Events
Instead of constantly polling for changes, we can actively notify the client about updates. My preferred approach for this is using Server-Sent Events (SSE)78 as a standard notification mechanism. Unlike WebSockets, which allow full bi-directional communication between client and server, SSE is a much simpler one-way notification mechanism. It can be used to notify clients with a simple text-based protocol. It cannot be used for any communication from the client back to the server.
SSE is a mature technology but comes with a few caveats. Using HTTP 1.1, SSE is limited to 6 connections per browser and domain. That ́s not much and every open Browser-Tab increases the number of active connections. Using HTTP/2 this number is increased to 100 by default and can be increased even further.
77 https://github.com/dilgerma/eventsourcing-book/blob/missing-chapters- 4/ui-with-polling/src/main/nextjs/app/components/inventories/inventor ies.tsx
78 https://en.wikipedia.org/wiki/Server-sent_events 578
 
HANDLING THE UI
Fig. 41.12 shows the simple data flow. First, the client opens a connection and keeps it open. Whenever the state of the system changes, the server notifies the client by sending a notification over the open connection.
Fig. 41.12
Now you can use SSE as a straightforward notification mech- anism to inform the client about a version change, or you can leverage it to forward the projection data directly. You could even take it a step further by sending events directly to the client, enabling a form of client-side Event Sourcing.
Event Sourcing and especially Axon work extremely well to- gether with SSE. Axon supports the Concept of Subscription
 579

UNDERSTANDING EVENTSOURCING
Queries79, which allows to trigger actions whenever a Projection is updated as highlighted in Fig. 41.13
Fig. 41.13 - Axon Subscription Queries
A client subscribes to a query (1) and receives the initial result immediately (2). Whenever a command is issued (3) and the corresponding event is stored (4), the projection is updated (5). Once the projection is updated, the projector triggers the query update emitter (one crucial component of Subscription Queries)
79 https://docs.axoniq.io/axon-framework-reference/4.10/queries/query-dis patchers/#subscription-queries
  580

HANDLING THE UI
in (6), which notifies the client using Server-Sent Events (SSE) in (7).
Although it requires a bit of adjustment to get used to this setup, it enables the creation of highly flexible and interactive client integrations.
Client Side Subscription
How do clients connect to the SSE event stream? Fortunately, this is straightforward and requires just a few lines of code. In the following Code-Snippet you ́ll see an example how to integrate SSE in a Javascript-Client. We only need to instantiate an EventSource80 on the client-side and point it to the corre- sponding API endpoint.
We can now easily react to messages by providing the onmessage-Hook.
 const eventSource = new EventSource(
   `/inventories/sse/<productId>`, {
});
 eventSource.onmessage = (event) => {
    // (1) parse the payload
    let payload = JSON.parse(event.data)
    setInventory(payload.data.inventory)
};
Since the inventory now gets constantly updated, the client can
80 https://developer.mozilla.org/en-US/docs/Web/API/EventSource 581
 
UNDERSTANDING EVENTSOURCING
display or work with the data as it arrives.
One important caveat of Subscription Queries is that they are not supported (as of my best knowledge) in multi-node envi- ronments without using Axon Server, which is the commercial Event Store provided by AxonIQ. As the client needs to maintain an active subscription, the QueryBus needs to support this also, which is not the case without using Axon Server.
Conclusion
This was a lengthy but important chapter, as questions about how to integrate the UI come up repeatedly.
The two approaches presented in this chapter—polling and active notifications—are not necessarily better or worse than each other; they are simply different.
If you need a reliable solution that works in all environments, polling is a solid choice to get started, as long as it is done correctly. Especially using Server-Side Polling allows to remove a lot of complexity from a client. On the other hand, using Server- Sent Events with Subscription Queries offers great flexibility and power, but requires a deeper understanding of the Axon mechanics.
One key takeaway from this chapter, regardless of the chosen technology stack, is the importance of providing the client with as much context as possible. Instead of blindly polling for updated information, communicate to the client which version
582

HANDLING THE UI
of the data it should load. Avoid indiscriminate polling of multiple endpoints in the system without proper guardrails in place. This is why I call it “fenced polling”—it operates within a limited scope, specifically until the projection delivers the expected version.
There are certainly many other ways to combine Event Sourcing with user interfaces. This chapter has covered just two of them in detail. The ideas and concepts should be applicable regardless of the technology stack.
583

Where to go from here?
What an incredible journey this has been—hopefully for both of us.
From the initial idea promoted on LinkedIn in July 2024 to the release of this book in October 2024, the amount of work involved was immense. But in reality, the real work began much earlier.
The actual writing felt like the easy part. Every chapter, every concept in this book has been living in my mind for years. It was simply a matter of putting onto paper what had been forming beneath the surface for the last 15 years.
In 2021, I discovered Event Modeling, and that was one of the biggest “Aha!” moments of my life. I had already read Adam Dymitruk’s articles on Event Modeling back in 2018, but I wasn’t ready at the time. I just couldn ́t fully grasp it. Three years slipped by. I often wonder where I’d be today if someone had written this book back then.
Finally, writing down all the things I’ve been discussing in communities—whether on LinkedIn, the Event Modeling Dis- cord, or during various meetups—felt like a huge relief. It’s now captured in black and white, ready for anyone open to exploring
584

WHERE TO GO FROM HERE?
new ways of thinking.
For me, this approach to system design and implementation just feels right.
The past few weeks, polishing the manuscript, refining the structure, and removing what wasn’t essential, have been exhausting—but worth it.
This is the book I wanted to write.
When I first published it, I couldn’t believe it—someone bought the book, even though not a single chapter was finished. That one purchase gave me the confidence to push forward, knowing there were people willing to read along as I write. It was surreal.
Then came the call for reviewers. Reviewing a book is no easy task. You have to sift through the content and provide feedback, all for a simple “thank you.” I didn’t expect much of a response, but many showed up. Again, surreal.
This taught me something important—this work matters.
In the end, it’s not just about writing software. It’s not about creating faster systems or the debate between Event Sourcing and CRUD.
It’s about communication.
I want to be able to sit down with a subject-matter expert, using plain language from the start, to clearly outline what a system
585

UNDERSTANDING EVENTSOURCING
should do. And I want this expert to confidently say, “Yes, this is exactly what I need. Thank you.”
This book is the one I so desperately needed five years ago.
And my hope is that, five years from now, someone will pick up this book and say, “Thank goodness I found this. It saved me so much time.”
With that being said, I want to thank you for reading this book and embarking on this journey with me. After all these years, I think we’ve finally found a path that is leading to a promising future. It was always there—we just needed a little help seeing it.
If you haven ́t realized it until now - you are all but alone on this path. Many already started the journey and you can find them here:
https://www.eventmodeling.org - here it all began
https://www.eventmodelers.de - a starting point for the german eventmodeling community
They are all eager to help, or maybe right now, one of them needs your help, who knows? I learned so much from this community, and so can you. Just reach out, we are here.