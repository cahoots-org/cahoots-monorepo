services:
  # Redis Stack for storage, caching, and RediSearch (required by Context Engine)
  redis:
    image: redis/redis-stack-server:latest
    ports:
      - "6380:6379"
    volumes:
      - redis_data:/data
    environment:
      - REDIS_ARGS=--appendonly yes
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 3

  # Cahoots Monolith API
  cahoots-api:
    build: .
    platform: linux/amd64
    ports:
      - "8000:8000"
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_DB=0
      - JWT_SECRET_KEY=${JWT_SECRET_KEY:-cahoots-dev-secret-key-please-change-in-production}
      - GOOGLE_CLIENT_ID=${GOOGLE_CLIENT_ID:-}
      - GOOGLE_CLIENT_SECRET=${GOOGLE_CLIENT_SECRET:-}
      - LLM_PROVIDER=${LLM_PROVIDER:-mock}
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - GROQ_API_KEY=${GROQ_API_KEY:-}
      - GROQ_MODEL=${GROQ_MODEL:-mixtral-8x7b-32768}
      - LAMBDA_API_KEY=${LAMBDA_API_KEY:-}
      - CEREBRAS_API_KEY=${CEREBRAS_API_KEY:-}
      - CEREBRAS_MODEL=${CEREBRAS_MODEL:-llama3.1-70b}
      - FEATHERLESS_API_KEY=${FEATHERLESS_API_KEY:-}
      - FEATHERLESS_MODEL=${FEATHERLESS_MODEL:-moonshotai/Kimi-K2-Instruct}
      - AWS_BEARER_TOKEN_BEDROCK=${AWS_BEARER_TOKEN_BEDROCK:-}
      - AWS_BEDROCK_MODEL=${AWS_BEDROCK_MODEL:-}
      - AWS_BEDROCK_REGION=${AWS_BEDROCK_REGION:-us-east-1}
      - HUGGINGFACE_API_KEY=${HUGGINGFACE_API_KEY:-}
      - LOCAL_LLM_URL=${LOCAL_LLM_URL:-http://host.docker.internal:11434/v1}
      - LOCAL_LLM_MODEL=${LOCAL_LLM_MODEL:-llama3.1:70b}
      - MAX_DEPTH=${MAX_DEPTH:-5}
      - COMPLEXITY_THRESHOLD=${COMPLEXITY_THRESHOLD:-0.45}
      - BATCH_SIZE=${BATCH_SIZE:-3}
      - ENVIRONMENT=${ENVIRONMENT:-development}
      - CONTEXT_ENGINE_URL=http://context-engine:8001
      - CONTEXT_ENGINE_API_KEY=${CONTEXT_ENGINE_API_KEY:-}
      - STRIPE_SECRET_KEY=${STRIPE_SECRET_KEY:-}
      - STRIPE_PUBLISHABLE_KEY=${STRIPE_PUBLISHABLE_KEY:-}
      - STRIPE_WEBHOOK_SECRET=${STRIPE_WEBHOOK_SECRET:-}
      - STRIPE_PRICE_PRO_MONTHLY=${STRIPE_PRICE_PRO_MONTHLY:-}
      - STRIPE_PRICE_PRO_YEARLY=${STRIPE_PRICE_PRO_YEARLY:-}
      - STRIPE_SUCCESS_URL=${STRIPE_SUCCESS_URL:-http://localhost:3000/settings?success=true}
      - STRIPE_CANCEL_URL=${STRIPE_CANCEL_URL:-http://localhost:3000/pricing?canceled=true}
    volumes:
      - ./app:/app/app:ro
    depends_on:
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "python", "-c", "import httpx; httpx.get('http://localhost:8000/health')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: unless-stopped

  # OpenSearch for vector embeddings and full-text search
  opensearch:
    image: opensearchproject/opensearch:2.11.0
    environment:
      - discovery.type=single-node
      - DISABLE_SECURITY_PLUGIN=true
      - "OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m"
    ports:
      - "9201:9200"
    volumes:
      - opensearch_data:/usr/share/opensearch/data
    healthcheck:
      test: ["CMD-SHELL", "curl -s http://localhost:9200/_cluster/health | grep -q '\"status\":\"green\"\\|\"status\":\"yellow\"'"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped

  # Contex - Semantic context routing for AI agents
  # Using published image from: https://github.com/cahoots-org/contex
  context-engine:
    image: ghcr.io/cahoots-org/contex:latest
    platform: linux/amd64
    ports:
      - "8003:8001"
    environment:
      - REDIS_URL=redis://redis:6379
      - OPENSEARCH_URL=http://opensearch:9200
      - SIMILARITY_THRESHOLD=${SIMILARITY_THRESHOLD:-0.5}
      - MAX_MATCHES=${MAX_MATCHES:-10}
      - MAX_CONTEXT_SIZE=${MAX_CONTEXT_SIZE:-51200}
      - HYBRID_SEARCH_ENABLED=true
      - RRF_K=60
      - VECTOR_BOOST=1.0
    depends_on:
      redis:
        condition: service_healthy
      opensearch:
        condition: service_healthy
    deploy:
      resources:
        limits:
          memory: 2GB
          cpus: '2.0'
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "python", "-c", "import httpx; httpx.get('http://localhost:8001/')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # vLLM Local Inference Server
  # NOTE: Due to Docker Desktop GPU limitations, run vLLM directly on host instead
  # Install: pip install vllm
  # Run: HF_TOKEN=$HUGGINGFACE_API_KEY vllm serve Qwen/Qwen3-32B-Instruct \
  #        --quantization awq --dtype auto --max-model-len 32768 \
  #        --gpu-memory-utilization 0.9 --enable-prefix-caching --port 8001
  # Then set LLM_PROVIDER=local and LOCAL_LLM_URL=http://localhost:8001/v1
  #
  # Alternatively, if you switch from Docker Desktop to native Docker, uncomment below:
  # vllm:
  #   image: vllm/vllm-openai:latest
  #   command:
  #     - --model=Qwen/Qwen3-32B-Instruct
  #     - --quantization=awq
  #     - --dtype=auto
  #     - --max-model-len=32768
  #     - --gpu-memory-utilization=0.9
  #     - --enable-prefix-caching
  #   ports:
  #     - "8001:8000"
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: all
  #             capabilities: [gpu]
  #   environment:
  #     - CUDA_VISIBLE_DEVICES=0
  #     - HF_TOKEN=${HUGGINGFACE_API_KEY}
  #   volumes:
  #     - vllm-cache:/root/.cache/huggingface
  #   profiles:
  #     - local-inference
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 3
  #     start_period: 600s
  #   restart: unless-stopped

  # Gitea - Self-hosted Git server for code generation
  gitea:
    image: gitea/gitea:1.21-rootless
    platform: linux/amd64
    ports:
      - "3001:3000"
      - "2222:22"
    environment:
      - USER_UID=1000
      - USER_GID=1000
      - GITEA__database__DB_TYPE=sqlite3
      - GITEA__server__ROOT_URL=http://localhost:3001
      - GITEA__server__HTTP_PORT=3000
      - GITEA__service__DISABLE_REGISTRATION=true
      - GITEA__security__INSTALL_LOCK=true
    volumes:
      - gitea_data:/var/lib/gitea
      - gitea_config:/etc/gitea
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/api/v1/version"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # Workspace Service - Git-backed file operations for code generation agents
  workspace-service:
    build:
      context: ./workspace-service
      dockerfile: Dockerfile
    platform: linux/amd64
    ports:
      - "8010:8001"
    environment:
      - WORKSPACE_GITEA_URL=http://gitea:3000
      - WORKSPACE_GITEA_API_TOKEN=${GITEA_API_TOKEN:-}
      - WORKSPACE_GITEA_BOT_USERNAME=cahoots-bot
      - WORKSPACE_CONTEXT_ENGINE_URL=http://context-engine:8001
      - WORKSPACE_REDIS_URL=redis://redis:6379
      - WORKSPACE_WORKSPACES_ROOT=/workspaces
      - WORKSPACE_COMMIT_AUTHOR_NAME=Cahoots Bot
      - WORKSPACE_COMMIT_AUTHOR_EMAIL=bot@cahoots.dev
    volumes:
      - workspaces_data:/workspaces
    depends_on:
      redis:
        condition: service_healthy
      gitea:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "python", "-c", "import httpx; httpx.get('http://localhost:8001/health')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # Runner Service - Test execution on Cloud Run Jobs
  runner-service:
    build:
      context: ./runner-service
      dockerfile: Dockerfile
    platform: linux/amd64
    ports:
      - "8011:8002"
    environment:
      - RUNNER_REDIS_URL=redis://redis:6379
      - RUNNER_WORKSPACE_SERVICE_URL=http://workspace-service:8001
      - RUNNER_GCP_PROJECT_ID=${GCP_PROJECT_ID:-}
      - RUNNER_GCP_REGION=${GCP_REGION:-us-central1}
      - GOOGLE_APPLICATION_CREDENTIALS=/app/gcp-credentials.json
    volumes:
      - ${GCP_CREDENTIALS_PATH:-./gcp-credentials.json}:/app/gcp-credentials.json:ro
      # Mount Docker socket for local test execution (used when GCP not configured)
      - /var/run/docker.sock:/var/run/docker.sock
    depends_on:
      redis:
        condition: service_healthy
      workspace-service:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "python", "-c", "import httpx; httpx.get('http://localhost:8002/health')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # Frontend (React)
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    ports:
      - "3000:80"
    environment:
      - REACT_APP_API_URL=http://localhost:8000
      - REACT_APP_WS_URL=ws://localhost:8000
      - NODE_ENV=production
      - BACKEND_URL=http://cahoots-api:8000
      - PORT=80
    volumes:
      - ./nginx.conf:/etc/nginx/conf.d/default.conf:ro
    depends_on:
      cahoots-api:
        condition: service_healthy
    restart: unless-stopped

  # Development API (with hot reload)
  api-dev:
    build:
      context: .
      target: builderhttps://cahoots-api-production.up.railway.app
    ports:
      - "8001:8000"
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_DB=1
      - JWT_SECRET_KEY=${JWT_SECRET_KEY:-cahoots-dev-secret-key-please-change-in-production}
      - ENV=development
      - LLM_PROVIDER=mock
    volumes:
      - .:/app
    depends_on:
      redis:
        condition: service_healthy
    profiles:
      - dev
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
    restart: unless-stopped

  # Prometheus - Metrics collection
  prometheus:
    image: prom/prometheus:latest
    platform: linux/amd64
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.enable-lifecycle'
    depends_on:
      cahoots-api:
        condition: service_healthy
    restart: unless-stopped

  # Grafana - Metrics visualization
  grafana:
    image: grafana/grafana:latest
    platform: linux/amd64
    ports:
      - "3002:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=cahoots
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning:ro
    depends_on:
      - prometheus
    restart: unless-stopped

volumes:
  redis_data:
  opensearch_data:
  vllm-cache:
  gitea_data:
  gitea_config:
  workspaces_data:
  prometheus_data:
  grafana_data: