services:
  # Redis for storage and caching
  redis:
    image: redis:7-alpine
    ports:
      - "6380:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 3

  # Cahoots Monolith API
  api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_DB=0
      - JWT_SECRET_KEY=${JWT_SECRET_KEY:-cahoots-dev-secret-key-please-change-in-production}
      - GOOGLE_CLIENT_ID=${GOOGLE_CLIENT_ID:-}
      - GOOGLE_CLIENT_SECRET=${GOOGLE_CLIENT_SECRET:-}
      - LLM_PROVIDER=${LLM_PROVIDER:-mock}
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - GROQ_API_KEY=${GROQ_API_KEY:-}
      - GROQ_MODEL=${GROQ_MODEL:-mixtral-8x7b-32768}
      - LAMBDA_API_KEY=${LAMBDA_API_KEY:-}
      - CEREBRAS_API_KEY=${CEREBRAS_API_KEY:-}
      - CEREBRAS_MODEL=${CEREBRAS_MODEL:-llama3.1-70b}
      - HUGGINGFACE_API_KEY=${HUGGINGFACE_API_KEY:-}
      - LOCAL_LLM_URL=${LOCAL_LLM_URL:-http://host.docker.internal:11434/v1}
      - LOCAL_LLM_MODEL=${LOCAL_LLM_MODEL:-llama3.1:70b}
      - MAX_DEPTH=${MAX_DEPTH:-5}
      - COMPLEXITY_THRESHOLD=${COMPLEXITY_THRESHOLD:-0.45}
      - BATCH_SIZE=${BATCH_SIZE:-3}
      - ENVIRONMENT=${ENVIRONMENT:-development}
    volumes:
      - ./app:/app/app:ro
    depends_on:
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "python", "-c", "import httpx; httpx.get('http://localhost:8000/health')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: unless-stopped

  # Contex - Semantic context routing for AI agents
  # Using published image from: https://github.com/cahoots-org/contex
  context-engine:
    image: ghcr.io/cahoots-org/contex:latest
    ports:
      - "8001:8001"
    environment:
      - REDIS_URL=redis://redis:6379
      - SIMILARITY_THRESHOLD=${SIMILARITY_THRESHOLD:-0.5}
      - MAX_MATCHES=${MAX_MATCHES:-10}
    depends_on:
      redis:
        condition: service_healthy
    deploy:
      resources:
        limits:
          memory: 2GB
          cpus: '2.0'
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "python", "-c", "import httpx; httpx.get('http://localhost:8001/')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # vLLM Local Inference Server
  # NOTE: Due to Docker Desktop GPU limitations, run vLLM directly on host instead
  # Install: pip install vllm
  # Run: HF_TOKEN=$HUGGINGFACE_API_KEY vllm serve Qwen/Qwen3-32B-Instruct \
  #        --quantization awq --dtype auto --max-model-len 32768 \
  #        --gpu-memory-utilization 0.9 --enable-prefix-caching --port 8001
  # Then set LLM_PROVIDER=local and LOCAL_LLM_URL=http://localhost:8001/v1
  #
  # Alternatively, if you switch from Docker Desktop to native Docker, uncomment below:
  # vllm:
  #   image: vllm/vllm-openai:latest
  #   command:
  #     - --model=Qwen/Qwen3-32B-Instruct
  #     - --quantization=awq
  #     - --dtype=auto
  #     - --max-model-len=32768
  #     - --gpu-memory-utilization=0.9
  #     - --enable-prefix-caching
  #   ports:
  #     - "8001:8000"
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: all
  #             capabilities: [gpu]
  #   environment:
  #     - CUDA_VISIBLE_DEVICES=0
  #     - HF_TOKEN=${HUGGINGFACE_API_KEY}
  #   volumes:
  #     - vllm-cache:/root/.cache/huggingface
  #   profiles:
  #     - local-inference
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 3
  #     start_period: 600s
  #   restart: unless-stopped

  # Frontend (React)
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    ports:
      - "3000:80"
    environment:
      - REACT_APP_API_URL=http://localhost:8000
      - REACT_APP_WS_URL=ws://localhost:8000
      - NODE_ENV=production
      - BACKEND_URL=http://api:8000
      - PORT=80
    volumes:
      - ./nginx.conf:/etc/nginx/conf.d/default.conf:ro
    depends_on:
      api:
        condition: service_healthy
    restart: unless-stopped

  # Development API (with hot reload)
  api-dev:
    build:
      context: .
      target: builderhttps://cahoots-api-production.up.railway.app
    ports:
      - "8001:8000"
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_DB=1
      - JWT_SECRET_KEY=${JWT_SECRET_KEY:-cahoots-dev-secret-key-please-change-in-production}
      - ENV=development
      - LLM_PROVIDER=mock
    volumes:
      - .:/app
    depends_on:
      redis:
        condition: service_healthy
    profiles:
      - dev
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
    restart: unless-stopped

volumes:
  redis_data:
  vllm-cache: